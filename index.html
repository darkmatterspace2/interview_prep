<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interview Prep Notes</title>
    <!-- GitHub Markdown CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.2.0/github-markdown.min.css">
    <!-- Marked.js for Markdown rendering -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        :root {
            --bg-color: #ffffff;
            --text-color: #24292f;
            --sidebar-bg: #f6f8fa;
            --sidebar-border: #d0d7de;
            --link-color: #0969da;
            --markdown-bg: #ffffff;
            --markdown-text: #24292f;
        }

        [data-theme="dark"] {
            --bg-color: #0d1117;
            --text-color: #c9d1d9;
            --sidebar-bg: #161b22;
            --sidebar-border: #30363d;
            --link-color: #58a6ff;
            --markdown-bg: #0d1117;
            --markdown-text: #c9d1d9;
        }

        body {
            margin: 0;
            display: flex;
            height: 100vh;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            overflow: hidden;
            background-color: var(--bg-color);
            color: var(--text-color);
            transition: background-color 0.3s, color 0.3s;
        }
        
        #sidebar {
            width: 300px;
            background-color: var(--sidebar-bg);
            border-right: 1px solid var(--sidebar-border);
            overflow-y: auto;
            padding: 20px;
            flex-shrink: 0;
            transition: transform 0.3s ease, width 0.3s ease;
        }
        
        #content {
            flex-grow: 1;
            padding: 40px;
            overflow-y: auto;
            background-color: var(--markdown-bg);
            position: relative;
        }

        /* Zen Mode: Hide Sidebar */
        body.zen-mode #sidebar {
            width: 0;
            padding: 0;
            border: none;
            overflow: hidden;
            transform: translateX(-100%);
        }

        body.zen-mode #content {
            padding: 40px 15%; /* Center content more in Zen Mode */
        }

        /* Markdown Dark Mode Overrides */
        [data-theme="dark"] .markdown-body {
            color-scheme: dark;
            --color-canvas-default: #0d1117;
            --color-canvas-subtle: #161b22;
            --color-border-default: #30363d;
            --color-border-muted: #21262d;
            --color-fg-default: #c9d1d9;
            --color-fg-muted: #8b949e;
            --color-accent-fg: #58a6ff;
            /* Add more GitHub Dark variables as needed */
        }

        .markdown-body {
            box-sizing: border-box;
            min-width: 200px;
            max-width: 980px;
            margin: 0 auto;
            padding: 45px;
            background-color: transparent !important; /* Let body bg show through */
        }

        @media (max-width: 767px) {
            .markdown-body {
                padding: 15px;
            }
            #sidebar {
                width: 100%;
                height: 30%;
                border-right: none;
                border-bottom: 1px solid var(--sidebar-border);
                transform: none !important; /* Disable Zen transformation on mobile for now */
            }
            body {
                flex-direction: column;
            }
            body.zen-mode #sidebar {
                height: 0;
                padding: 0;
            }
        }
        
        /* Tree View Styles */
        ul {
            list-style-type: none;
            padding-left: 20px;
            margin: 0;
        }
        
        li {
            margin: 5px 0;
        }
        
        .folder-name {
            font-weight: bold;
            cursor: pointer;
            color: var(--text-color);
            display: flex;
            align-items: center;
        }
        
        .folder-name::before {
            content: 'üìÇ';
            margin-right: 5px;
        }

        .file-link {
            cursor: pointer;
            color: var(--link-color);
            text-decoration: none;
            display: flex;
            align-items: center;
        }
        
        .file-link:hover {
            text-decoration: underline;
        }
        
        .file-link::before {
            content: 'üìÑ';
            margin-right: 5px;
        }

        .folder-content {
            display: none;
        }
        
        .folder-open > .folder-content {
            display: block;
        }
        
        /* Toolbar */
        .toolbar {
            margin-bottom: 20px;
            display: flex;
            gap: 10px;
            justify-content: space-between;
        }
        
        button {
            padding: 5px 10px;
            cursor: pointer;
            border-radius: 6px;
            border: 1px solid var(--sidebar-border);
            background-color: var(--bg-color);
            color: var(--text-color);
        }

        button:hover {
            opacity: 0.8;
        }

        /* Floating Zen Toggle (Visible only when sidebar is hidden) */
        #zen-toggle-floating {
            position: fixed;
            bottom: 20px;
            left: 20px;
            z-index: 1000;
            display: none;
            background: var(--sidebar-bg);
            border: 1px solid var(--sidebar-border);
            border-radius: 50%;
            width: 40px;
            height: 40px;
            font-size: 20px;
            align-items: center;
            justify-content: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.2);
            padding: 0;
        }

        body.zen-mode #zen-toggle-floating {
            display: flex;
        }

    </style>
</head>
<body>

<!-- Floating button to exit Zen Mode -->
<button id="zen-toggle-floating" title="Exit Zen Mode">üëÅÔ∏è</button>

<div id="sidebar">
    <div class="toolbar">
        <button id="theme-toggle" title="Toggle Dark/Light Mode">üåô</button>
        <button id="zen-toggle-sidebar" title="Enter Zen Mode">‚ñ£</button>
    </div>
    <h3>üìö Knowledge Base</h3>
    <div id="tree-root"></div>
</div>

<div id="content">
    <div id="markdown-viewer" class="markdown-body">
        <h1>Select a file to view</h1>
        <p>Click on any file in the sidebar to load its content.</p>
    </div>
</div>

<script>
    const fileData = {"name": "interview_prep", "type": "directory", "children": [{"name": "Advance_data_engineering", "type": "directory", "children": [{"name": "clickstream.md", "type": "file", "content": "# Advanced Data Engineering: Clickstream Analysis\n\n## Classic FAANG Interview Question: Sessionization\n\n### Scenario\nYou are processing a massive stream of clickstream data for a major e-commerce platform. The data arrives with the following schema:\n*   `user_id` (String): Unique identifier for the user.\n*   `timestamp` (Timestamp): Time of the click event.\n*   `url` (String): The page visited.\n\n**Goal:**\nIdentify \"user sessions\". A session is defined as a sequence of activities by the same user where the time gap between any two consecutive events is less than **30 minutes**. If a user is inactive for 30 minutes or more, the next event starts a new session.\n\n**Deliverable:**\nWrite a SQL query (or PySpark logic) to assign a unique `session_id` to each event row.\n\n---\n\n### Solution: The \"New Session Flag\" & Cumulative Sum Approach\n\nThis is the standard pattern for solving sessionization problems in SQL-based environments (BigQuery, Redshift, Spark SQL).\n\n#### Step 1: Calculate Time Difference\nUse `LAG` to look at the previous event's timestamp for the same user.\n\n#### Step 2: Mark Session Boundaries\nCreate a boolean flag (or 1/0 integer) that is `1` if the gap > 30 mins (or if it's the first event), and `0` otherwise.\n\n#### Step 3: Generate Session ID\nPerform a cumulative sum of the flags over the user's history. This running total effectively works as a unique ID for each session.\n\n### SQL Implementation\n\n```sql\nWITH PreviousEvent AS (\n    SELECT \n        user_id,\n        timestamp,\n        url,\n        LAG(timestamp) OVER (PARTITION BY user_id ORDER BY timestamp) as prev_ts\n    FROM clicks\n),\nNewSessionFlag AS (\n    SELECT \n        *,\n        CASE \n            WHEN prev_ts IS NULL THEN 1                            -- First event ever for user\n            WHEN timestamp - prev_ts > INTERVAL '30' MINUTE THEN 1 -- Inactivity gap exceeded\n            ELSE 0 \n        END as is_new_session\n    FROM PreviousEvent\n)\nSELECT \n    user_id,\n    timestamp,\n    url,\n    -- Concat user_id with the running total to make it truly unique globally\n    CONCAT(user_id, '-', SUM(is_new_session) OVER (PARTITION BY user_id ORDER BY timestamp)) as session_id\nFROM NewSessionFlag;\n```\n\n### Follow-up Questions (The \"FAANG Twist\")\n1.  **Scaling:** How would you handle this if the data volume is too large for a standard window function shuffle? (Answer key: Salting, or using `mapGroupsWithState` in Spark Streaming for O(1) state management per user).\n2.  **Late Data:** What if events arrive out of order? (Answer key: Watermarking in streaming, or re-processing partitions in batch).\n"}, {"name": "streaming_sessions.md", "type": "file", "content": "# PySpark Streaming & Sessionization Scenarios\n\nCommon interview questions for product-based companies (Netflix, Amazon, Airbnb) focusing on user session management and clickstream analysis using PySpark.\n\n## Scenario 1: Sessionization based on Inactivity (Gap Analysis)\n**Problem:** Given a stream of click events with `user_id` and `timestamp`, group events into sessions. A new session starts if the user is inactive for more than 30 minutes.\n\n**Technique:**\n1.  **Sort** events by user and time.\n2.  Calculate **Time Lag** from the previous event.\n3.  Flag a **New Session** if `current_ts - prev_ts > 30 mins`.\n4.  Generate **Session ID** using a cumulative sum of the flags.\n\n**Code:**\n```python\nfrom pyspark.sql.functions import col, lag, sum, when, unix_timestamp\nfrom pyspark.sql.window import Window\n\n# 1. Define Window\nuser_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n\n# 2. Calculate time difference in seconds\ndf = df.withColumn(\"prev_timestamp\", lag(\"timestamp\").over(user_window))\ndf = df.withColumn(\"time_diff_sec\", \n                   unix_timestamp(\"timestamp\") - unix_timestamp(\"prev_timestamp\"))\n\n# 3. Mark start of new session (30 mins = 1800 seconds)\n# Null prev_timestamp means it's the first event -> New Session\ndf = df.withColumn(\"is_new_session\", \n                   when((col(\"time_diff_sec\") > 1800) | col(\"time_diff_sec\").isNull(), 1).otherwise(0))\n\n# 4. Generate Session ID (Cumulative Sum of markers)\ndf = df.withColumn(\"session_id\", sum(\"is_new_session\").over(user_window))\n\n# Result: Each user event now has a 'session_id' (1, 2, 3...) localized to that user.\n# You can then aggregate by user_id + session_id to get start/end times and count.\n```\n\n## Scenario 2: Login / Logout Session Duration\n**Problem:** You have `Login` and `Logout` events. Calculate the duration of each completed session. Handle cases where a `Logout` might be missing (ignore or cap).\n\n**Technique:** Use `lead()` to look ahead at the next event.\n\n**Code:**\n```python\nfrom pyspark.sql.functions import lead, col, unix_timestamp\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n\n# Look ahead to see the next event type and time\ndf_with_next = df.withColumn(\"next_event\", lead(\"event_type\").over(window_spec)) \\\n                 .withColumn(\"next_timestamp\", lead(\"timestamp\").over(window_spec))\n\n# Filter for completed sessions: Current is Login, Next is Logout\ncompleted_sessions = df_with_next.filter(\n    (col(\"event_type\") == \"Login\") & (col(\"next_event\") == \"Logout\")\n)\n\n# Calculate Duration\nresult = completed_sessions.withColumn(\"duration_sec\", \n    unix_timestamp(\"next_timestamp\") - unix_timestamp(\"timestamp\")\n)\n```\n\n## Scenario 3: Clickstream Path Analysis (Funnel)\n**Problem:** Identify the most common 3-step path users take (e.g., Home -> Search -> Product).\n\n**Technique:** `collect_list` or string concatenation with `lead`.\n\n**Code:**\n```python\nfrom pyspark.sql.functions import concat_ws, lead, count, col, lit\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n\n# Create the 3-step path for every row\ndf_path = df.withColumn(\"step_1\", col(\"page_name\")) \\\n            .withColumn(\"step_2\", lead(\"page_name\", 1).over(window_spec)) \\\n            .withColumn(\"step_3\", lead(\"page_name\", 2).over(window_spec))\n\n# Filter out incomplete paths\ndf_path = df_path.dropna(subset=[\"step_2\", \"step_3\"])\n\n# Concatenate to form a path string\ndf_path = df_path.withColumn(\"path\", \n              concat_ws(\" -> \", col(\"step_1\"), col(\"step_2\"), col(\"step_3\")))\n\n# Count frequencies\ntop_paths = df_path.groupBy(\"path\").agg(count(\"*\").alias(\"frequency\")) \\\n                   .orderBy(col(\"frequency\").desc())\n```\n\n## Scenario 4: Finding Max Concurrent Users (Advanced)\n**Problem:** Given session start and end times, find the maximum number of users active at the same time.\n\n**Technique:** Explode start/end logic. Treat Start as +1, End as -1.\n\n**Code:**\n```python\n# 1. Decompose sessions into points\nstart_df = sessions.select(col(\"start_time\").alias(\"time\"), lit(1).alias(\"change\"))\nend_df = sessions.select(col(\"end_time\").alias(\"time\"), lit(-1).alias(\"change\"))\n\ncombined = start_df.union(end_df)\n\n# 2. Verify order (process starts before ends if timestamps match)\nwindow_cumulative = Window.orderBy(\"time\")\n\n# 3. Running Sum\nresult = combined.withColumn(\"active_users\", sum(\"change\").over(window_cumulative))\nmax_concurrency = result.agg({\"active_users\": \"max\"}).collect()[0][0]\n```\n\n## Scenario 5: Identifying Bots (High Frequency)\n**Problem:** Flag users who perform more than 50 actions in any 1-minute sliding window.\n\n**Technique:** Range Window (sliding time window).\n\n**Code:**\n```python\nfrom pyspark.sql.functions import count\nfrom pyspark.sql.window import Window\n\n# Define window: Partition by User, Order by Time, Range of 60 seconds looking back\n# Range between 60 seconds preceding and current row\nwindow_bot = Window.partitionBy(\"user_id\").orderBy(col(\"timestamp\").cast(\"long\")) \\\n                   .rangeBetween(-60, 0)\n\ndf_bots = df.withColumn(\"events_last_min\", count(\"*\").over(window_bot))\n\nbots = df_bots.filter(col(\"events_last_min\") > 50).select(\"user_id\").distinct()\n```\n"}]}, {"name": "Azure", "type": "directory", "children": [{"name": "azure_aks.md", "type": "file", "content": "# Azure Kubernetes Service (AKS) Interview Questions\n\n## Q1: Explain the architecture of AKS. What does Azure manage vs. what do you manage?\n**Answer**:\n*   **Control Plane (Master Node)**: Hosted by Azure. Includes API Server, Scheduler, etcd. It is free (unless you pay for Uptime SLA). You cannot SSH into it.\n*   **Worker Nodes**: The VMs where your pods run. You manage these (though AKS handles patching/upgrades). You pay for these VMs.\n\n## Q2: What is the difference between Kubenet and Azure CNI networking?\n**Answer**:\n*   **Kubenet**: Nodes get a VNet IP, but Pods use a NAT (Overlay) network. Saves IP addresses but has a slight performance hit due to NAT.\n*   **Azure CNI**: Every Pod gets a real IP from the VNet. Faster, better for connectivity with on-prem, but consumes massive IP address space.\n\n## Q3: How does AKS Autoscaling differ from HPA?\n**Answer**:\n*   **Cluster Autoscaler (CA)**: Watches for pods appearing in \"Pending\" state (due to lack of resources) and spins up new Azure VMs (Nodes).\n*   **Horizontal Pod Autoscaler (HPA)**: Watches CPU/Memory of pods and increases the *number of replicas* (Pods).\n*   *Flow*: Traffic spike -> HPA adds pods -> Nodes get full -> CA adds Nodes.\n\n## Q4: How do you safely upgrade an AKS cluster?\n**Answer**:\nAKS uses a \"Surge\" strategy.\n1.  It creates a new Node with the new version.\n2.  It \"Cordons\" an old node (stops scheduling).\n3.  It \"Drains\" the old node (moves pods to the new node).\n4.  It deletes the old node.\n*   **Tip**: Always set `maxSurge` to control speed/cost.\n\n## Q5: What is a Node Pool?\n**Answer**:\nA group of VMs with the same configuration. You can have a \"User Node Pool\" for apps (e.g., GPU enabled) and a \"System Node Pool\" for CoreDNS/Metrics (standard VMs).\n"}, {"name": "azure_apps.md", "type": "file", "content": "# Azure App Service Interview Questions\n\n## Q1: What is the difference between an App Service and an App Service Plan?\n**Answer**:\n*   **App Service Plan (ASP)**: Represents the physical resources (VMs) and the billing unit. It defines the region, OS (Linux/Windows), and scale (CPU/RAM).\n*   **App Service**: The actual web application running *on top* of the ASP. You can run multiple Apps on a single Plan to save money, but they share the same CPU/RAM.\n\n## Q2: explain Deployment Slots and their benefits.\n**Answer**:\nDeployment Slots are live apps with their own hostnames (e.g., `myapp-staging.azurewebsites.net`).\n**Benefits**:\n1.  **Zero Downtime Deployment**: You deploy to \"Staging\", warm it up, and then \"Swap\" with \"Production\". The swap just repoints the internal load balancer.\n2.  **A/B Testing**: You can route a % of traffic to a slot to test new features.\n\n## Q3: How does Auto-scaling work in App Service?\n**Answer**:\n*   **Scale Up**: Increasing the tier of the VM (e.g., B1 -> P1V2) for more CPU/RAM. Requires downtime.\n*   **Scale Out**: Increasing the *number* of instances (VMs).\n    *   **Autoscale Rules**: Define triggers (e.g., \"If CPU > 70% for 5 mins, add 1 instance\").\n    *   **Max Limit**: Prevents runaway costs (e.g., \"Max 10 instances\").\n\n## Q4: How do you secure an App Service?\n**Answer**:\n1.  **Authentication**: Enable built-in \"Easy Auth\" (Azure AD, Google, Facebook login).\n2.  **Networking**: Use **VNet Integration** to access private backend resources (SQL/Redis) and **Private Endpoints** to prevent public internet access to the App.\n3.  **Managed Identity**: Use System-assigned Identity to access Key Vault/SQL without storing secrets in code.\n"}, {"name": "azure_data_factory.md", "type": "file", "content": "# Azure Data Factory (ADF) Interview Questions\n\n## Section 1: Core Concepts & Architecture\n\n### Q1: What are the different types of Integration Runtimes (IR)?\n**Answer**:\n1.  **Azure IR**: Fully managed, auto-scaling. Used for copying data between Cloud stores (Blob -> SQL) using public endpoints.\n2.  **Self-Hosted IR**: Installed on a local machine/VM inside a private network. Used to connect to **On-Premise** data (SQL Server, Oracle) or resources inside a private VNet.\n3.  **Azure-SSIS IR**: Dedicated cluster to run legacy SSIS packages in the cloud.\n\n### Q2: Copy Activity vs. Data Flow?\n**Answer**:\n*   **Copy Activity**: Optimized purely for **data movement** (Extract & Load). Fast, cheap, limited transformations (column mapping, type conversion). No Spark cluster needed.\n*   **Mapping Data Flow**: Visual interface for **data transformation** (Transform). Runs on a managed Spark cluster behind the scenes. Can do Joins, Aggregates, Pivots, Window functions, and SCD logic.\n\n### Q3: Trigger Types in ADF?\n**Answer**:\n1.  **Schedule Trigger**: Wall-clock time (e.g., \"Every day at 8 AM\").\n2.  **Tumbling Window Trigger**: Slices time into discrete windows. Supports state (can depend on previous window) and heavy backfilling.\n3.  **Event-Based Trigger**: Reacts to storage events (New Blob Created/Deleted) via Event Grid.\n4.  **Custom Events Trigger**: Triggered by custom events on Event Grid.\n\n### Q4: Pipeline vs Activity vs Dataset vs Linked Service?\n**Answer**:\n*   **Linked Service**: Connection String (The \"Key\").\n*   **Dataset**: Reference to Data (The \"Table/File\").\n*   **Activity**: Action to perform (The \"Verb\").\n*   **Pipeline**: Logical grouping of activities (The \"Sentence\").\n\n### Q5: Variables vs Parameters?\n**Answer**:\n*   **Parameters**: External values passed *into* the pipeline at runtime (e.g., `StartDATE`). Immutable during the run.\n*   **Variables**: Internal values used to hold temporary state *during* the run. Can be changed using `Set Variable`.\n\n---\n\n## Section 2: Data Flows & Transformations (SCD)\n\n### Q6: How do you implement SCD Type 1 (Overwrite) in Mapping Data Flow?\n**Answer**:\nType 1 updates existing records vs inserts new ones (Upsert).\n1.  **Source**: Read generic source.\n2.  **Lookup/Join**: Match with Target table on Key.\n3.  **Alter Row**: Use expression `Upsert if: true()`.\n4.  **Sink**: Select \"Allow Upsert\" and specify \"Key Columns\".\n\n### Q7: How do you implement SCD Type 2 (History Preservation) in ADF?\n**Answer**:\nType 2 maintains history by adding `isActive`, `StartDate`, `EndDate` methods.\n1.  **Join**: Join Source (New) with Sink (Existing) on Business Key.\n2.  **Derived Column**: Create hashes (MD5) of columns to detect changes.\n3.  **New Records**: Mark as Insert with `isActive=1`.\n4.  **Changed Records (Old)**: Mark existing record in Sink as Update (`isActive=0`, `EndDate=Now`).\n5.  **Changed Records (New)**: Insert new version as Insert (`isActive=1`, `StartDate=Now`).\n6.  Use **Alter Row** strategies to route these to the Sink.\n\n### Q8: What represents the \"Alter Row\" transformation?\n**Answer**:\nIt tags rows with policies: **Insert**, **Update**, **Delete**, or **Upsert**.\n*   *Example*: `Delete if year(TransactionDate) < 2020`.\n*   These tags are enforced at the **Sink** step.\n\n### Q9: How to optimize a Join in Data Flow?\n**Answer**:\n*   **Broadcast Join**: If one side is small (Reference table), load it entirely into memory to avoid shuffling.\n*   **Partitioning**: Use \"Hash Partitioning\" on Join Keys to ensure related data stays on the same Spark node.\n\n### Q10: What is the purpose of \"Debug Mode\" in Data Flow?\n**Answer**:\nIt spins up a warm Spark cluster session. Allows interactive data preview at every step without running the full pipeline triggering overhead. Useful for development.\n\n---\n\n## Section 3: Performance Optimization\n\n### Q11: How to optimize Copy Activity performance?\n**Answer**:\n1.  **DIUs (Data Integration Units)**: Increase DIUs for Azure IR (more CPU/Network).\n2.  **Parallel Copies**: In Settings, increase \"Max parallel copies\" to read multiple files at once.\n3.  **Staged Copy**: When loading into Snowflake/Synapse from On-prem, use Blob Storage as a staging area. (On-Prem -> Blob -> PolyBase -> Synapse) is faster than direct ODBC insert.\n\n### Q12: How to handle copying millions of small files?\n**Answer**:\nSmall files cause overhead.\n*   **Zip/Merge**: Zip them at source if possible.\n*   **Binary Dataset**: Treat them as binary to avoid parsing each one.\n*   **Distcp**: Use specialized tools inside custom data flow logic or Databricks.\n\n### Q13: What implies the \"Degree of Copy Parallelism\"?\n**Answer**:\nADF determines how many threads to use based on the source/sink.\n*   For **File Sources**, it's the number of files.\n*   For **Partitioned Relational DBs**, mapping partitions to threads speeds up concurrent reads.\n\n### Q14: Self-Hosted IR is slow. How to scale?\n**Answer**:\n1.  **Scale Up**: Add CPU/RAM to the VM.\n2.  **Scale Out**: Install IR on up to 4 nodes in a cluster (Active-Active). ADF load balances the tasks.\n\n### Q15: How to optimize Data Flow execution time?\n**Answer**:\n*   **Integration Runtime**: Use a **Memory Optimized** compute type (more RAM per core).\n*   **TTL (Time To Live)**: Keep the cluster alive for sequential activities to avoid 5-min spin-up time per activity.\n\n---\n\n## Section 4: Advanced Scenarios & Logic\n\n### Q16: How to handle REST API Pagination?\n**Answer**:\nIn Copy Activity Source settings:\n1.  **Pagination Rules**: Define the logic (e.g., `AbsoluteUrl = $.nextLink` or `QueryParameter.page = Range`).\n2.  ADF loops automatically until the condition is met.\n\n### Q17: lookup activity vs Get Metadata activity?\n**Answer**:\n*   **Lookup**: Reads the *content* of a file/table (e.g., \"Get the last processed date\"). Limited to 5000 rows / 4MB.\n*   **Get Metadata**: Reads *attributes* of files (Size, Name, LastModified, ChildItems). Does not read content. Used for looping over file lists.\n\n### Q18: Difference between ForEach and Filter activity?\n**Answer**:\n*   **Filter**: Filters an input array based on a condition, outputting a smaller array.\n*   **ForEach**: Iterates over an array. Default runs in **Parallel** (up to 50 threads). Can be set to **Sequential**.\n\n### Q19: How to restart a pipeline from a specific failed point?\n**Answer**:\nBefore, you couldn't. Now, you can perform a **\"Rerun from failure\"** in the Monitor tab. It skips already succeeded activities and resumes state.\n\n### Q20: How to handle errors (Try-Catch logic)?\n**Answer**:\nADF doesn't have a Try-Catch block.\n*   Use the **onFailure** path (Red arrow) from an activity (e.g., Copy) to an error handling activity (e.g., Send Notification / Log Error).\n*   To ignore error and proceed, perform `onCompletion` or link `onFailure` to a dummy Wait activity.\n\n### Q21: Usage of \"Until\" Activity?\n**Answer**:\nDo-While loop equivalent. Runs activities until a condition evaluates to true.\n*   *Use Case*: Polling an API status endpoint waiting for `status == \"COMPLETED\"`.\n\n### Q22: Custom Activity vs Azure Function Activity?\n**Answer**:\n*   **Azure Function**: Good for lightweight logic (C#, Python, Node) running in Serverless mode. Max timeout 10 mins (Consumption).\n*   **Custom Activity**: Runs on an **Azure Batch** pool (VMs). Good for long-running, heavy computational scripts (e.g., proprietary executable).\n\n### Q23: How to pass values from a Child Pipeline to Parent Pipeline?\n**Answer**:\nNote: Execute Pipeline doesn't natively return variables.\n*   **Workaround**: Write the output to the database/file in Child, read it in Parent.\n*   **New Feature**: \"Set Pipeline Return Value\" activity in Child. Parent accesses it via `activity('ExecuteChild').output.pipelineReturnValue.key`.\n\n---\n\n## Section 5: Integration & Security\n\n### Q24: How to secure credentials?\n**Answer**:\nUse **Azure Key Vault**. Create a Linked Service to AKV, then inside data Linked Services, select \"Azure Key Vault\" for the password field and provide the Secret Name.\n\n### Q25: Managed Identity (MSI) in ADF?\n**Answer**:\nADF has its own AD Object ID.\n*   Grant this Object ID access to resources (e.g., \"Blob Data Contributor\" on Storage Account).\n*   Authentication happens via Azure Backbone without managing user/pass.\n\n### Q26: CI/CD in ADF?\n**Answer**:\n1.  **Git Integration**: Connect to Azure DevOps/GitHub.\n2.  **branches**: Develop in `main` or feature branch.\n3.  **Publish**: Generates **ARM Templates** in `adf_publish` branch.\n4.  **Release Pipeline**: Deploys these ARM templates to QA/Prod Resource Groups, overriding parameters (Connection Strings).\n\n### Q27: How to utilize \"Global Parameters\"?\n**Answer**:\nConstants defined at the Factory level (e.g., `EnvName`, `DataLakeURL`).\n*   Useful for CI/CD to override values across environments easily.\n*   Scoped properly rather than repeating pipeline parameters.\n\n### Q28: How to monitor pipelines externally?\n**Answer**:\n*   **Azure Monitor**: Send logs to Log Analytics Workspace.\n*   **Alerts**: Create rules for \"Failed Runs\" to email/SMS.\n*   **SDK**: Use Python/PowerShell SDK to query run status programmatically.\n\n---\n\n## Section 6: Real-World Scenarios\n\n### Q29: Incremental Loading pattern?\n**Answer**:\n1.  **Watermark Table**: Store `LastLoadDate` in SQL.\n2.  **Lookup**: Retrieve `LastLoadDate`.\n3.  **Copy**: Source Query `SELECT * FROM Source WHERE ModDate > @LastLoadDate`.\n4.  **Update**: On success, update Watermark Table with `MAX(ModDate)`.\n\n### Q30: Handling Schema Drift?\n**Answer**:\nIn Mapping Data Flow:\n*   Check **\"Allow Schema Drift\"** in Source/Sink.\n*   Use patterns like `byName()` or `byPosition()` to map columns dynamically.\n*   Use **\"Drifted Column\"** feature to process columns that weren't defined at design time.\n\n### Q31: How to execute a Databricks Notebook?\n**Answer**:\nUse **Databricks Notebook Activity**.\n*   Requires a Linked Service to Databricks Workspace (Token/MSI).\n*   Can pass parameters to Widgets in the notebook via `Base Parameters`.\n\n### Q32: Limit concurrency of a pipeline?\n**Answer**:\nIf a trigger runs every minute but pipeline takes 5 mins, they pile up.\n*   **Pipeline Concurrency**: Settings -> Concurrency. Set to 1 for Singleton behavior.\n\n### Q33: Secure Input/Output?\n**Answer**:\nIn Activity settings, check **\"Secure Input\"** and **\"Secure Output\"**. This prevents sensitive data (like passwords or PII tokens) from appearing in the ADF Monitor logs in plain text.\n\n### Q34: How to deduplicate data in ADF?\n**Answer**:\nUse **Mapping Data Flow**.\n1.  **Aggregate** Transformation.\n2.  Group By all columns (or Key columns).\n3.  Aggregate function: `first(col)`.\n\n### Q35: Copy data from REST API and add a timestamp column?\n**Answer**:\nUse **Additional Columns** feature in Copy Activity Source settings.\n*   Name: `IngestDate`\n*   Value: `$$NOW` (ADF System Variable).\n"}, {"name": "azure_functions.md", "type": "file", "content": "# Azure Functions Interview Questions\n\n## Q1: Consumption Plan vs. Premium Plan?\n**Answer**:\n*   **Consumption**: Pay-per-execution. Scales to 0. Cheapest.\n    *   *Issue*: **Cold Start**. If no requests for 20 mins, the app sleeps. Next request takes seconds to spin up.\n*   **Premium**: Pay for pre-warmed instances. No cold start. VNet connectivity supported.\n*   **Dedicated (App Service Plan)**: Run functions on a dedicated VM. Predictable cost/performance.\n\n## Q2: What are Triggers and Bindings?\n**Answer**:\n*   **Trigger**: What starts the function (e.g., HTTP Request, New Blob, Timer, CosmosDB Change). Only 1 trigger per function.\n*   **Binding**: Declarative way to connect input/output data.\n    *   *Example*: Use an \"Output Binding\" to write a result to Azure SQL. You don't need to write connection code (`SqlConnection.Open()`), the runtime handles it.\n\n## Q3: What are Durable Functions?\n**Answer**:\nAn extension to write **stateful** functions in a serverless environment.\n*   **Orchestrator Function**: Describes workflow in code (e.g., \"Wait for Activity A, then run B and C in parallel\").\n*   **Activity Function**: The worker.\n*   *Patterns*: Chaining, Fan-out/Fan-in, Monitor, Human Interaction (Wait for approval).\n\n## Q4: How long can a Function run?\n**Answer**:\n*   **Consumption**: Default 5 mins. Max 10 mins.\n*   **Premium/Dedicated**: Unlimited (technically, but usually limited by restarts/deployments).\n*   *Tip*: If you need > 10 mins, use Durable Functions or Batch/AKS.\n"}, {"name": "azure_log_analytics.md", "type": "file", "content": "# Azure Log Analytics Interview Questions\n\n## Q1: What is a Log Analytics Workspace?\n**Answer**:\nIt is the logical container for data used by Azure Monitor. It collects telemetry from various sources (VMs, Application Insights, Diagnostics).\n*   Configures data retention (e.g., 30 days vs 2 years).\n*   Scope for access control (Who can see logs).\n\n## Q2: What is KQL? Write a simple query.\n**Answer**:\n**Kusto Query Language**. Optimized for high-performance read-only queries on large datasets.\n*   *Query*: Find all Errors in the last hour.\n```kusto\nAppTraces\n| where TimeGenerated > ago(1h)\n| where SeverityLevel == \"Error\"\n| project TimeGenerated, Message, Component\n| order by TimeGenerated desc\n```\n\n## Q3: How do you connect a VM to Log Analytics?\n**Answer**:\nInstall the **Azure Monitor Agent (AMA)** (formerly Log Analytics Agent / MMA) on the VM.\n*   Configure Data Collection Rules (DCR) to specify which logs (Syslog, Windows Event Logs) to send to which Workspace.\n\n## Q4: How do Alerts work with Log Analytics?\n**Answer**:\nYou create a **Metric Alert** or **Log Search Alert**.\n1.  **Condition**: Run a KQL query every 5 mins. If `count() > 0`, trigger.\n2.  **Action Group**: Send SMS, Email, Call Webhook, or trigger Automation Runbook.\n"}, {"name": "azure_storage.md", "type": "file", "content": "# Azure Storage Interview Questions\n\n## Q1: Blob Storage vs. Data Lake Storage Gen2 (ADLS Gen2)?\n**Answer**:\n*   **Blob Storage**: Flat namespace. To emulate a folder `data/2023/file.csv`, it's just a long filename string. Renaming the `2023` folder requires rewriting all millions of files inside.\n*   **ADLS Gen2**: Hierarchical namespace (Real folders). Renaming a folder is an atomic O(1) metadata operation. Essential for Hadoop/Spark performance.\n\n## Q2: Explain Storage Access Tiers.\n**Answer**:\n1.  **Hot**: Highest storage cost, lowest access cost. Use for active data.\n2.  **Cool**: Lower storage cost, higher access cost. Min retention 30 days. Use for backups/short-term logs.\n3.  **Archive**: Lowest storage cost, very high retrieval cost. Data is offline. Rehydration takes hours. Min retention 180 days.\n\n## Q3: What is the difference between LRS, GRS, and RA-GRS?\n**Answer**:\n*   **LRS (Locally Redundant)**: 3 copies in 1 datacenter. Cheapest.\n*   **ZRS (Zone Redundant)**: 3 copies across 3 Availability Zones in 1 region. Survives datacenter fire.\n*   **GRS (Geo-Redundant)**: LRS in Primary Region + Async copy to LRS in Secondary Region (Paired Region). You cannot read secondary unless MSFT fails over.\n*   **RA-GRS (Read-Access Geo)**: Same as GRS, but you can *read* from secondary instantly.\n\n## Q4: What is a SAS Token?\n**Answer**:\n**Shared Access Signature**. A URI that grants restricted access rights to Azure Storage resources for a limited time interval.\n*   *Use Case*: Giving a user temporary access to upload a file directly to Blob without giving them the Account Key.\n"}, {"name": "core_services.md", "type": "file", "content": "# Azure Core Services for Data Engineering & DevOps\n\nThis guide covers essential Azure services frequently asked about in technical interviews.\n\n## 1. Azure App Service\n**Platform-as-a-Service (PaaS)** for hosting web applications, REST APIs, and mobile backends.\n\n*   **Key Features**:\n    *   **Fully Managed**: Auto-patching, load balancing, and scaling.\n    *   **Deployment Slots**: Create a \"Staging\" slot, deploy code, then swap with \"Production\" (Zero Downtime).\n    *   **Scaling**: Support for Manual (fixed count) or Auto-scaling (based on CPU/Memory).\n*   **Use Case**: Hosting a Python/Node.js API that serves your ML model or frontend.\n*   **Interview Tip**: Know the difference between **App Service Plan** (The computed resources/VM) and the **App Service** (The application running on it). Accessing App Service inside a VNet often requires \"VNet Integration\".\n\n## 2. Azure Kubernetes Service (AKS)\n**Managed Kubernetes** service that simplifies deploying and managing containerized applications.\n\n*   **Key Features**:\n    *   **Control Plane**: Managed by Azure (Free). You only pay for the Worker Nodes (VMs).\n    *   **Integration**: Native integration with Azure Container Registry (ACR) and Azure Active Directory (RBAC).\n    *   **Scaling**: Supports **Cluster Autoscaler** (adds nodes) and **Horizontal Pod Autoscaler** (adds pods).\n*   **Use Case**: Running microservices architectures or heavy distributed jobs (Spark on K8s).\n*   **Interview Tip**: Understand **Node Pools** (grouping VMs) and how **CNI Networking** allows pods to get real IP addresses from the VNet.\n\n## 3. Azure Storage (Blob & ADLS Gen2)\nThe fundamental storage layer for the cloud.\n\n*   **Types**:\n    *   **Blob Storage**: Object storage (Images, Logs, Backups). Flat namespace.\n    *   **Data Lake Storage Gen2 (ADLS Gen2)**: Built on top of Blob. Adds a **Hierarchical Namespace** (Folders/Directories). Critical for Hadoop/Spark performance.\n*   **Access Tiers**:\n    *   **Hot**: Frequent access (Higher storage cost, lower access cost).\n    *   **Cool**: Infrequent access (> 30 days).\n    *   **Archive**: Rare access (> 180 days). High latency to retrieve.\n*   **Use Case**: Landing zone for ETL pipelines (ADF -> ADLS).\n*   **Interview Tip**: Explain why ADLS Gen2 is better for Big Data than generic Blob (Atomic directory renames are O(1) in ADLS vs O(N) in Blob).\n\n## 4. Azure Functions\n**Serverless Compute** service. Run code snippets (Events) without managing infrastructure.\n\n*   **Key Features**:\n    *   **Triggers**: HTTP (API), Timer (Cron), Blob (New File), CosmosDB (Change Feed).\n    *   **Bindings**: Declarative way to connect to data (e.g., Output Binding to write to SQL) without boilerplate code.\n    *   **Plans**:\n        *   **Consumption**: Pay per execution. Cold start potential.\n        *   **Premium**: Pre-warmed instances (No cold start), VNet connectivity.\n*   **Use Case**: Lightweight event processing (image resizing when uploaded) or glue code between services.\n*   **Interview Tip**: Discuss **Durable Functions** for stateful workflows (e.g., Fan-out/Fan-in patterns).\n\n## 5. Log Analytics (Azure Monitor)\nCentralized repository for storing and querying logs.\n\n*   **Key Features**:\n    *   **KQL (Kusto Query Language)**: SQL-like language to query logs. Extremely fast.\n    *   **Sources**: Collects logs from VMs, AKS, App Services, and custom apps.\n    *   **Alerts**: Trigger emails/webhooks based on query results (e.g., \"Error count > 5\").\n*   **Use Case**: Debugging application crashes or monitoring resource usage across the entire fleet.\n*   **Interview Tip**: Know a basic KQL query: `AppTraces | where SeverityLevel == \"Error\" | summarize count() by bin(TimeGenerated, 1h)`.\n\n## 6. Azure Data Factory (ADF)\n**Cloud ETL Service** for data integration and orchestration.\n\n*   **Key Concepts**:\n    *   **Pipeline**: Logical grouping of activities.\n    *   **Activity**: A step (Copy Data, Run Databricks Notebook, Call API).\n    *   **Dataset**: Reference to data (e.g., \"CSV in Blob\").\n    *   **Linked Service**: Connection string (credentials) to external systems (e.g., connection to Snowflake).\n    *   **Integration Runtime (IR)**: The compute infrastructure.\n        *   *Azure IR*: Cloud-to-Cloud.\n        *   *Self-Hosted IR*: Connects to On-Premise data / Private VNet.\n*   **Use Case**: Orchestrating the \"E\" (Extract) and \"L\" (Load) from On-Prem SQL to Cloud Data Lake.\n*   **Interview Tip**: ADF is primarily an **Orchestrator**. It is not meant for heavy row-level transformation logic (use Databricks/Spark for that), although \"Data Flows\" in ADF provide a GUI for transformations.\n"}, {"name": "databricks.md", "type": "file", "content": "# Azure Databricks Interview Questions\n\n## Section 1: Architecture & Compute\n\n### Q1: Job Cluster vs All-Purpose Cluster?\n**Answer**:\n*   **All-Purpose Cluster**: Manually created for interactive analysis (Notebooks). Cost refers to \"Standard\" pricing. Can be shared by multiple users.\n*   **Job Cluster**: Created *automatically* when a Job starts and terminates when it ends. Significantly cheaper (approx 50% less) than All-Purpose. Always use this for production pipelines.\n\n### Q2: What are Instance Pools?\n**Answer**:\nA set of idle, pre-warmed instances (VMs). \n*   **Problem**: Starting a cluster takes 5-10 minutes (VM provisioning).\n*   **Solution**: Pools keep VMs ready. Cluster start-up time drops to < 10 seconds. You pay for the VMs (cloud cost) while they idle, but not the Databricks DBU cost.\n\n### Q3: What is the Photon Engine?\n**Answer**:\nA native vectorized execution engine written in C++ (instead of JVM).\n*   Replaces the traditional Spark Execution Engine.\n*   Speeds up SQL queries and DataFrame operations (filter, join, agg) by 2x-10x.\n*   Price is slightly higher but total TCO is lower due to faster completion.\n\n### Q4: Driver Node vs Worker Node?\n**Answer**:\n*   **Driver**: The \"Brain\". Maintains the SparkSession, translates code to DAGs, schedules tasks, and collects results (`collect()`). Memory overflow here causes OOM.\n*   **Worker**: The \"Muscle\". Executes tasks (Task logic) and stores data partitions.\n\n### Q5: Standard vs Premium Workspace?\n**Answer**:\n*   **Standard**: Basic Role-Based Access Control (RBAC). No fine-grained data security.\n*   **Premium**: Required for **Unity Catalog**, Credential Passthrough, Dynamic Partition Pruning, and Audit Logs.\n\n---\n\n## Section 2: Unity Catalog & Governance\n\n### Q6: What is Unity Catalog (UC)?\n**Answer**:\nA unified governance solution for Data & AI.\n*   **Centralized Metadata**: One Metastore for all workspaces in a region.\n*   **3-Level Namespace**: `Catalog.Schema.Table`.\n*   **Data Lineage**: Automatically tracks table/column level lineage.\n*   **Fine-grained ACL**: `GRANT SELECT ON TABLE sales TO group analysts`.\n\n### Q7: Managed Table vs External Table in UC?\n**Answer**:\n*   **Managed Table**: Data is stored in the \"Managed Storage Account\" (Root bucket) defined in the Metastore. Dropping the table *deletes* the underlying data files.\n*   **External Table**: Data is stored in an external storage container (e.g., ADLS Gen2). You provide the path. Dropping the table *only deletes metadata*; files remain.\n\n### Q8: What are Storage Credentials and External Locations?\n**Answer**:\n*   **Storage Credential**: Secure object holding specific Managed Identity / Service Principal details to access ADLS.\n*   **External Location**: Maps a Storage Path (`abfss://...`) to a Storage Credential. Used to govern *who* can create External Tables.\n\n### Q9: Difference between Hive Metastore and Unity Catalog?\n**Answer**:\n*   **Hive (Legacy)**: Workspace-local. Permissions defined at cluster level (unsafe). No lineage.\n*   **Unity Catalog**: Account-level. Permissions defined on data objects (SQL). Cross-workspace sharing.\n\n### Q10: How to share data across different Databricks accounts?\n**Answer**:\nUse **Delta Sharing**.\n*   An open protocol included in Unity Catalog.\n*   Allows sharing tables with external organizations (even if they don't use Databricks) securely without file replication.\n\n---\n\n## Section 3: Data Engineering Implementation\n\n### Q11: How to implement SCD Type 1 in Delta Lake?\n**Answer**:\nUse `MERGE INTO`.\n```sql\nMERGE INTO target t USING source s ON t.id = s.id\nWHEN MATCHED THEN UPDATE SET t.name = s.name, t.amount = s.amount\nWHEN NOT MATCHED THEN INSERT *\n```\n\n### Q12: How to implement SCD Type 2 (History)?\n**Answer**:\nUse `MERGE INTO` with advanced logic.\n1.  Mark old record as inactive (`UPDATE SET isActive = false, endDate = current_date()`).\n2.  Insert new record (`INSERT (id, name, isActive, startDate) VALUES (s.id, s.name, true, current_date())`).\n*   Requires matching on `t.id = s.id AND t.isActive = true`.\n\n### Q13: How to pass parameters to a Databricks Notebook?\n**Answer**:\nUse **Widgets**.\n*   **Create**: `dbutils.widgets.text(\"env\", \"dev\")`.\n*   **Read**: `env = dbutils.widgets.get(\"env\")`.\n*   **Pass from ADF**: In Databricks Activity -> Base Parameters -> Key: `env`, Value: `prod`.\n\n### Q14: What is a \"Bundle\" (Databricks Asset Bundles - DABs)?\n**Answer**:\nA tool to define infrastructure and code as project files (YAML) for CI/CD.\n*   Replaces the UI-based workflow deployment.\n*   Define Jobs, Pipelines, and Clusters as code.\n*   Deploy using CLI: `databricks bundle deploy -t prod`.\n\n### Q15: How to handle Schema Evolution?\n**Answer**:\n*   **Append Mode**: `option(\"mergeSchema\", \"true\")`. Adds new columns to the table schema automatically.\n*   **Schema Enforcement**: By default, Delta throws error if schema mismatches to prevent corruption.\n\n### Q16: How to delete old data to save cost?\n**Answer**:\nUse `VACUUM`.\n*   `VACUUM table_name RETAIN 168 HOURS;`\n*   Deletes physical files that are no longer referenced by the transaction log and are older than retention period (default 7 days).\n*   **Warning**: You cannot Time Travel back to versions before the Vacuum.\n\n---\n\n## Section 4: Performance Optimization\n\n### Q17: What is Z-Ordering?\n**Answer**:\nA technique to co-locate related information in the same set of files.\n*   `OPTIMIZE table_name ZORDER BY (col1, col2)`\n*   Used for columns frequently used in `WHERE` clauses (High cardinality like ID, Timestamp).\n*   Enables extremely effective **Data Skipping** (read 2 files instead of 100).\n\n### Q18: Partitioning vs Z-Ordering?\n**Answer**:\n*   **Partitioning**: Physical folder separation. Good for Low Cardinality (Date, Region). *Don't partition < 1TB tables*.\n*   **Z-Ordering**: File-internal organization. Good for High Cardinality (CustomerID). Use when tables are medium-sized or partition columns have too many unique values.\n\n### Q19: What is Disk Cache (formerly Delta Cache)?\n**Answer**:\nUses local SSDs on the Worker nodes to cache remote data (Parquet files) for faster subsequent reads.\n*   **Difference from Spark Cache**: Spark Cache (`.cache()`) stores data in RAM (and can OOM). Disk Cache stores on SSD (safe).\n*   Enabled by default on certain worker types (Standard_L series).\n\n### Q20: Broadcast Hash Join?\n**Answer**:\nOptimization for **Large Table x Small Table** join.\n*   Spark sends the small table to every executor node (Broadcast).\n*   Avoids \"Shuffling\" the large table (expensive network transfer).\n*   Hint: `df.join(F.broadcast(small_df), \"id\")`.\n\n### Q21: What is Skew Join optimization?\n**Answer**:\nWhen one key has way more data than others (e.g., \"NULL\" key).\n*   Spark splits the skewed partition into smaller sub-partitions.\n*   Hint: `option(\"skewJoin\", \"true\")` (Enabled by default in Databricks Runtime).\n\n---\n\n## Section 5: Advanced & General\n\n### Q22: Delta Lake vs Parquet?\n**Answer**:\nParquet is just the file format. Delta Lake is the **Transaction Log** (ACID) on top of Parquet.\n*   Delta adds: Time Travel, ACID Transactions, Schema Enforcement, Merge/Update support.\n\n### Q23: How to interact with Databricks using API?\n**Answer**:\n**Databricks REST API**.\n*   Manage clusters (create/delete), start jobs, upload files (DBFS).\n*   Auth: PAT Token or OAuth (Service Principal).\n*   Example: `POST /api/2.1/jobs/run-now` to start a job externally.\n\n### Q24: What is Auto Loader?\n**Answer**:\nEfficient way to ingest files incrementally from Cloud Storage.\n*   Uses `spark.readStream.format(\"cloudFiles\")`.\n*   Automatically detects new files using event notifications (Queue) instead of listing directories (slow).\n*   Handles Schema drift automatically (`cloudFiles.schemaLocation`).\n\n### Q25: Explain \"Optimize Write\"?\n**Answer**:\nAn automated shuffle before write to produce evenly sized files (e.g., 1GB each).\n*   Prevents \"Small File Problem\" during ingestion.\n\n### Q26: Secrets Management?\n**Answer**:\nUse **Secret Scopes**.\n*   **Azure Key Vault Backed Scope**: Maps directly to AKV.\n*   **Databricks Backed Scope**: Stored internally.\n*   Access: `dbutils.secrets.get(scope = \"my-scope\", key = \"db-pass\")`.\n*   Result is redacted as `[REDACTED]` in logs.\n\n### Q27: Delta Live Tables (DLT)?\n**Answer**:\nA framework for building reliable pipelines using declarative syntax (SQL/Python).\n*   Auto-manages infrastructure, retries, and dependencies.\n*   **Expectations**: Built-in data quality rules (`CONSTRAINT valid_id EXPECT id IS NOT NULL`).\n\n### Q28: How to clone a table?\n**Answer**:\n*   **Deep Clone**: Copies metadata + data files. Independent copy.\n*   **Shallow Clone**: Copies metadata only. Points to original files. Fast/Cheap backup.\n\n### Q29: What is the \"Transaction Log\" (_delta_log)?\n**Answer**:\nSingle source of truth. Folder containing JSON files (`000.json`, `001.json`). Actions: AddFile, RemoveFile, Metadata.\n*   Allows atomic reads (readers see snapshot X while writer creates snapshot X+1).\n\n### Q30: How to handle dependency libraries?\n**Answer**:\n1.  **Cluster Libraries**: Install on cluster start (PyPI/Maven).\n2.  **Notebook-scoped**: `%pip install pandas`. Isolate env for that notebook session.\n"}]}, {"name": "Cheatsheet", "type": "directory", "children": [{"name": "cloud_comparison.md", "type": "file", "content": "# Cloud Service Comparison Cheat Sheet\n\nA quick reference mapping services across the three major cloud providers for interview preparation.\n\n| Category | Service Function | **Microsoft Azure** | **Amazon Web Services (AWS)** | **Google Cloud Platform (GCP)** |\n| :--- | :--- | :--- | :--- | :--- |\n| **Compute** | Virtual Machines (IaaS) | Azure Virtual Machines | Amazon EC2 | Google Compute Engine |\n| | Platform as a Service (PaaS) | Azure App Service | AWS Elastic Beanstalk | Google App Engine |\n| | Serverless Functions | Azure Functions | AWS Lambda | Google Cloud Functions |\n| | Object Storage (Blob) | Azure Blob Storage | Amazon S3 | Google Cloud Storage |\n| **Storage** | Archive Storage | Blob Archive Tier | Amazon S3 Glacier | Archive Storage Class |\n| | File Storage (SMB/NFS) | Azure Files | Amazon EFS / FSx | Google Cloud Filestore |\n| | Disk Storage (Block) | Azure Managed Disks | Amazon EBS | Persistent Disk |\n| **Containers** | Managed Kubernetes | Azure Kubernetes Service (AKS) | Amazon EKS | Google Kubernetes Engine (GKE) |\n| | Container Registry | Azure Container Registry (ACR) | Amazon ECR | Google Container Registry (GCR) |\n| | Container Instances (Serverless) | Azure Container Instances (ACI) | AWS Fargate | Cloud Run (Stateless) |\n| **Database** | Relational (Managed) | Azure SQL Database / SQL Managed Instance | Amazon RDS / Aurora | Cloud SQL / Cloud Spanner |\n| | NoSQL (Document/Key-Value) | Azure Cosmos DB | Amazon DynamoDB | Cloud Firestore / Bigtable |\n| | In-Memory Caching | Azure Cache for Redis | Amazon ElastiCache | Cloud Memorystore |\n| **Data & Analytics** | Data Warehouse | Azure Synapse Analytics | Amazon Redshift | Google BigQuery |\n| | Big Data Processing (Hadoop/Spark) | Azure Databricks / HDInsight | Amazon EMR | Google Cloud Dataproc |\n| | ETL / Data Integration | Azure Data Factory (ADF) | AWS Glue | Cloud Dataflow / Data Fusion |\n| | Real-Time Streaming | Azure Event Hubs / Stream Analytics | Amazon Kinesis | Google Pub/Sub / Dataflow |\n| | Data Lake | Data Lake Storage Gen2 (ADLS) | S3 (Lake Formation) | Cloud Storage |\n| **Messaging** | Message Queue | Azure Service Bus | Amazon SQS | Google Pub/Sub |\n| | Pub/Sub Topic | Azure Service Bus Topics | Amazon SNS | Google Pub/Sub |\n| **Networking** | Virtual Network | Azure VNet | Amazon VPC | Google Cloud VPC |\n| | Load Balancing | Azure Load Balancer / App Gateway | ELB / ALB / NLB | Cloud Load Balancing |\n| | DNS Management | Azure DNS | Amazon Route 53 | Cloud DNS |\n| | Content Delivery Network | Azure CDN | Amazon CloudFront | Cloud CDN |\n| **DevOps & Mgmt** | Infrastructure as Code | ARM Templates / Bicep | AWS CloudFormation | Google Cloud Deployment Manager |\n| | Monitoring & Logging | Azure Monitor / Log Analytics | Amazon CloudWatch | Google Cloud Operations (Stackdriver) |\n| | Identity Management | Microsoft Entra ID (formerly Azure AD) | AWS IAM | Cloud IAM |\n| | Secret Management | Azure Key Vault | AWS Secrets Manager / Parameter Store | Secret Manager |\n\n## Key Differences for Interviews\n\n1.  **Scope**: Azure resources are grouped in **Resource Groups**, whereas AWS uses **Tags** loosely but relies on region/account boundaries. GCP uses **Projects**.\n2.  **Storage**: Azure ADLS Gen2 is unique because it combines Blob storage with a true *Hierarchical Filesystem* (folders), which S3 only emulates (until recently with S3 Express/Directory buckets).\n3.  **Hybrid**: Azure has strong hybrid focus with **Azure Arc** (manage on-prem K8s/Servers from cloud). AWS has **Outposts**. GCP has **Anthos**.\n"}, {"name": "coding_patterns_cheat_sheet.md", "type": "file", "content": "# Coding Interview Patterns Cheat Sheet\n\nA quick reference guide for identifying which coding pattern to use based on problem characteristics.\n\n## 1. Sliding Window\n**When to use:**\n- Input is a linear data structure (Array, String, Linked List).\n- You need to calculate something for a **contiguous** subarray or substring.\n- You are asked to find the longest/shortest substring, subarray, or specific target value.\n- keywords: \"maximum sum subarray of size 'K'\", \"longest substring with 'K' distinct characters\".\n\n**Why:** Avoids re-calculating the overlapping part of the window, reducing time complexity from O(N*K) to O(N).\n\n**Common Problems:**\n- Maximum Sum Subarray of Size K\n- Longest Substring with K Distinct Characters\n- String Anagrams\n- Minimum Window Substring\n\n## 2. Two Pointers\n**When to use:**\n- Input is a **sorted** array or linked list (usually).\n- You need to find a set of elements that fulfill certain constraints (e.g., sum to a target).\n- You are trying to find a pair, triplet, or subarray.\n- Palindrome checks (checking edges moving inwards).\n\n**Why:** Reduces nested loop complexity (O(N^2)) to linear time (O(N)) by processing elements from ends or different speeds.\n\n**Common Problems:**\n- Pair with Target Sum\n- Remove Duplicates from Sorted Array\n- Squaring a Sorted Array\n- 3Sum, 4Sum, Dutch National Flag Problem\n- Valid Palindrome\n\n## 3. Fast & Slow Pointers (Tortoise and Hare)\n**When to use:**\n- The data structure is linear (Array, Linked List).\n- You need to detect a **cycle** or find the middle element.\n- You need to find the start of a cycle.\n\n**Why:** Efficiently detects cycles and finds midpoints in a single pass with O(1) space, unlike using a Set/Map.\n\n**Common Problems:**\n- LinkedList Cycle\n- Middle of the LinkedList\n- Start of LinkedList Cycle\n- Happy Number\n\n## 4. Merge Intervals\n**When to use:**\n- You are given a set of intervals (start, end) or time ranges.\n- You need to find overlapping intervals, merge them, or find gaps.\n- Keywords: \"overlapping\", \"merge\", \"interval\", \"meeting times\".\n\n**Common Problems:**\n- Merge Intervals\n- Insert Interval\n- Intervals Intersection\n- Conflicting Appointments / Meeting Rooms\n\n## 5. Cyclic Sort\n**When to use:**\n- Input is an array containing numbers in a **given range** (e.g., 1 to N, 0 to N).\n- You need to find missing, duplicate, or corrupted numbers in that range.\n- You want O(N) time and O(1) space.\n\n**Common Problems:**\n- Cyclic Sort\n- Find the Missing Number\n- Find all Disappeared Numbers\n- Find the Duplicate Number\n- Set Mismatch\n\n## 6. In-place Reversal of a Linked List\n**When to use:**\n- You need to reverse a Linked List (or a sub-part of it).\n- Constraint: Do it in-place (O(1) space).\n\n**Common Problems:**\n- Reverse a Linked List\n- Reverse a Sub-list\n- Reverse every K-element Sub-list\n\n## 7. Tree BFS (Breadth-First Search)\n**When to use:**\n- You need to traverse a tree level-by-level (level order traversal).\n- You need to find the shortest path in an unweighted graph/tree.\n- Problems asking for \"levels\", \"averages of levels\", or \"connect nodes at same level\".\n\n**Technique:** Use a **Queue**.\n\n**Common Problems:**\n- Binary Tree Level Order Traversal\n- Reverse Level Order Traversal\n- Zigzag Traversal\n- Level Averages in a Binary Tree\n- Minimum Depth of a Binary Tree\n\n## 8. Tree DFS (Depth-First Search)\n**When to use:**\n- You need to explore as deep as possible before backtracking.\n- You need to search for a node that is likely far from the root or requires visiting all nodes (like tree path sums).\n- In-order, Pre-order, Post-order traversals.\n\n**Technique:** Use Recursion or a **Stack**.\n\n**Common Problems:**\n- Path Sum (I, II, III)\n- Sum of Path Numbers\n- Count Paths for a Sum\n- Diameter of Binary Tree\n\n## 9. Two Heaps\n**When to use:**\n- You need to divide a set of numbers into two parts (e.g., smaller half and larger half) to find the **median** or other order statistics dynamically.\n- Priority Queue scheduling problems (Min-heap for one, Max-heap for other).\n\n**Common Problems:**\n- Find the Median of a Number Stream\n- Sliding Window Median\n- Maximize Capital\n\n## 10. Subsets (Backtracking / BFS)\n**When to use:**\n- You need to find all **permutations**, **combinations**, or **subsets** of a set.\n- The input size is generally small (since these are exponential algorithms).\n\n**Common Problems:**\n- Subsets / Subsets II\n- Permutations\n- Letter Case Permutation\n- Generate Parentheses\n\n## 11. Modified Binary Search\n**When to use:**\n- Input is a **sorted** array (or almost sorted / rotated sorted).\n- You need to find a target value, an insertion position, or a boundary.\n- Time complexity constraint is O(log N).\n\n**Common Problems:**\n- Order-agnostic Binary Search\n- Ceiling of a Number\n- Next Letter\n- Search in a Sorted Infinite Array\n- Search in Rotated Sorted Array\n\n## 12. Top 'K' Elements\n**When to use:**\n- You need to find the top/smallest/most frequent 'K' elements.\n- Keywords: \"top K\", \"smallest K\", \"most frequent\".\n\n**Technique:** Use a **Heap** (Min-Heap for top K largest, Max-Heap for top K smallest).\n\n**Common Problems:**\n- Top K Frequent Elements\n- Kth Largest Element in a Stream\n- 'K' Closest Points to the Origin\n- Connect Ropes\n\n## 13. K-way Merge\n**When to use:**\n- You have 'K' sorted arrays, linked lists, or matrices and you need to merge them or find the specific element in the combined sorted order.\n\n**Technique:** Use a **Min-Heap** to keep track of the smallest element from each of the K structures.\n\n**Common Problems:**\n- Merge K Sorted Lists\n- Kth Smallest Number in M Sorted Lists\n- Smallest Number Range\n\n## 14. Topological Sort (Graph)\n**When to use:**\n- The problem deals with tasks that have **dependencies**.\n- You need to order nodes in a Directed Acyclic Graph (DAG) such that for every directed edge U -> V, node U comes before V.\n- Keywords: \"prerequisites\", \"scheduling\", \"course schedule\".\n\n**Common Problems:**\n- Task Scheduling / Course Schedule\n- Alien Dictionary\n- All Tasks Scheduling Orders\n\n## 15. Dynamic Programming (DP)\n**When to use:**\n- The problem has **overlapping subproblems** and **optimal substructure**.\n- You are asked for a maximum/minimum result, or the number of ways to do something.\n- \"Optimization\" problems.\n\n**Technique:** Memoization (Top-Down) or Tabulation (Bottom-Up).\n\n**Common Problems:**\n- 0/1 Knapsack\n- Unbounded Knapsack\n- Fibonacci Numbers\n- Longest Common Subsequence\n- Longest Palindromic Subsequence\n\n## 16. Hash Maps\n**When to use:**\n- You need direct access to data (O(1) lookup).\n- You need to track frequencies or pair elements.\n- Often used in conjunction with other patterns (e.g., Two Sum with Map, Sliding Window with Map).\n\n**Common Problems:**\n- Two Sum\n- Isomorphic Strings\n- Longest Palindrome\n\n## 17. Monotonic Stack\n**When to use:**\n- You need to find the \"next greater\" or \"next smaller\" element for every element in an array.\n- Optimizing nested loops that look for nearest values.\n\n**Common Problems:**\n- Next Greater Element\n- Daily Temperatures\n- Largest Rectangle in Histogram\n"}, {"name": "common_coding_topics.md", "type": "file", "content": "Here is a list of the most common coding questions frequently asked by big tech companies like Google, Amazon, and Microsoft, categorized for easy reference:\n\n1. Arrays & Hashing\nTwo Sum / 3Sum\n\nContains Duplicate\n\nValid Anagram / Group Anagrams\n\nTwo Sum II (Sorted Array)\n\nTop K Frequent Elements\n\nProduct of Array Except Self\n\nLongest Consecutive Sequence\n\nValid Sudoku\n\n2. Strings & Substrings\nValid Palindrome / Longest Palindromic Substring\n\nLongest Substring Without Repeating Characters\n\nLongest Repeating Character Replacement\n\nMinimum Window Substring\n\nReverse String / Reverse Words in a String\n\nString to Integer (atoi)\n\nImplement strStr()\n\n3. Linked Lists\nReverse Linked List\n\nDetect Cycle in a Linked List (Floyd's Cycle-Finding)\n\nMerge Two Sorted Lists / Merge K Sorted Lists\n\nRemove Nth Node From End of List\n\nReorder List\n\nIntersection of Two Linked Lists\n\n4. Trees & Graphs\nInvert Binary Tree\n\nMaximum Depth of Binary Tree\n\nValidate Binary Search Tree\n\nBinary Tree Level Order Traversal\n\nLowest Common Ancestor (LCA)\n\nNumber of Islands\n\nCourse Schedule\n\nClone Graph\n\n5. Dynamic Programming & Optimization\nClimbing Stairs\n\nCoin Change\n\nLongest Increasing Subsequence\n\nMaximum Subarray (Kadane\u2019s Algorithm)\n\nHouse Robber\n\nWord Break\n\n0/1 Knapsack\n\n6. Search & Sorting\nBinary Search\n\nSearch in Rotated Sorted Array\n\nMedian of Two Sorted Arrays\n\nMerge Intervals / Insert Interval\n\nKth Largest Element in an Array"}, {"name": "data_engineering_tech_stack.md", "type": "file", "content": "# Data Engineering Technology Stack\n\nA categorized list of trending tools, languages, and frameworks used in modern Data Engineering projects.\n\n## 1. Programming Languages\n*   **Python**: The dominant language (Airflow, PySpark, Pandas).\n*   **SQL**: Essential for transformation (dbt, Warehouse queries).\n*   **Scala**: Native language of Spark, used for high-performance streaming.\n*   **Java**: The foundation of the Hadoop ecosystem (Kafka, Flink).\n*   **Rust / Go**: Emerging for high-performance tooling (Delta Lake Rs, Redpanda).\n\n## 2. Distributed Storage & Formats\n*   **File Formats**:\n    *   **Parquet**: Columnar, high compression, ideal for OLAP/Spark.\n    *   **Avro**: Row-based, schema evolution support, ideal for Streaming/Kafka.\n    *   **ORC**: Optimized Row Columnar (often used with Hive).\n    *   **JSON / CSV**: Common for interchange (landing zone).\n*   **Table Formats (Lakehouse)**:\n    *   **Delta Lake**: ACID transactions, Time Travel (Databricks).\n    *   **Apache Iceberg**: Open standard, partition evolution (Netflix).\n    *   **Apache Hudi**: Upserts/Incremental processing (Uber).\n*   **Object Storage**:\n    *   **AWS S3**\n    *   **Azure Data Lake Storage Gen2 (ADLS)**\n    *   **Google Cloud Storage (GCS)**\n\n## 3. Streaming & Real-Time\n*   **Apache Kafka**: The standard for event streaming.\n*   **Confluent Platform**: Managed Kafka with Schema Registry, Connectors.\n*   **Apache Flink**: Stateful stream processing (low latency).\n*   **Spark Structured Streaming**: Micro-batch processing.\n*   **Azure Event Hubs / AWS Kinesis / GCP Pub/Sub**: Cloud-native alternatives.\n\n## 4. Big Data Processing (Compute)\n*   **Apache Spark**: Unified engine for big data processing (Batch + Stream).\n*   **Databricks**: Managed Spark platform + Lakehouse.\n*   **Snowflake**: Cloud Data Warehouse (Separated Compute/Storage).\n*   **Google BigQuery**: Serverless DWH.\n*   **Azure Synapse Analytics**: Integrated Analytics (SQL + Spark).\n*   **Hadoop (MapReduce/Hive)**: Legacy but still present in on-prem.\n\n## 5. ETL Workflows & Orchestration\n*   **Apache Airflow**: Python-based DAGs, industry standard.\n*   **Azure Data Factory (ADF)**: GUI-based cloud ETL.\n*   **Informatica / Talend**: Enterprise GUI ETL tools.\n*   **dbt (data build tool)**: SQL-based transformation (T in ELT).\n*   **Dagster / Prefect**: Next-gen orchestrators (Data-aware).\n\n## 6. Databases (OLTP & NoSQL)\n*   **PostgreSQL / MySQL**: Operational Relational DBs.\n*   **Apache Cassandra / ScyllaDB**: Wide-column store for high write throughput.\n*   **MongoDB**: Document store (JSON).\n*   **Redis**: In-memory cache / Key-Value store.\n*   **Elasticsearch / OpenSearch**: Search engine and log analytics.\n*   **Neo4j**: Graph database.\n\n## 7. Infrastructure as Code (IaC)\n*   **Terraform**: Cloud-agnostic infrastructure provisioning.\n*   **Ansible**: Configuration management.\n*   **Azure Bicep / ARM Templates**: Azure-native IaC.\n*   **AWS CloudFormation**: AWS-native IaC.\n*   **Pulumi**: IaC using general-purpose languages (Python/TS).\n\n## 8. CI/CD (DevOps)\n*   **Jenkins**: The classic open-source CI server.\n*   **GitHub Actions**: Integrated CI/CD in GitHub.\n*   **Azure DevOps**: Pipelines, Boards, Repos (Enterprise favorite).\n*   **GitLab CI**: Integrated DevOps platform.\n\n## 9. Containerization & Virtualization\n*   **Docker**: Container runtime.\n*   **Kubernetes (K8s)**: Container orchestration (AKS/EKS/GKE).\n*   **Helm**: Package manager for Kubernetes.\n\n## 10. Data Governance & Quality\n*   **Great Expectations**: Python library for data testing.\n*   **Amundsen / DataHub**: Data Catalog and discovery.\n*   **Apache Atlas**: Metadata management.\n"}, {"name": "databricks_interview_cheat_sheet.md", "type": "file", "content": "# Databricks Interview Cheat Sheet\n\nA quick reference guide for Databricks architecture, features, and optimization techniques.\n\n## 1. The Lakehouse Architecture\n**Concept:** Combines the best elements of Data Lakes (low cost, flexibility, supports unstructured data) and Data Warehouses (ACID transactions, schema enforcement, high performance).\n\n### Medallion Architecture (Multi-hop)\n- **Bronze Layer (Raw):**\n    - Raw data ingestion (append-only).\n    - Stores unvalidated data \"as-is\".\n    - Often usually JSON, Parquet, or CSV.\n- **Silver Layer (Curated/Enriched):**\n    - Cleaned, filtered, and augmented data.\n    - Schema validation, deduplication, and joins happen here.\n    - Single source of truth.\n- **Gold Layer (Aggregated):**\n    - Business-level aggregates for reporting and dashboards.\n    - Modeled for performance (Star/Snowflake schema).\n    - Ready for consumption by BI tools (PowerBI, Tableau).\n\n## 2. Delta Lake\n**Definition:** An open-source storage layer that brings reliability to data lakes. It sits on top of object storage (S3/ADLS/GCS).\n\n**Key Features:**\n- **ACID Transactions:** Ensures data integrity (Atomicity, Consistency, Isolation, Durability). Parallel reads/writes via Optimistic Concurrency Control.\n- **Time Travel:** Query data at a specific point in time (using `VERSION AS OF` or `TIMESTAMP AS OF`). Useful for auditing and rollbacks.\n- **Schema Enforcement & Evolution:**\n    - *Enforcement:* Rejects writes that don't match the schema.\n    - *Evolution:* Allows schema updates (add columns) automatically using `.option(\"mergeSchema\", \"true\")`.\n- **Merge (Upsert):** Efficently insert, update, and delete data using `MERGE INTO`.\n\n## 3. Unity Catalog & Governance\n**Definition:** A unified governance solution for data and AI assets across all workspaces.\n\n**Hierarchy:**\n1.  **Metastore:** The top-level container (usually one per region).\n2.  **Catalog:** The first level (e.g., `prod`, `dev`).\n3.  **Schema (Database):** Contains tables, views, functions.\n4.  **Table/Volume/Model:** The actual assets.\n\n**Key Features:**\n- **Centralized Access Control (ACLs):** Manage permissions in one place using standard SQL (`GRANT SELECT ON TABLE...`).\n- **Data Lineage:** Automatically tracks how data flows from source to target (Table-level and Column-level).\n- **Audit Logs:** Tracks who accessed what data.\n\n## 4. Performance Optimization\n**Techniques to speed up queries and jobs:**\n\n- **Photon Engine:** Native vectorized execution engine written in C++ for extreme performance on SQL and DataFrame API calls.\n- **Z-Ordering (Z-Order Clustering):** Co-locates related information in the same set of files. Used with `OPTIMIZE`.\n    - Best for: Columns frequently used in `WHERE` clauses (filters).\n    - *Command:* `OPTIMIZE table_name ZORDER BY (col1, col2)`\n- **Liquid Clustering:** Newer, dynamic clustering that replaces Z-Order/Partitioning. Automatically adjusts data layout.\n- **Partitioning:** (Legacy/Large Scale) Physically splitting data into directories by column (e.g., Date). avoid over-partitioning (small files problem).\n- **Auto Optimize:** Automatically compacts small files during writes.\n\n## 5. Streaming (Structured Streaming)\n**Auto Loader (`cloudFiles`):**\n- Efficiently ingests millions of files from cloud storage as they arrive.\n- Uses file notification events (SNS/SQS/Event Grid) or directory listing.\n- *Schema Inference:* Automatically detects schema changes and \"rescues\" bad data (`_rescued_data` column).\n\n**Delta Live Tables (DLT):**\n- Declarative framework for building reliable ETL pipelines.\n- Automates infrastructure management, dependency resolution (DAGs), and quality checks (Expectations).\n- *Expectation:* `CONSTRAINT valid_id EXPECT (id IS NOT NULL) ON VIOLATION DROP ROW`\n\n## 6. Common Interview Questions\n1.  **Map vs FlatMap?**\n    - Map: 1 input -> 1 output.\n    - FlatMap: 1 input -> 0 or more outputs (flattening).\n2.  **Narrow vs Wide Transformations?**\n    - *Narrow:* Data stays in same partition (e.g., `filter`, `map`). Fast.\n    - *Wide:* Data shuffles across network (e.g., `groupBy`, `join`, `distinct`). Slower.\n3.  **Coalesce vs Repartition?**\n    - *Coalesce:* Decreases partitions. No shuffle (mostly). Efficient.\n    - *Repartition:* Increases or decreases. Full shuffle. Balanced distribution.\n4.  **What is the \"Small File Problem\"?**\n    - Too many small files cause metadata overhead and slow listing.\n    - *Fix:* `OPTIMIZE`, `VACUUM`, Auto Optimize.\n"}, {"name": "industry_use_cases.md", "type": "file", "content": "# Industry-Specific Tech Stacks & Use Cases\n\nA guide to understanding *why* certain technologies are preferred in specific sectors, with example pipeline flows.\n\n## 1. Banking & Fintech (Security & Consistency)\n**Top Priority**: ACID Compliance, Data Security (PII), Low Latency Fraud Detection.\n*   **Preferred Tech**: Oracle/PostgreSQL (Strong transactional support), Kafka (Reliable logs), On-prem HDFS (sometimes required by regulation).\n\n### Use Case: Real-time Fraud Detection\n`ATM Transaction` -> `Kafka (Secure Topic)` -> `Apache Flink (Pattern Matching)` -> `Oracle DB (Block Account)` -> `Tableau (Alert Dashboard)`\n\n*   **Why?**\n    *   **Flink**: Low latency (<100ms) is critical to stop the card swipe.\n    *   **Oracle**: Banks trust its durability and row-locking mechanisms.\n\n## 2. Telecom (Volume & Batch)\n**Top Priority**: Handling massive throughput (Call Detail Records - CDRs), Cost-effective long-term storage.\n*   **Preferred Tech**: Hadoop/Spark (Batch), NoSQL (HBase/Cassandra), Columnar DBs (Vertica/Sybase IQ).\n\n### Use Case: Daily Billing & Network Quality\n`Cell Towers` -> `Flume/FTP` -> `HDFS (Raw Files)` -> `Spark (Optimization/Aggregates)` -> `SAP IQ (Columnar DB)` -> `Power BI`\n\n*   **Why?**\n    *   **Spark**: Best for crunching Petabytes of daily logs.\n    *   **SAP IQ / Vertica**: Highly optimized for analytical queries on billions of rows (Billing reports).\n\n## 3. Healthcare (Privacy & Unstructured Data)\n**Top Priority**: HIPAA Compliance, Handling Images (X-Rays)/Text (Doctor Notes).\n*   **Preferred Tech**: Databricks (Delta Lake), Azure Data Lake (HIPAA certified), NLP libraries.\n\n### Use Case: Patient 360 & Diagnosis Prediction\n`EHR System (HL7 msgs)` -> `Azure Data Factory` -> `ADLS Gen2 (Bronze)` -> `Databricks (Spark+NLP)` -> `Delta Lake (Gold)` -> `MLflow Model`\n\n*   **Why?**\n    *   **Delta Lake**: Supports \"Right to be Forgotten\" (DELETE specific user data GDPR/HIPAA).\n    *   **Unstructured Support**: Spark supports binary files (MRI scans) and text better than a Warehouse like Snowflake.\n\n## 4. Social Media (Relationships & Real-Time)\n**Top Priority**: Graph connections, Infinite scale, Global availability.\n*   **Preferred Tech**: Graph DB (Neo4j), Wide-column (Cassandra), Real-time stream (Kafka).\n\n### Use Case: \"People You May Know\"\n`User Graph` -> `Graph Processing (Giraph/Spark GraphX)` -> `Cassandra (Pre-computed Recs)` -> `API`\n\n*   **Why?**\n    *   **Neo4j**: Native support for query `(User)-[:FRIEND]->(User)`. SQL Joins fail here.\n    *   **Cassandra**: Write-heavy. Great for storing feed data replicated globally.\n\n## 5. IoT & Manufacturing (Time Series & High Frequency)\n**Top Priority**: High write ingestion (millions of sensors), Time-series compression.\n*   **Preferred Tech**: MQTT (Protocol), InfluxDB/TimescaleDB, Kafka.\n\n### Use Case: Predictive Maintenance (Factory)\n`Sensor (Vibration)` -> `MQTT` -> `Kafka` -> `InfluxDB` -> `Grafana Dashboard`\n\n*   **Why?**\n    *   **MQTT**: Lightweight protocol for small sensors with bad internet.\n    *   **InfluxDB**: Specialized for Time-Series. Compresses `timestamp, value` data 100x better than SQL.\n\n## 6. Video Streaming (Blob & CDN)\n**Top Priority**: Bandwidth management, Binary storage, Global delivery.\n*   **Preferred Tech**: Object Storage (S3), CDN (Akamai/CloudFront), Serverless (Lambda).\n\n### Use Case: Video Transcoding\n`Upload` -> `S3 (Trigger)` -> `AWS Lambda (Run Ffmpeg)` -> `S3 (MP4 output)` -> `CDN Origin`\n\n*   **Why?**\n    *   **S3**: Infinite scalability for binary files.\n    *   **Lambda**: Auto-scales to handle 1 video or 1000 concurrent uploads. No idle servers.\n\n## 7. Retail / E-Commerce (Customer 360)\n**Top Priority**: Connecting Silos (Sales, Marketing, Inventory), Analytics.\n*   **Preferred Tech**: Snowflake (Data Sharing), dbt (Transformation), Airflow.\n\n### Use Case: Marketing Attribution\n`Shopify (Sales)` + `Facebook Ads` + `Google Analytics` -> `Fivetran` -> `Snowflake` -> `dbt` -> `Looker`\n\n*   **Why?**\n    *   **Snowflake**: Great for complex joins across different schemas (JSON support).\n    *   **dbt**: Allows analysts (SQL users) to build their own pipelines without engineering help.\n"}, {"name": "leetcode_code_templates.md", "type": "file", "content": "# LeetCode Problem Solving & Code Templates\n\nA guide on **how to approach** problems and **reusable code templates** for efficient solving.\n\n## 0. Quick Pattern Reference\nA mapping of common problems to the standard pattern used to solve them.\n\n| Problem Type | Example Problem | Recommended Pattern |\n| :--- | :--- | :--- |\n| **Two Sum** | \"Find pair summing to target\" | **Hash Map** (if unsorted) or **Two Pointers** (if sorted) |\n| **Three Sum** | \"Find triplets summing to zero\" | **Two Pointers** (Sort array first) |\n| **Anagram** | \"Check if s is anagram of t\" | **Hash Map** (Frequency Count) or **Sorting** ($O(N \\log N)$) |\n| **Duplicate** | \"Find duplicates in array\" | **Hash Set** or **Cyclic Sort** ($O(1)$ Space) |\n| **Subarray Sum** | \"Smallest subarray with sum $\\ge S$\" | **Sliding Window** (Dynamic size) |\n| **Palindrome** | \"Longest Palindromic Substring\" | **Expand Around Center** (Two Pointers) |\n| **Top K** | \"Find K largest elements\" | **Min-Heap** (Size K) |\n| **Merge Intervals** | \"Merge overlapping intervals\" | **Sorting** (by Start time) + **Iteration** |\n| **Cycle Detection** | \"Detect cycle in Linked List\" | **Fast & Slow Pointers** |\n| **Dependencies** | \"Course Schedule\" | **Topological Sort** (Kahn's or DFS) |\n| **Islands** | \"Number of Islands\" | **DFS** (Recursive) or **BFS** (Queue) |\n| **Shortest Path** | \"Shortest path in maze\" | **BFS** (Unweighted) or **Dijkstra** (Weighted) |\n\n---\n\n\n## 1. Constraint Analysis (Time Complexity)\nAlways look at the input size constraints ($N$) to guess the required time complexity.\n\n| Input Size ($N$) | Required Time Complexity | Likely Algorithm |\n| :--- | :--- | :--- |\n| $\\le 10$ | $O(N!)$ or $O(2^N)$ | Backtracking, Recursion |\n| $\\le 20$ | $O(2^N)$ | Backtracking, Power Set |\n| $\\le 500$ | $O(N^3)$ | DP (3 states), Floyd-Warshall |\n| $\\le 2,000$ | $O(N^2)$ | DP (2 states), Selection Sort, All-pairs check |\n| $\\le 10^5$ | $O(N \\log N)$ or $O(N)$ | Sort, Heap, Binary Search, 2-Pointers |\n| $\\le 10^6$ | $O(N)$ | Hash Map, Two Pointers, Sliding Window |\n| $\\le 10^{18}$ | $O(1)$ or $O(\\log N)$ | Math, Binary Search |\n\n---\n\n## 2. Universal Code Templates\n\n### A. Binary Search (Find Target / Boundary)\n```python\ndef binary_search(nums, target):\n    left, right = 0, len(nums) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if nums[mid] == target:\n            return mid\n        elif nums[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n```\n\n### B. DFS (Depth First Search) - Grid\n```python\ndef dfs(r, c, grid, visited):\n    if (r < 0 or c < 0 or r >= len(grid) \n        or c >= len(grid[0]) or (r,c) in visited \n        or grid[r][c] == '0'):\n        return\n    \n    visited.add((r, c))\n    \n    # Visit 4 neighbors\n    dfs(r+1, c, grid, visited)\n    dfs(r-1, c, grid, visited)\n    dfs(r, c+1, grid, visited)\n    dfs(r, c-1, grid, visited)\n```\n\n### C. BFS (Breadth First Search) - Shortest Path\n```python\nfrom collections import deque\n\ndef bfs_shortest_path(start, target, grid):\n    rows, cols = len(grid), len(grid[0])\n    queue = deque([(start[0], start[1], 0)]) # r, c, dist\n    visited = set([(start[0], start[1])])\n    directions = [(0,1), (0,-1), (1,0), (-1,0)]\n    \n    while queue:\n        r, c, dist = queue.popleft()\n        \n        if (r, c) == target:\n            return dist\n            \n        for dr, dc in directions:\n            nr, nc = r + dr, c + dc\n            if (0 <= nr < rows and 0 <= nc < cols \n                and (nr, nc) not in visited and grid[nr][nc] != 'X'):\n                visited.add((nr, nc))\n                queue.append((nr, nc, dist + 1))\n    return -1\n```\n\n### D. Union-Find (Disjoint Set) - Cycle Detection / Components\n```python\nclass UnionFind:\n    def __init__(self, n):\n        self.parent = list(range(n))\n        self.rank = [1] * n\n        self.count = n # Number of connected components\n        \n    def find(self, p):\n        if self.parent[p] != p:\n            self.parent[p] = self.find(self.parent[p]) # Path compression\n        return self.parent[p]\n        \n    def union(self, p, q):\n        rootP = self.find(p)\n        rootQ = self.find(q)\n        if rootP != rootQ:\n            # Union by rank\n            if self.rank[rootP] > self.rank[rootQ]:\n                self.parent[rootQ] = rootP\n            elif self.rank[rootP] < self.rank[rootQ]:\n                self.parent[rootP] = rootQ\n            else:\n                self.parent[rootQ] = rootP\n                self.rank[rootP] += 1\n            self.count -= 1\n            return True\n        return False\n```\n\n### E. Trie (Prefix Tree) - STRING Search\n```python\nclass TrieNode:\n    def __init__(self):\n        self.children = {}\n        self.is_end = False\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()\n\n    def insert(self, word):\n        node = self.root\n        for char in word:\n            if char not in node.children:\n                node.children[char] = TrieNode()\n            node = node.children[char]\n        node.is_end = True\n\n    def search(self, word):\n        node = self.root\n        for char in word:\n            if char not in node.children:\n                return False\n            node = node.children[char]\n        return node.is_end\n```\n\n### F. Topological Sort (Kahn's Algorithm)\nUsed for Course Schedule problems (Dependency resolution).\n```python\nfrom collections import deque, defaultdict\n\ndef topological_sort(numCourses, prerequisites):\n    adj = defaultdict(list)\n    in_degree = [0] * numCourses\n    \n    # Build Graph\n    for dest, src in prerequisites:\n        adj[src].append(dest)\n        in_degree[dest] += 1\n        \n    # BFS\n    queue = deque([i for i in range(numCourses) if in_degree[i] == 0])\n    result = []\n    \n    while queue:\n        node = queue.popleft()\n        result.append(node)\n        for neighbor in adj[node]:\n            in_degree[neighbor] -= 1\n            if in_degree[neighbor] == 0:\n                queue.append(neighbor)\n                \n    return result if len(result) == numCourses else [] # Empty if cycle detected\n```\n\n### G. Dijkstra's Algorithm (Shortest Path in Weighted Graph)\n```python\nimport heapq\n\ndef dijkstra(n, edges, start_node):\n    adj = defaultdict(list)\n    for u, v, w in edges:\n        adj[u].append((v, w))\n        \n    min_heap = [(0, start_node)] # (cost, node)\n    shortest_dist = {}\n    \n    while min_heap:\n        w1, n1 = heapq.heappop(min_heap)\n        \n        if n1 in shortest_dist:\n            continue\n        shortest_dist[n1] = w1\n        \n        for n2, w2 in adj[n1]:\n            if n2 not in shortest_dist:\n                heapq.heappush(min_heap, (w1 + w2, n2))\n                \n    return shortest_dist\n```\n"}, {"name": "leetcode_coding_categories.md", "type": "file", "content": "This is a comprehensive list of the core building blocks for computer science and competitive programming. To make this manageable, I\u2019ve grouped them into logical categories based on how they are typically applied.\n\n---\n\n## 1. Core Data Structures\n\nThese categories focus on how data is organized and accessed.\n\n* **String:** Problems involving text manipulation, such as checking for palindromes or reversing words.\n* **Hash Table / Hash Function:** Using Key-Value pairs for  lookup. Essential for frequency counting or finding duplicates.\n* **Linked List / Doubly-Linked List:** Sequential data structures where elements point to the next (and previous).\n* **Stack / Queue:** Last-In-First-Out (LIFO) and First-In-First-Out (FIFO) structures.\n* **Tree / Binary Tree / Binary Search Tree (BST):** Hierarchical structures. BSTs keep data sorted for fast search/insertion.\n* **Heap (Priority Queue):** Automatically keeps the smallest or largest element at the top.\n* **Trie:** A \"Prefix Tree\" used for efficient string searches (like autocomplete).\n* **Design / Iterator:** Building your own data structures (e.g., \"Design a Least Recently Used (LRU) Cache\").\n\n---\n\n## 2. Fundamental Algorithms & Techniques\n\nThese are the \"how-to\" methods for solving problems.\n\n* **Sorting:** Arranging data (Merge Sort, Quickselect, Counting Sort, Radix Sort, Bucket Sort).\n* **Two Pointers:** Using two indices to traverse an array (often from both ends) to find a pair or range.\n* **Sliding Window:** A sub-segment of an array that \"slides\" to find a specific range (e.g., longest substring without repeating characters).\n* **Prefix Sum:** Pre-calculating sums of array segments to answer range sum queries in .\n* **Recursion:** A function calling itself to solve smaller versions of the same problem.\n* **Divide and Conquer:** Breaking a problem into two halves, solving them, and merging (like Merge Sort).\n* **Binary Search:** Searching a **sorted** range by repeatedly halving the search area.\n\n---\n\n## 3. Optimization & Decision Making\n\nThese are the heavy hitters for finding the \"best\" or \"most efficient\" solution.\n\n* **Dynamic Programming (DP):** Breaking a problem into overlapping subproblems and storing results (Memoization).\n* **Greedy:** Making the locally optimal choice at each step with the hope of finding the global optimum.\n* **Backtracking:** \"Trial and error\" approach. If a path doesn't work, you go back and try a different one (e.g., Sudoku solvers).\n* **Bitmask DP:** Using bits (0s and 1s) to represent states in DP, often for \"subset\" problems.\n\n---\n\n## 4. Graph & Advanced Search\n\nUsed when data is interconnected (like social networks or maps).\n\n* **Depth-First Search (DFS) / Breadth-First Search (BFS):** The two primary ways to traverse graphs and trees.\n* **Graph Theory:** General problems involving nodes and edges.\n* **Union-Find (Disjoint Set Union):** Tracking elements split into non-overlapping sets; great for detecting cycles.\n* **Shortest Path:** Finding the quickest route between nodes (Dijkstra, Bellman-Ford).\n* **Topological Sort:** Ordering tasks that have dependencies (e.g., \"Course Schedule\").\n* **Minimum Spanning Tree:** Connecting all nodes with the minimum total edge weight.\n* **Strongly Connected Components:** Groups of nodes where every node is reachable from every other node in the group.\n\n---\n\n## 5. Mathematical & Specialized Topics\n\n* **Math / Number Theory:** Problems involving primes, GCD, or modular arithmetic.\n* **Combinatorics / Probability and Statistics:** Counting permutations, combinations, or calculating odds.\n* **Geometry:** Problems involving points, lines, and shapes in a 2D/3D plane.\n* **Bit Manipulation:** Using bitwise operators (`&`, `|`, `^`, `~`) to solve problems with high performance.\n* **Segment Tree / Binary Indexed Tree (Fenwick Tree):** Advanced structures for frequent range updates and queries.\n* **Game Theory:** Problems involving \"optimal play\" between two players (e.g., Nim game).\n* **Brainteaser:** Problems that require a \"Eureka!\" moment or a clever trick rather than a standard algorithm.\n\n---\n\n## 6. System & Miscellaneous\n\n* **Database:** SQL-based queries.\n* **Concurrency:** Handling multiple threads/tasks simultaneously.\n* **Data Stream:** Processing data that arrives one by one, where you can't store everything.\n* **Shell:** Problems involving Bash/Terminal scripting.\n\n**Would you like me to pick one of these categories and give you a \"Roadmap\" of 3-5 problems to solve to master it?**"}, {"name": "pyspark_cheatsheet.md", "type": "file", "content": "# PySpark Cheat Sheet (SQL Equivalent)\n\nA guide mapping common SQL interview patterns to PySpark DataFrame API.\n\n## 1. Basic Query Structure\n`SELECT` -> `FILTER` -> `GROUP BY` -> `AGG` -> `ORDER BY` -> `LIMIT`\n\n```python\nfrom pyspark.sql import functions as F\n\ndf.select(\"col1\", F.count(\"col2\")) \\\n  .join(df2, \"id\") \\\n  .filter(F.col(\"cond\") > 10) \\\n  .groupBy(\"col1\") \\\n  .agg(F.count(\"col2\").alias(\"cnt\")) \\\n  .where(F.col(\"cnt\") > 5) \\\n  .orderBy(F.col(\"col1\").asc()) \\\n  .limit(10)\n```\n\n---\n\n## 2. Joins\nCombine DataFrames.\n\n| Join Type | PySpark Syntax |\n| :--- | :--- |\n| **INNER** | `df1.join(df2, \"id\", \"inner\")` |\n| **LEFT** | `df1.join(df2, \"id\", \"left\")` |\n| **FULL** | `df1.join(df2, \"id\", \"full\")` |\n| **CROSS** | `df1.crossJoin(df2)` |\n| **SELF** | `df1.alias(\"a\").join(df1.alias(\"b\"), F.col(\"a.mgr\") == F.col(\"b.emp\"))` |\n\n---\n\n## 3. Aggregate Functions\n\n| SQL | PySpark |\n| :--- | :--- |\n| `COUNT(*)` | `F.count(\"*\")` |\n| `SUM(col)` | `F.sum(\"col\")` |\n| `AVG(col)` | `F.mean(\"col\")` |\n| `MIN/MAX` | `F.min(\"col\"), F.max(\"col\")` |\n| `COUNT(DISTINCT col)` | `F.countDistinct(\"col\")` |\n\n```python\n# Count users per city > 10\ndf.groupBy(\"city\") \\\n  .agg(F.count(\"user_id\").alias(\"cnt\")) \\\n  .filter(F.col(\"cnt\") > 10)\n```\n\n---\n\n## 4. Window Functions\n`from pyspark.sql.window import Window`\n\n### Syntax\n```python\nwindowSpec = Window.partitionBy(\"dept\").orderBy(F.col(\"salary\").desc())\ndf.withColumn(\"rnk\", F.dense_rank().over(windowSpec))\n```\n\n### Examples\n| Function | PySpark |\n| :--- | :--- |\n| `ROW_NUMBER()` | `F.row_number().over(w)` |\n| `RANK()` | `F.rank().over(w)` |\n| `LEAD(col, 1)` | `F.lead(\"col\", 1).over(w)` |\n| `LAG(col, 1)` | `F.lag(\"col\", 1).over(w)` |\n\n### Framing (Rolling Window)\n```python\n# 3-day Moving Average\nw = Window.orderBy(\"date\").rowsBetween(-2, Window.currentRow)\ndf.withColumn(\"moving_avg\", F.avg(\"sales\").over(w))\n```\n\n---\n\n## 5. String Functions\n\n| SQL | PySpark | Output (Input: 'abc') |\n| :--- | :--- | :--- |\n| `CONCAT(a, b)` | `F.concat(F.col(\"a\"), F.col(\"b\"))` | 'ab' |\n| `SUBSTRING(s, 1, 2)` | `F.substring(\"s\", 1, 2)` | 'ab' |\n| `TRIM(s)` | `F.trim(\"s\")` | 'abc' |\n| `UPPER(s)` | `F.upper(\"s\")` | 'ABC' |\n| `REPLACE` | `F.regexp_replace(\"s\", \"pattern\", \"repl\")` | - |\n\n---\n\n## 6. Date Functions\n\n| SQL | PySpark | Example |\n| :--- | :--- | :--- |\n| `CURRENT_DATE` | `F.current_date()` | 2023-01-01 |\n| `DATE_ADD` | `F.date_add(\"col\", 7)` | +7 Days |\n| `DATEDIFF` | `F.datediff(\"end\", \"start\")` | Days diff |\n| `YEAR/MONTH` | `F.year(\"col\")`, `F.month(\"col\")` | 2023, 1 |\n| `TRUNC` | `F.date_trunc(\"month\", \"col\")` | First of month |\n\n```python\n# Filter last 7 days\ndf.filter(F.col(\"date\") >= F.date_add(F.current_date(), -7))\n```\n\n---\n\n## 7. Regex & Matching\n\n| SQL | PySpark | Example |\n| :--- | :--- | :--- |\n| `LIKE 'A%'` | `col.startswith(\"A\")` | 'Apple' |\n| `LIKE '%B'` | `col.endswith(\"B\")` | 'Bob' |\n| `LIKE '%M%'` | `col.contains(\"M\")` | 'Mary' |\n| `REGEXP` | `col.rlike(\"regex\")` | `email.rlike(\"^\\d+\")` |\n\n### Regex Extract\n```python\n# Extract domain: 'user@gmail.com' -> 'gmail.com'\ndf.withColumn(\"domain\", F.regexp_extract(\"email\", \"@(.*)\", 1))\n```\n\n---\n\n## 8. Pivot / Unpivot\n\n### Pivot (Rows to Columns)\n```python\n# Input: exam_id, result ('Pass'/'Fail')\npivot_df = df.groupBy(\"exam_id\") \\\n             .pivot(\"result\") \\\n             .count() \\\n             .na.fill(0)\n# Output Cols: exam_id, Pass, Fail\n```\n\n### Unpivot (Columns to Rows)\nUsing `stack` inside `selectExpr` (SQL expression).\n```python\n# Input: student, math, science\nunpivot_df = df.selectExpr(\"student\", \"stack(2, 'Math', math, 'Science', science) as (subject, score)\")\n# Output: student, subject, score\n```\n\n---\n\n## 9. Set Operations\n\n| SQL | PySpark | Note |\n| :--- | :--- | :--- |\n| `UNION ALL` | `df1.union(df2)` | **Keeps duplicates**. Fast. |\n| `UNION` | `df1.union(df2).distinct()` | Removes duplicates. |\n| `INTERSECT` | `df1.intersect(df2)` | Common rows. |\n| `EXCEPT` | `df1.subtract(df2)` | In df1 not in df2. |\n"}, {"name": "python_cheatsheet.md", "type": "file", "content": "# Python Cheat Sheet for LeetCode\n\nEssential syntax and libraries for competitive programming and coding interviews.\n\n## 1. Lists (Arrays)\n```python\nnums = [1, 2, 3]\n\n# Operations\nnums.append(4)          # O(1)\nnums.pop()              # O(1) - Remove last\nnums.pop(0)             # O(n) - Remove first (Avoid!)\nnums.insert(0, 5)       # O(n) - Insert at index\nnums.sort()             # O(n log n) - In-place\nsorted(nums)            # Returns new list\nnums[::-1]              # Reverse list\n\n# Slicing [start:end:step]\nnums[1:3]               # index 1 to 2\nnums[-1]                # Last element\n\n# List Comprehension\nsquares = [x**2 for x in nums if x > 0]\nmatrix = [[0]*5 for _ in range(5)] # 5x5 matrix\n```\n\n## 2. Strings\nStrings are **immutable**.\n```python\ns = \"Hello World\"\n\n# Common Methods\ns.lower() / s.upper()\ns.strip()               # Remove whitespace\ns.split(\" \")            # Return list\n\",\".join(['a', 'b'])    # Join list -> \"a,b\"\ns.find(\"Wor\")           # Returns index or -1\ns.replace(\"l\", \"x\")     # Returns new string\n\n# ASCII\nord('a')                # 97\nchr(97)                 # 'a'\n```\n\n## 3. Hash Maps (Dictionary)\n```python\nd = {}\nd = {'a': 1, 'b': 2}\n\n# keys, values, items\nfor k, v in d.items():\n    print(k, v)\n\n# Get with default\nd.get('c', 0)           # Returns 0 if key missing\n\n# DefaultDict (Cleaner code)\nfrom collections import defaultdict\ngraph = defaultdict(list)   # Default value is []\ngraph[1].append(2)          # No KeyError\n```\n\n## 4. Hash Set\nUnordered collection of unique elements. Average O(1) ops.\n```python\ns = set()\ns.add(1)\ns.remove(1)             # Raises KeyError if missing\ns.discard(1)            # No error\n1 in s                  # Check existence\n```\n\n## 5. Queue & Stack (Deque)\nUse `collections.deque` for O(1) appends/pops from both ends.\n```python\nfrom collections import deque\n\n# Stack (LIFO)\nstack = []\nstack.append(1)\nstack.pop()\n\n# Queue (FIFO)\nq = deque([1, 2])\nq.append(3)             # Enqueue\nq.popleft()             # Dequeue O(1)\n```\n\n## 6. Heap (Priority Queue)\nPython has only **Min Heap** by default. For Max Heap, insert negative values.\n```python\nimport heapq\n\nmin_heap = []\nheapq.heappush(min_heap, 3)\nheapq.heappush(min_heap, 1)\nsmallest = heapq.heappop(min_heap)  # Returns 1\n\n# Heapify (O(n))\nnums = [5, 1, 3]\nheapq.heapify(nums)     # nums becomes [1, 5, 3]\n\n# Max Heap trick\nmax_heap = []\nheapq.heappush(max_heap, -5)\nprint(-heapq.heappop(max_heap))     # 5\n```\n\n## 7. Binary Search (Bisect)\nBuilt-in module for sorted arrays.\n```python\nimport bisect\n\nnums = [1, 3, 4, 4, 5]\n\n# Find insertion point (Left = first index, Right = after last)\nbisect.bisect_left(nums, 4)     # Returns 2\nbisect.bisect_right(nums, 4)    # Returns 4\n```\n\n## 8. Math & Infinity\n```python\nimport math\n\nval = float('inf')\nval = float('-inf')\n\nmath.gcd(12, 18)\nmath.ceil(2.3)          # 3\nmath.floor(2.3)         # 2\npow(2, 3, 5)            # (2^3) % 5 = 3\n```\n\n## 9. Common Patterns\n\n### Sliding Window\n```python\nl = 0\nfor r in range(len(nums)):\n    # add nums[r] to window\n    while invalid(window):\n        # remove nums[l]\n        l += 1\n```\n\n### Two Pointers\n```python\nl, r = 0, len(nums) - 1\nwhile l < r:\n    if nums[l] + nums[r] == target:\n        return True\n    elif nums[l] + nums[r] < target:\n        l += 1\n    else:\n        r -= 1\n```\n\n### BFS (Graph/Matrix)\n```python\nq = deque([(0, 0)])\nvisited = set([(0,0)])\ndirections = [(0,1), (0,-1), (1,0), (-1,0)]\n\nwhile q:\n    r, c = q.popleft()\n    for dr, dc in directions:\n        nr, nc = r + dr, c + dc\n        if 0 <= nr < ROWS and 0 <= nc < COLS and (nr, nc) not in visited:\n            visited.add((nr, nc))\n            q.append((nr, nc))\n```\n"}, {"name": "sql_interview_cheat_sheet.md", "type": "file", "content": "# SQL Interview Cheat Sheet\n\nA comprehensive guide to SQL functions, syntax, and concepts for technical interviews.\n\n## 1. Basic Query Structure\nOrder of execution: `FROM` -> `JOIN` -> `WHERE` -> `GROUP BY` -> `HAVING` -> `SELECT` -> `DISTINCT` -> `ORDER BY` -> `LIMIT`.\n\n```sql\nSELECT DISTINCT column1, AGG(column2)\nFROM table1\nJOIN table2 ON table1.id = table2.id\nWHERE condition\nGROUP BY column1\nHAVING AGG(column2) > value\nORDER BY column1 ASC/DESC\nLIMIT n;\n```\n\n---\n\n## 2. Joins\nCombine rows from two or more tables based on a related column.\n\n| Join Type | Syntax | Description | Example |\n| :--- | :--- | :--- | :--- |\n| **INNER** | `FROM A JOIN B ON A.id = B.id` | Matching rows in both tables. | \"Users who ordered\" |\n| **LEFT** | `FROM A LEFT JOIN B ON A.id = B.id` | All rows from A, matches from B (NULL if no match). | \"All users + orders (if any)\" |\n| **RIGHT** | `FROM A RIGHT JOIN B ON A.id = B.id` | All rows from B, matches from A. | Rarely used. |\n| **FULL** | `FROM A FULL JOIN B ON A.id = B.id` | All rows from both A and B. | \"All users and all orders\" |\n| **CROSS** | `FROM A CROSS JOIN B` | Cartesian product (MxN rows). | \"Every combination of User and Product\" |\n| **SELF** | `FROM A a1 JOIN A a2 ON a1.mgr_id = a2.emp_id` | Join table to itself. | \"Employees earning more than managers\" |\n\n---\n\n## 3. Aggregate Functions\nPerform a calculation on a set of values to return a single scalar value.\n\n*   `COUNT(*)`: Count all rows.\n*   `COUNT(col)`: Count non-NULL values in col.\n*   `SUM(col)`: Sum of values.\n*   `AVG(col)`: Average of values.\n*   `MIN(col) / MAX(col)`: Minimum / Maximum value.\n\n```sql\n-- Count users per city having more than 10 users\nSELECT city, COUNT(user_id)\nFROM users\nGROUP BY city\nHAVING COUNT(user_id) > 10;\n```\n\n---\n\n## 4. Window Functions\nPerform calculations across a set of table rows related to the current row, *without* collapsing them (unlike GROUP BY).\n\n**Syntax**: `FUNCTION() OVER (PARTITION BY col ORDER BY col)`\n\n### Ranking\n*   `ROW_NUMBER()`: 1, 2, 3, 4 (Unique rank, ties broken arbitrarily).\n*   `RANK()`: 1, 2, 2, 4 (Skips ranks for ties).\n*   `DENSE_RANK()`: 1, 2, 2, 3 (No skipped ranks).\n\n```sql\n-- Find top 3 highest paid employees per department\nSELECT * FROM (\n    SELECT name, dept, salary,\n           DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary DESC) as rnk\n    FROM employees\n) WHERE rnk <= 3;\n```\n\n### Lead/Lag\n*   `LAG(col, n)`: Value from `n` rows before.\n*   `LEAD(col, n)`: Value from `n` rows after.\n\n```sql\n-- Calculate Mom (Month-over-Month) growth\nSELECT month, revenue,\n       LAG(revenue) OVER (ORDER BY month) as prev_month_revenue,\n       (revenue - LAG(revenue) OVER (ORDER BY month)) / LAG(revenue) OVER (ORDER BY month) as growth_pct\nFROM sales;\n```\n\n### Framing (Rolling Windows)\nDefining the scope of rows for calculation (e.g., Running Totals, Moving Averages).\n**Syntax**: `ROWS BETWEEN [START] AND [END]`\n*   `UNBOUNDED PRECEDING`: Start of partition.\n*   `CURRENT ROW`: Current row.\n*   `N PRECEDING`: N rows before.\n\n```sql\n-- 3-day Moving Average (Rows)\nAVG(sales) OVER (\n    ORDER BY date\n    ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n)\n\n-- Rolling 7-day Sum (Time-based)\n-- Useful when dates are missing in the sequence\nSUM(amount) OVER (\n    ORDER BY visited_on \n    RANGE BETWEEN INTERVAL '6' DAY PRECEDING AND CURRENT ROW\n)\n```\n\n---\n\n## 5. Common Table Expressions (CTEs)\nTemporary result set defined within the scope of a single statement. Improves readability over subqueries.\n\n```sql\nWITH HighSalary AS (\n    SELECT * FROM employees WHERE salary > 100000\n),\nEngineering AS (\n    SELECT * FROM departments WHERE name = 'Engineering'\n)\nSELECT h.name\nFROM HighSalary h\nJOIN Engineering e ON h.dept_id = e.id;\n```\n\n---\n\n## 6. String Functions\n*   `CONCAT(a, b)`: 'ab'. `CONCAT_WS('-', 'A', 'B')` -> 'A-B'.\n*   `SUBSTRING(str, start, len)`: Extract part.\n*   `TRIM(str)`: Remove whitespace.\n*   `UPPER(str) / LOWER(str)`\n*   `REPLACE(str, from, to)`: `REPLACE('1-800', '-', '')` -> '1800'\n*   `COALESCE(val1, val2)`: Return first non-null.\n\n---\n\n## 7. Date Functions\n*   `CURRENT_DATE` / `NOW()` / `GETDATE()`\n*   `EXTRACT(part FROM date)` or `DATEPART`: Get Year/Month/Day. `DATEPART(year, '2023-10-25')` -> 2023.\n*   `DATEDIFF(interval, start, end)`: `DATEDIFF(day, '2023-01-01', '2023-01-10')` -> 9.\n*   `DATE_ADD(date, interval)`: `DATEADD(day, 7, '2023-01-01')` -> '2023-01-08'.\n\n```sql\n-- Find users who signed up in the last 7 days\nSELECT * FROM users\nWHERE created_at >= DATE_ADD(CURRENT_DATE, INTERVAL -7 DAY);\n```\n\n---\n\n## 8. Regex (Regular Expressions)\nSupport varies significantly by SQL dialect.\n\n### Common Patterns\n*   `^`: Start of string. `^A` -> Starts with A.\n*   `$`: End of string. `com$` -> Ends with com.\n*   `.`: Any single character.\n*   `*`: Zero or more. `a*` -> '', 'a', 'aa'.\n*   `+`: One or more. `a+` -> 'a', 'aa'.\n*   `?`: Zero or one.\n*   `[abc]`: Any character in set.\n*   `[^abc]`: Any character NOT in set.\n*   `\\d`: Digit (0-9).\n*   `\\s`: Whitespace.\n\n### Examples (PostgreSQL / Spark)\n```sql\n-- Emails from specific domain (insensitive) using Anchors\nWHERE email ~* '@gmail\\.com$'\n\n-- Phone numbers starting with 555\nWHERE phone ~ '^555-\\d{4}'\n\n-- Strings containing at least one digit\nWHERE password ~ '\\d+'\n\n-- Extract Domain from Email (Spark)\nregexp_extract(email, '@(.*)', 1)\n```\n\n### PostgreSQL\n*   `~`: Case-sensitive match.\n*   `~*`: Case-insensitive match.\n*   `!~`: No match.\n\n```sql\nSELECT * FROM users WHERE email ~* '\\.(net|org)$';\n```\n\n### Spark SQL / Databricks\n*   `regexp_extract(str, regex, idx)`\n*   `regexp_replace(str, regex, replacement)`\n\n```sql\n-- Mask digits: '123-456' -> 'XXX-XXX'\nSELECT regexp_replace('123-456', '\\d', 'X'); \n```\n\n---\n\n## 9. Set Operations\nCombine results from two queries.\n*   `UNION`: Combined distinctive rows (Removes duplicates). Use `UNION ALL` to keep duplicates (Faster).\n*   `INTERSECT`: Rows present in *both* sets.\n*   `EXCEPT` (or `MINUS`): Rows in first set *not* in second.\n\n---\n\n## 10. Performance Tuning (Indexing)\n*   **Index**: Data structure (B-Tree) to speed up `SELECT` based on specific columns. Slows down `INSERT/UPDATE`.\n*   **Clustered Index**: Sorts the physical data rows (Only 1 per table, usually PK).\n*   **Non-Clustered Index**: Separate structure pointing to data rows.\n*   **Composite Index**: Index on multiple columns `(col1, col2)`. Order matters! query on `col2` alone wont use index `(col1, col2)`.\n\n```sql\nCREATE INDEX idx_users_email ON users(email);\n```\n\n---\n\n## 11. Pivot / Unpivot (CASE WHEN)\nTurning rows into columns manually.\n\n```sql\n-- Calculate count of 'Pass' and 'Fail' students per exam\nSELECT exam_id,\n       SUM(CASE WHEN result = 'Pass' THEN 1 ELSE 0 END) as pass_count,\n       SUM(CASE WHEN result = 'Fail' THEN 1 ELSE 0 END) as fail_count\nFROM results\nGROUP BY exam_id;\n```\n\n**Input Table (`results`)**\n| exam_id | result |\n| :--- | :--- |\n| 101 | Pass |\n| 101 | Pass |\n| 101 | Fail |\n| 102 | Pass |\n\n**Output**\n| exam_id | pass_count | fail_count |\n| :--- | :--- | :--- |\n| 101 | 2 | 1 |\n| 102 | 1 | 0 |\n\n### Unpivot (Columns to Rows)\nTurning wide data into long data using `UNION ALL`.\n\n```sql\nSELECT student, 'Math' as subject, math as score FROM scores\nUNION ALL\nSELECT student, 'Science' as subject, science as score FROM scores;\n```\n\n**Input Table (`scores`)**\n| student | math | science |\n| :--- | :--- | :--- |\n| Alice | 90 | 85 |\n\n**Output**\n| student | subject | score |\n| :--- | :--- | :--- |\n| Alice | Math | 90 |\n| Alice | Science | 85 |\n\n\n"}]}, {"name": "DSA", "type": "directory", "children": [{"name": "data_structures_python.md", "type": "file", "content": "# Data Structures and Algorithms in Python for FAANG Interviews\n\nThis guide covers essential data structures and algorithms using Python, tailored for technical interviews. We use standard libraries where possible (`collections`, `heapq`) and custom classes for pointer-based structures.\n\n## 1. Arrays (Dynamic Arrays)\nIn Python, the built-in `list` serves as a dynamic array.\n\n### Concept\n- Contiguous memory.\n- Dynamic resizing (amortized O(1) append).\n- Random access O(1).\n\n### Implementation & Examples\n```python\n# Initialization\narr = [1, 2, 3, 4, 5]\n\n# Access\nprint(f\"Element at index 2: {arr[2]}\") # Output: 3\n\n# Append (O(1))\narr.append(6)\n\n# Insert (O(n)) - avoid in tight loops\narr.insert(0, 0)\n\n# Delete/Remove (O(n))\narr.pop() # Removes last element (O(1))\narr.remove(3) # Removes first occurrence of 3 (O(n))\n\n# Slicing\nsub_arr = arr[1:3] # Creates a new list [1, 2]\n\n# Iteration\nfor num in arr:\n    pass\n\n# List Comprehension (Pythonic way)\nsquares = [x**2 for x in arr]\n```\n\n---\n\n## 2. Linked Lists\nA collection of nodes where each node points to the next. Python doesn't have a built-in Linked List class, so we define `ListNode`.\n\n### Concept\n- Non-contiguous memory.\n- Efficient insertion/deletion at known positions O(1).\n- Linear access time O(n).\n\n### Implementation\n```python\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def append(self, val):\n        if not self.head:\n            self.head = ListNode(val)\n            return\n        curr = self.head\n        while curr.next:\n            curr = curr.next\n        curr.next = ListNode(val)\n\n    def print_list(self):\n        curr = self.head\n        values = []\n        while curr:\n            values.append(str(curr.val))\n            curr = curr.next\n        print(\" -> \".join(values))\n\n# Usage\nll = LinkedList()\nll.append(1)\nll.append(2)\nll.append(3)\nll.print_list()\n# Output: 1 -> 2 -> 3\n```\n**Algo Note:** A dummy/sentinel node is often used in interview problems (e.g., merging sorted lists) to simplify edge cases.\n\n---\n\n## 3. Stacks\nLIFO (Last In, First Out). Use `collections.deque` or a simple `list`.\n\n### Concept\n- `push`: Add to top.\n- `pop`: Remove from top.\n- `peek`: Look at top.\n\n### Implementation\n```python\nfrom collections import deque\n\n# Using deque (Preferred over list for O(1) appends/pops from both ends)\nstack = deque()\n\n# Push\nstack.append(10)\nstack.append(20)\nstack.append(30)\n\n# Peek\nprint(f\"Top: {stack[-1]}\") # Output: 30\n\n# Pop\nval = stack.pop()\nprint(f\"Popped: {val}\")    # Output: 30\n\n# Check empty\nif not stack:\n    print(\"Stack is empty\")\n```\n\n---\n\n## 4. Queues\nFIFO (First In, First Out). Always use `collections.deque`. Using `list` for queues is inefficient (O(n) pop from front).\n\n### Concept\n- `enqueue`: Add to rear.\n- `dequeue`: Remove from front.\n\n### Implementation\n```python\nfrom collections import deque\n\nqueue = deque()\n\n# Enqueue\nqueue.append(1)\nqueue.append(2)\nqueue.append(3)\n\n# Dequeue\nfirst = queue.popleft() # O(1) operation\nprint(f\"Dequeued: {first}\") # Output: 1\n\nprint(f\"Next in line: {queue[0]}\") # Output: 2\n```\n\n---\n\n## 5. Binary Trees\nHierarchical structure. Essential for searching and sorting.\n\n### Concept\n- **Max Depth**: O(h), h is height.\n- **Traversals**: Inorder (Left-Root-Right), Preorder (Root-Left-Right), Postorder (Left-Right-Root).\n\n### Implementation\n```python\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\n# Example: Constructing a simple tree\n#      1\n#     / \\\n#    2   3\nroot = TreeNode(1)\nroot.left = TreeNode(2)\nroot.right = TreeNode(3)\n\n# DFS Traversal (Recursive)\ndef inorder(node):\n    if not node:\n        return\n    inorder(node.left)\n    print(node.val, end=' ')\n    inorder(node.right)\n\nprint(\"Inorder:\")\ninorder(root) # Output: 2 1 3 (approx)\nprint()\n\n# BFS Traversal (Level Order) using Queue\nfrom collections import deque\ndef bfs(root):\n    if not root: return\n    q = deque([root])\n    while q:\n        node = q.popleft()\n        print(node.val, end=' ')\n        if node.left: q.append(node.left)\n        if node.right: q.append(node.right)\n\nprint(\"BFS:\")\nbfs(root) # Output: 1 2 3\nprint()\n```\n\n---\n\n## 6. Heaps (Priority Queues)\nPython's `heapq` module implements a **Min-Heap** by default.\n\n### Concept\n- Access Min/Max: O(1).\n- Insert: O(log n).\n- Pop Min/Max: O(log n).\n- Useful for \"Kth largest/smallest\" problems.\n\n### Implementation\n```python\nimport heapq\n\n# Min Heap\nmin_heap = []\nheapq.heappush(min_heap, 10)\nheapq.heappush(min_heap, 1)\nheapq.heappush(min_heap, 5)\n\nprint(f\"Smallest: {min_heap[0]}\") # Output: 1\n\nsmallest = heapq.heappop(min_heap)\nprint(f\"Popped: {smallest}\")      # Output: 1\n\n# Max Heap: Python does not have strict max-heap, multiply by -1\nnums = [1, 10, 5]\nmax_heap = [-n for n in nums]\nheapq.heapify(max_heap) # O(n) to build\n\nlargest = -heapq.heappop(max_heap)\nprint(f\"Largest: {largest}\")     # Output: 10\n```\n\n---\n\n## 7. Genres (Graphs)\nRepresented via Adjacency Lists (Dict of Lists/Sets) or Adjacency Matrix.\n\n### Concept\n- **Nodes/Vertices** connected by **Edges**.\n- **DFS**: Deep exploration (stack/recursion).\n- **BFS**: Level exploration (queue), shortest path in unweighted graphs.\n\n### Implementation\n```python\nfrom collections import deque\n\n# Adjacency List: { 'A': ['B', 'C'], 'B': ['A'], ... }\ngraph = {\n    'A': ['B', 'C'],\n    'B': ['D', 'E'],\n    'C': ['F'],\n    'D': [],\n    'E': ['F'],\n    'F': []\n}\n\ndef bfs_graph(start_node):\n    visited = set()\n    queue = deque([start_node])\n    visited.add(start_node)\n    \n    while queue:\n        node = queue.popleft()\n        print(node, end=\" \")\n        \n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append(neighbor)\n\nprint(\"Graph BFS:\")\nbfs_graph('A')\n# Output: A B C D E F (Order may vary slightly based on implementation)\nprint()\n```\n\n---\n\n## 8. Hash Maps (Dictionaries)\nPython's `dict` is a robust hash map.\n\n### Concept\n- Key-Value pairs.\n- Average O(1) for insert, delete, get.\n- Keys must be immutable (hashable).\n\n### Implementation\n```python\n# Initialization\nhash_map = {}\nhash_map['name'] = 'Alice'\nhash_map['age'] = 25\n\n# Existence check\nif 'name' in hash_map:\n    print(hash_map['name'])\n\n# Iteration\nfor key, value in hash_map.items():\n    print(f\"{key}: {value}\")\n\n# Collections.defaultdict (Very useful for interviews)\nfrom collections import defaultdict\n# Automatically creates default value if key missing\ncount = defaultdict(int) \nnames = [\"a\", \"b\", \"a\", \"c\"]\nfor n in names:\n    count[n] += 1\nprint(dict(count)) # {'a': 2, 'b': 1, 'c': 1}\n```\n\n## Summary of Complexities\n\n| Data Structure | Access | Search | Insertion | Deletion |\n| :--- | :--- | :--- | :--- | :--- |\n| **Array** | O(1) | O(n) | O(n) | O(n) |\n| **Stack/Queue** | O(n) | O(n) | O(1) | O(1) |\n| **Linked List** | O(n) | O(n) | O(1)* | O(1)* |\n| **Doubly Linked List** | O(n) | O(n) | O(1) | O(1) |\n| **Hash Table** | N/A | O(1) | O(1) | O(1) |\n| **BST** | O(log n) | O(log n) | O(log n) | O(log n) |\n\n*\\* At known position (head/tail).*\n"}]}, {"name": "Data Modeling", "type": "directory", "children": [{"name": "airbnb.md", "type": "file", "content": "# Airbnb Data Model for System Design Interview\n\nThis document outlines the data modeling approach for an Airbnb-type lodging reservation system. This is a common FAANG system design and SQL interview question.\n\n## 1. Requirement & Assumptions\n\n*   **Users**: A user can be both a generic user, a host, or a guest. We typically model this as a single `Users` table.\n*   **Listings**: Hosts create listings (properties).\n*   **Bookings**: Guests book listings for a date range.\n*   **Availability**: The system must prevent double bookings and manage calendar availability efficiently.\n*   **Reviews**: Guests review listings; Hosts review guests (optional but standard).\n*   **Pricing**: Prices can vary by date (dynamic pricing).\n\n## 2. Core Entities (ER Diagram)\n\n1.  **User**: `user_id`, name, email, phone, is_host (bool).\n2.  **Listing**: `listing_id`, host_id (FK), address, description, property_type, base_price.\n3.  **Booking**: `booking_id`, listing_id (FK), guest_id (FK), check_in_date, check_out_date, status (PENDING, CONFIRMED, CANCELLED), total_price.\n4.  **Review**: `review_id`, booking_id (FK), reviewer_id (FK), target_id (FK/Listing or User), rating, comment.\n5.  **Calendar/Availability**: `listing_id`, `date`, `is_available`, `price` (daily override).\n\n## 3. Logical SQL Schema\n\n```sql\n-- 1. Users Table\nCREATE TABLE Users (\n    user_id BIGINT PRIMARY KEY,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    password_hash VARCHAR(255),\n    first_name VARCHAR(100),\n    last_name VARCHAR(100),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- 2. Listings Table\nCREATE TABLE Listings (\n    listing_id BIGINT PRIMARY KEY,\n    host_id BIGINT NOT NULL,\n    title VARCHAR(255),\n    description TEXT,\n    address_line1 VARCHAR(255),\n    city VARCHAR(100),\n    state VARCHAR(50),\n    country VARCHAR(50),\n    zip_code VARCHAR(20),\n    latitude DECIMAL(10, 8),\n    longitude DECIMAL(11, 8),\n    base_price_per_night DECIMAL(10, 2),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (host_id) REFERENCES Users(user_id)\n);\n\n-- 3. Calendar / Availability Table\n-- Critical for checking availability and daily pricing.\n-- Approach: Store one row per date per listing is simple but heavy. \n-- Optimization: Ranges can be used, but daily rows make queries easier.\nCREATE TABLE ListingAvailability (\n    availability_id BIGINT PRIMARY KEY,\n    listing_id BIGINT NOT NULL,\n    calendar_date DATE NOT NULL,\n    is_available BOOLEAN DEFAULT TRUE,\n    price_per_night DECIMAL(10, 2), -- Overrides base price\n    UNIQUE(listing_id, calendar_date), -- Prevent duplicate dates for same listing\n    FOREIGN KEY (listing_id) REFERENCES Listings(listing_id)\n);\n\n-- 4. Bookings Table\nCREATE TABLE Bookings (\n    booking_id BIGINT PRIMARY KEY,\n    listing_id BIGINT NOT NULL,\n    guest_id BIGINT NOT NULL,\n    check_in DATE NOT NULL,\n    check_out DATE NOT NULL,\n    status ENUM('PENDING', 'CONFIRMED', 'CANCELLED', 'COMPLETED'),\n    total_price DECIMAL(10, 2),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (listing_id) REFERENCES Listings(listing_id),\n    FOREIGN KEY (guest_id) REFERENCES Users(user_id)\n);\n\n-- 5. Reviews Table\nCREATE TABLE Reviews (\n    review_id BIGINT PRIMARY KEY,\n    booking_id BIGINT NOT NULL,\n    author_id BIGINT NOT NULL, -- User writing the review\n    listing_id BIGINT NOT NULL, -- Listing being reviewed\n    rating INT CHECK (rating >= 1 AND rating <= 5),\n    comment TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (booking_id) REFERENCES Bookings(booking_id),\n    FOREIGN KEY (author_id) REFERENCES Users(user_id),\n    FOREIGN KEY (listing_id) REFERENCES Listings(listing_id)\n);\n```\n\n## 4. Key Design Patterns & Queries\n\n### A. Searching for Available Listings\nFinding listings available between `start_date` and `end_date` is the most critical query.\n\n**Naive Approach (using Bookings table):**\nFind listings that *do not* have a booking overlapping with the requested range.\n```sql\nSELECT * FROM Listings l\nWHERE l.listing_id NOT IN (\n    SELECT b.listing_id \n    FROM Bookings b\n    WHERE b.listing_id = l.listing_id\n    AND NOT (b.check_out <= '2023-01-01' OR b.check_in >= '2023-01-05')\n    AND b.status = 'CONFIRMED'\n);\n```\n\n**Optimized Approach (using Availability Table):**\nOnly return listings where *every* day in the range is marked as available.\n```sql\nSELECT l.listing_id, l.title, SUM(cal.price_per_night) as total_trip_price\nFROM Listings l\nJOIN ListingAvailability cal ON l.listing_id = cal.listing_id\nWHERE cal.calendar_date BETWEEN '2023-01-01' AND '2023-01-04' -- Checkout is on 5th\n  AND cal.is_available = TRUE\nGROUP BY l.listing_id\nHAVING COUNT(cal.calendar_date) = 4; -- Must be available for all 4 nights\n```\n\n### B. Preventing Double Bookings (Concurrency)\nWhen a user clicks \"Book\", you must ensure no one else booked those dates in the millisecond before.\n\n**Transaction with Locking:**\n```sql\nSTART TRANSACTION;\n\n-- 1. Check if dates are still free\nSELECT * FROM ListingAvailability \nWHERE listing_id = 123 \n  AND calendar_date BETWEEN '2023-01-01' AND '2023-01-05'\n  AND is_available = TRUE\nFOR UPDATE; -- Locks these rows\n\n-- 2. If valid count, Insert Booking\nINSERT INTO Bookings (...) VALUES (...);\n\n-- 3. Update Availability\nUPDATE ListingAvailability\nSET is_available = FALSE\nWHERE listing_id = 123 \n  AND calendar_date BETWEEN '2023-01-01' AND '2023-01-05';\n\nCOMMIT;\n```\n\n## 5. Scalability Considerations (Use for System Design)\n*   **Sharding**: Shard `Listings` and `Bookings` by `Location` (Geohash) or `listing_id`.\n*   **Caching**: Cache search results heavily (Redis), invalidate when a booking occurs.\n*   **Hotspots**: Popular listings or events (e.g., Superbowl location) create read/write hotspots.\n"}, {"name": "e_commerce.md", "type": "file", "content": "# E-Commerce Data Model (Amazon/Shopify)\n\nThis document outlines the data model for a standard e-commerce platform.\n\n## 1. Requirement Analysis\n*   **Catalog**: Browse products, categories.\n*   **Cart**: Temporary storage for items.\n*   **Checkout/Orders**: Persistent record of purchases.\n*   **Inventory**: accurate stock tracking.\n\n## 2. Core Entities\n1.  **User**: Standard profile.\n2.  **Product**: Name, description, price, stock_quantity.\n3.  **Category**: Hierarchical organization (Tree structure).\n4.  **Order**: Status, total, shipping info.\n5.  **OrderItem**: Link between Order and Product (snapshot of price at time of purchase).\n6.  **Cart & CartItem**: Ephemeral or persistent depending on design.\n\n## 3. Logical SQL Schema\n\n```sql\n-- 1. Users\nCREATE TABLE Users (\n    user_id BIGINT PRIMARY KEY,\n    email VARCHAR(255) UNIQUE,\n    password_hash VARCHAR(255),\n    full_name VARCHAR(100),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- 2. Categories (Hierarchical)\nCREATE TABLE Categories (\n    category_id INT PRIMARY KEY,\n    parent_id INT, -- Self-referencing FK for hierarchy\n    name VARCHAR(100),\n    FOREIGN KEY (parent_id) REFERENCES Categories(category_id)\n);\n\n-- 3. Products\nCREATE TABLE Products (\n    product_id BIGINT PRIMARY KEY,\n    category_id INT,\n    name VARCHAR(255),\n    description TEXT,\n    price DECIMAL(10, 2),\n    stock_quantity INT DEFAULT 0,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (category_id) REFERENCES Categories(category_id)\n);\n\n-- 4. Orders\nCREATE TABLE Orders (\n    order_id BIGINT PRIMARY KEY,\n    user_id BIGINT,\n    status ENUM('PENDING', 'PAID', 'SHIPPED', 'DELIVERED', 'CANCELLED'),\n    total_amount DECIMAL(10, 2),\n    shipping_address TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (user_id) REFERENCES Users(user_id)\n);\n\n-- 5. OrderItems\n-- Stores price at purchase time in case product price changes later\nCREATE TABLE OrderItems (\n    order_item_id BIGINT PRIMARY KEY,\n    order_id BIGINT,\n    product_id BIGINT,\n    quantity INT,\n    unit_price DECIMAL(10, 2), -- Snapshot price\n    FOREIGN KEY (order_id) REFERENCES Orders(order_id),\n    FOREIGN KEY (product_id) REFERENCES Products(product_id)\n);\n```\n\n## 4. Key Design Patterns\n\n### A. Inventory Management (Concurrency)\nHandling 20 users trying to buy the last iPhone simultaneously.\n\n**Optimistic Locking (Version column):**\n```sql\nUPDATE Products \nSET stock_quantity = stock_quantity - 1, version = version + 1\nWHERE product_id = 101 \n  AND stock_quantity > 0 \n  AND version = 5; -- Check if version matches what we read\n```\nIf rows updated = 0, the transaction failed (data changed), user must retry.\n\n### B. Product Attributes (EAV Pattern vs JSONB)\nProducts have different attributes (Shirt size vs Laptop RAM).\n*   **SQL Anti-pattern**: `Entity-Attribute-Value` table (slow).\n*   **Modern Approach**: `JSONB` column in Postgres/MySQL.\n    *   `attributes = {\"size\": \"L\", \"color\": \"Blue\"}`\n"}, {"name": "facebook.md", "type": "file", "content": "# Facebook Data Model (Social Network)\n\nThis document outlines the data model for a social graph system like Facebook.\n\n## 1. Core Entities\n1.  **User**: The central node.\n2.  **Friendship**: Bidirectional relationship (Graph Edge).\n3.  **Post**: Content created by user.\n4.  **Reaction/Like**: Enum type (Like, Love, Wow, etc.).\n5.  **Comment**: Hierarchical text replies.\n\n## 2. Logical SQL Schema\n\n```sql\n-- 1. Users\nCREATE TABLE Users (\n    user_id BIGINT PRIMARY KEY,\n    username VARCHAR(50) UNIQUE,\n    email VARCHAR(255),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- 2. Friendships (Bidirectional)\n-- Represented as two rows (A->B, B->A) for easier querying OR one row with least ID first.\n-- Let's use two rows for simple \"Select friends of X\".\nCREATE TABLE Friendships (\n    user_id1 BIGINT,\n    user_id2 BIGINT,\n    status ENUM('PENDING', 'ACCEPTED', 'BLOCKED'),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    PRIMARY KEY (user_id1, user_id2),\n    FOREIGN KEY (user_id1) REFERENCES Users(user_id),\n    FOREIGN KEY (user_id2) REFERENCES Users(user_id)\n);\n\n-- 3. Posts\nCREATE TABLE Posts (\n    post_id BIGINT PRIMARY KEY,\n    author_id BIGINT,\n    content TEXT,\n    image_url VARCHAR(500),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (author_id) REFERENCES Users(user_id)\n);\n\n-- 4. Comments (Adjacency List for threading)\nCREATE TABLE Comments (\n    comment_id BIGINT PRIMARY KEY,\n    post_id BIGINT,\n    user_id BIGINT,\n    parent_comment_id BIGINT NULL, -- For nested replies\n    text TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (post_id) REFERENCES Posts(post_id),\n    FOREIGN KEY (user_id) REFERENCES Users(user_id)\n);\n\n-- 5. Likes/Reactions\nCREATE TABLE PostLikes (\n    user_id BIGINT,\n    post_id BIGINT,\n    reaction_type VARCHAR(20), -- 'LIKE', 'LOVE', etc.\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    PRIMARY KEY (user_id, post_id)\n);\n```\n\n## 3. Key Design Patterns\n\n### A. The \"News Feed\" Problem\nFetching posts from all 500 friends sorted by time.\n\n**Pull Model (Fan-out on Read):**\n1.  Get friend IDs.\n2.  `SELECT * FROM Posts WHERE author_id IN (friend_ids) ORDER BY time LIMIT 10`.\n    *   Good for small scale. Slow if user has thousands of friends.\n\n**Push Model (Fan-out on Write):**\n1.  User A posts.\n2.  System writes Post ID to a separate \"Feed\" table for every follower/friend.\n3.  User B reads their pre-computed feed table.\n    *   Good for read-heavy systems, bad for celebrities (millions of writes per post).\n\n### B. Graph Database\nFor complex queries like \"Friends of Friends who like Jazz\", a Relational DB requires heavy joins.\n**Solution**: Use Neo4j or a Graph Service where nodes = Users, edges = Friendships.\n`MATCH (u:User)-[:FRIEND]->(f:User)-[:FRIEND]->(fof:User) RETURN fof`\n"}, {"name": "food_delivery.md", "type": "file", "content": "# Food Delivery Data Model (UberEats, DoorDash)\n\nThis document outlines the data model for an on-demand food delivery service.\n\n## 1. Core Entities\n1.  **User/Customer**: Places orders.\n2.  **Restaurant**: Prepares food.\n3.  **Menu/MenuItem**: Valid products.\n4.  **Order**: The central transaction entity.\n5.  **Courier/Driver**: Delivers the order.\n6.  **Delivery**: Tracks the logistics.\n\n## 2. Logical SQL Schema\n\n```sql\n-- 1. Restaurants & Menu\nCREATE TABLE Restaurants (\n    restaurant_id BIGINT PRIMARY KEY,\n    name VARCHAR(255),\n    address VARCHAR(255),\n    is_open BOOLEAN,\n    created_at TIMESTAMP\n);\n\nCREATE TABLE MenuItems (\n    item_id BIGINT PRIMARY KEY,\n    restaurant_id BIGINT,\n    name VARCHAR(255),\n    price DECIMAL(10, 2),\n    is_available BOOLEAN, -- 86'd items\n    FOREIGN KEY (restaurant_id) REFERENCES Restaurants(restaurant_id)\n);\n\n-- 2. Orders\nCREATE TABLE Orders (\n    order_id BIGINT PRIMARY KEY,\n    customer_id BIGINT,\n    restaurant_id BIGINT,\n    courier_id BIGINT NULL, -- Assigned later\n    status ENUM('PLACED', 'CONFIRMED', 'PREPARING', 'PICKED_UP', 'DELIVERED', 'CANCELLED'),\n    total_price DECIMAL(10, 2),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    delivered_at TIMESTAMP,\n    FOREIGN KEY (customer_id) REFERENCES Users(user_id) -- Assuming generic Users table\n);\n\n-- 3. Order Details\nCREATE TABLE OrderItems (\n    order_item_id BIGINT PRIMARY KEY,\n    order_id BIGINT,\n    item_id BIGINT,\n    quantity INT,\n    special_instructions TEXT,\n    FOREIGN KEY (order_id) REFERENCES Orders(order_id)\n);\n```\n\n## 3. Key Design Patterns\n\n### A. Order State Machine\nAn order goes through specific transitions.\n*   `PLACED` -> Restaurant accepts -> `CONFIRMED`\n*   `CONFIRMED` -> Kitchen done -> `PREPARING` -> `READY_FOR_PICKUP`\n*   Driver arrives -> `PICKED_UP` -> `DELIVERED`\n\n**Concurrency**: Ensure a driver can only accept an order if it's not already taken by another driver.\n```sql\nUPDATE Orders \nSET courier_id = 101, status = 'DRIVER_ASSIGNED'\nWHERE order_id = 500 AND courier_id IS NULL;\n```\n\n### B. Geospatial Indexing\nFinding restaurants nearby.\n*   Store lat/lon for Restaurants.\n*   Use PostGIS (`ST_DWithin`) or Geohash for efficient queries.\n```sql\nSELECT * FROM Restaurants \nWHERE ST_DWithin(location, ST_MakePoint(user_lon, user_lat), 5000); -- 5km radius\n```\n"}, {"name": "netflix_streaming.md", "type": "file", "content": "# Netflix Data Model (Streaming Service)\n\nThis document outlines the data model for a video streaming platform.\n\n## 1. Core Entities\n1.  **Account**: Billing entity (Family plan).\n2.  **Profile**: Usage entity (Mom, Dad, Kids).\n3.  **Video**: The content (Movie or Episode).\n4.  **Series/Season**: Organizational hierarchy.\n5.  **WatchHistory**: Logs progress to support \"Resume Watching\".\n6.  **Genre/Category**: Classification.\n\n## 2. Logical SQL Schema\n\n```sql\n-- 1. Accounts & Profiles\nCREATE TABLE Accounts (\n    account_id BIGINT PRIMARY KEY,\n    email VARCHAR(255),\n    subscription_plan VARCHAR(50)\n);\n\nCREATE TABLE Profiles (\n    profile_id BIGINT PRIMARY KEY,\n    account_id BIGINT,\n    name VARCHAR(50),\n    is_kids BOOLEAN,\n    FOREIGN KEY (account_id) REFERENCES Accounts(account_id)\n);\n\n-- 2. Content Metadata\nCREATE TABLE Videos (\n    video_id BIGINT PRIMARY KEY,\n    title VARCHAR(255),\n    description TEXT,\n    duration_seconds INT,\n    file_url VARCHAR(500), -- CDM/CDN path\n    release_year INT,\n    type ENUM('MOVIE', 'EPISODE')\n);\n\nCREATE TABLE Series (\n    series_id BIGINT PRIMARY KEY,\n    title VARCHAR(255)\n);\n\nCREATE TABLE Episodes (\n    episode_id BIGINT PRIMARY KEY,\n    series_id BIGINT,\n    season_number INT,\n    episode_number INT,\n    video_id BIGINT, -- Links to physical file metadata\n    FOREIGN KEY (series_id) REFERENCES Series(series_id),\n    FOREIGN KEY (video_id) REFERENCES Videos(video_id)\n);\n\n-- 3. Watch Progress (Resume Watching)\n-- High write volume!\nCREATE TABLE WatchProgress (\n    profile_id BIGINT,\n    video_id BIGINT,\n    last_watched_timestamp TIMESTAMP,\n    progress_seconds INT,\n    is_finished BOOLEAN,\n    PRIMARY KEY (profile_id, video_id),\n    FOREIGN KEY (profile_id) REFERENCES Profiles(profile_id)\n);\n```\n\n## 3. Key Design Patterns\n\n### A. Handling \"Continue Watching\"\nThis feature requires frequent updates (heartbeats) while a user watches.\n*   **Design**: Do not update the main database every 10 seconds.\n*   **Optimization**: Send heartbeats to a Redis/Cache buffer. Aggregate updates and flush to Postgres/Cassandra every X minutes or on session end.\n\n### B. Content Delivery Network (CDN)\nThe DB stores *metadata* (`file_url`). The actual `.mp4` binaries are stored in Object Storage (S3) and distributed via CDN (Cloudfront/Akamai) to edges close to users. The database never acts as a blob store for video.\n"}, {"name": "uber.md", "type": "file", "content": "# Uber Data Model (Ride Sharing)\n\nThis document outlines the data model for a ride-sharing application.\n\n## 1. Core Entities\n1.  **Rider**: The customer.\n2.  **Driver**: The service provider.\n3.  **Vehicle**: Associated with driver.\n4.  **Trip/Ride**: The core transaction.\n5.  **Location**: Real-time geospatial data.\n\n## 2. Logical SQL Schema\n\n```sql\n-- 1. Users & Drivers\nCREATE TABLE Users (\n    user_id BIGINT PRIMARY KEY,\n    name VARCHAR(100),\n    rating DECIMAL(3, 2) -- 4.85\n);\n\nCREATE TABLE Drivers (\n    driver_id BIGINT PRIMARY KEY,\n    user_id BIGINT, -- Links to generic User\n    license_number VARCHAR(50),\n    current_lat DECIMAL(10, 8),\n    current_lon DECIMAL(11, 8),\n    is_active BOOLEAN, -- Online/Offline\n    FOREIGN KEY (user_id) REFERENCES Users(user_id)\n);\n\n-- 2. Vehicles\nCREATE TABLE Vehicles (\n    vehicle_id BIGINT PRIMARY KEY,\n    driver_id BIGINT,\n    license_plate VARCHAR(20),\n    model VARCHAR(50),\n    FOREIGN KEY (driver_id) REFERENCES Drivers(driver_id)\n);\n\n-- 3. Trips\nCREATE TABLE Trips (\n    trip_id BIGINT PRIMARY KEY,\n    rider_id BIGINT,\n    driver_id BIGINT,\n    status ENUM('REQUESTED', 'MATCHED', 'IN_PROGRESS', 'COMPLETED', 'CANCELLED'),\n    pickup_lat DECIMAL(10, 8),\n    pickup_lon DECIMAL(11, 8),\n    dropoff_lat DECIMAL(10, 8),\n    dropoff_lon DECIMAL(11, 8),\n    fare DECIMAL(10, 2),\n    requested_at TIMESTAMP,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    FOREIGN KEY (rider_id) REFERENCES Users(user_id),\n    FOREIGN KEY (driver_id) REFERENCES Drivers(driver_id)\n);\n```\n\n## 3. Key Design Patterns\n\n### A. Location Tracking (High Write Volume)\nDrivers emit location every 3 seconds.\n**Do NOT** update the main SQL `Drivers` table every 3 seconds for every driver.\n**Solution**:\n*   Use an in-memory geo-store like **Redis (Geo)** or **Uber H3** to track live positions.\n*   Only persist \"Trip Start\" and \"Trip End\" locations to SQL for billing/history.\n*   Breadcrumbs during a trip can be stored in Cassandra/NoSQL for route history, not relational DB.\n\n### B. Driver Matching\nFinding the nearest driver.\n*   Query Redis for drivers with `status=ACTIVE` within radius R.\n*   Lock the driver (Atomic check-and-set) to prevent assigning them to two riders.\n"}, {"name": "youtube.md", "type": "file", "content": "# YouTube Data Model (Video Sharing)\n\nThis document outlines the data model for a video hosting platform.\n\n## 1. Core Entities\n1.  **User**: Viewer/Creator.\n2.  **Channel**: Identity for uploading content.\n3.  **Video**: The media asset.\n4.  **Comment**: Social interaction.\n5.  **Like/Vote**: Binary feedback.\n6.  **Playlist**: Aggregation of videos.\n\n## 2. Logical SQL Schema\n\n```sql\n-- 1. Channels\nCREATE TABLE Channels (\n    channel_id BIGINT PRIMARY KEY,\n    owner_user_id BIGINT,\n    name VARCHAR(100),\n    subscriber_count BIGINT DEFAULT 0,\n    created_at TIMESTAMP\n);\n\n-- 2. Videos\nCREATE TABLE Videos (\n    video_id BIGINT PRIMARY KEY,\n    channel_id BIGINT,\n    title VARCHAR(255),\n    description TEXT,\n    video_url VARCHAR(500), -- Blob storage path\n    thumbnail_url VARCHAR(500),\n    duration_seconds INT,\n    view_count BIGINT DEFAULT 0, -- Sharded counter in practice\n    like_count BIGINT DEFAULT 0,\n    upload_status ENUM('PROCESSING', 'PUBLISHED', 'PRIVATE'),\n    created_at TIMESTAMP,\n    FOREIGN KEY (channel_id) REFERENCES Channels(channel_id)\n);\n\n-- 3. Comments (Nested)\nCREATE TABLE Comments (\n    comment_id BIGINT PRIMARY KEY,\n    video_id BIGINT,\n    user_id BIGINT,\n    parent_comment_id BIGINT NULL,\n    text TEXT,\n    likes INT,\n    created_at TIMESTAMP,\n    FOREIGN KEY (video_id) REFERENCES Videos(video_id)\n);\n\n-- 4. Likes (Relationships)\nCREATE TABLE VideoLikes (\n    user_id BIGINT,\n    video_id BIGINT,\n    created_at TIMESTAMP,\n    PRIMARY KEY (user_id, video_id) -- Prevent double like\n);\n```\n\n## 3. Key Design Patterns\n\n### A. View Counts (Scalability)\nPopular videos get thousands of views per second.\n**Problem**: Locking a row `UPDATE Videos SET view_count = view_count + 1` is a bottleneck.\n**Solution**:\n1.  **Buffer**: Append views to a Kafka topic.\n2.  **Aggregate**: Summarize views every X seconds (Stream processing).\n3.  **Batch Write**: Update DB once per batch `view_count = view_count + 500`.\n\n### B. Comments Pagination\nYouTube comments are effectively infinite.\n*   **Offset Pagination**: `LIMIT 10 OFFSET 1000` (Slow).\n*   **Cursor Pagination**: `WHERE created_at < last_seen_timestamp LIMIT 10` (Fast/Scalable).\n"}]}, {"name": "Interview", "type": "directory", "children": [{"name": "FAANG_Round_2_1_Machine_Coding.md", "type": "file", "content": "# FAANG Machine Coding Guide (Round 2.1 Focus)\n\nThis guide covers all key topics for the Machine Coding round, specifically tailored for companies like Flipkart, Uber, Swiggy, and others with similar interview styles.\n\n## 1. Object-Oriented Programming (OOP) Essentials\n\nThe foundation of machine coding is a valid object-oriented model.\n\n### Core Concepts\n\n*   **Classes & Objects**:\n    *   **Class**: Blueprint/Template (e.g., `Vehicle`).\n    *   **Object**: Instance (e.g., `Car`, `Bike`).\n    *   *Tip*: Always start by identifying the \"Nouns\" in the problem statement; these are likely your classes.\n\n*   **Interfaces / Abstract Classes**:\n    *   Use **Interfaces** to define abilities (e.g., `Drivable`, `PaymentMethod`).\n    *   Use **Abstract Classes** for shared base behavior + contracts (e.g., `BaseUser` handles `id` and `name`, concrete users handle roles).\n    *   *Flipkart Tip*: Always code against interfaces involved in services (e.g., `INotificationService` instead of `EmailService`).\n\n*   **Inheritance vs Composition**:\n    *   **Inheritance** (`IS-A`): `Car` is a `Vehicle`. Rigid structure.\n    *   **Composition** (`HAS-A`): `Car` has an `Engine`. Flexible.\n    *   *Rule of Thumb*: Prefer **Composition over Inheritance**. It allows you to swap behaviors at runtime (Strategy Pattern).\n\n*   **Encapsulation**:\n    *   Keep fields `private`.\n    *   Expose data only via public methods/getters.\n    *   Protect internal state from invalid updates (e.g., `setAge(-1)` should throw error).\n\n*   **Polymorphism**:\n    *   Runtime polymorphism (Overriding): `paymentService.pay()` triggers different logic for `UPIPayment` vs `CreditCardPayment`.\n\n---\n\n## 2. SOLID Principles (The Evaluator's Checklist)\n\nYour code is scored heavily on this.\n\n### S - Single Responsibility Principle (SRP)\n*   **Concept**: A class should have only one reason to change.\n*   **Example**: `Order` class should typically handle data. `OrderService` handles business logic. `InvoicePrinter` handles printing. Don't mix them.\n\n### O - Open/Closed Principle (OCP)\n*   **Concept**: Open for extension, closed for modification.\n*   **Example**: Adding a new Discount type shouldn't modify the existing `DiscountService`. Instead, add a new `TenPercentDiscount` class implementing `IDiscount`.\n\n### L - Liskov Substitution Principle (LSP)\n*   **Concept**: Subtypes must be substitutable for their base types.\n*   **Anti-Pattern**: A `Penguin` class inheriting `Bird` but throwing exception on `fly()`.\n\n### I - Interface Segregation Principle (ISP)\n*   **Concept**: Clients shouldn't depend on methods they don't use.\n*   **Example**: Split `Worker` interface into `Workable` and `Eatable` if robots don't eat.\n\n### D - Dependency Inversion Principle (DIP)\n*   **Concept**: Depend on abstractions, not concretions.\n*   **Example**: Inject `IPaymentProcessor` into `BookingService`, not `PayPalProcessor` directly.\n\n---\n\n## 3. Design Patterns (The \"Power Moves\")\n\nUse these to solve specific extensibility problems.\n\n### Factory Pattern\n*   **Use When**: Creating complex objects or when creation logic varies based on type.\n*   **Example**: `NotificationFactory.getService(\"EMAIL\")` returns `EmailService`.\n\n### Strategy Pattern (CRITICAL)\n*   **Use When**: You have interchangeable algorithms.\n*   **Example**: Pricing strategies (`FlatRate`, `SurgePricing`), Parking assignment strategies (`LowestFloorFirst`, `NearestToEntry`).\n\n### Singleton Pattern\n*   **Use When**: Exactly one instance is needed (e.g., `ConfigurationManager`, `DatabaseConnection`).\n*   **Warning**: Don't overuse. It makes testing hard. In machine coding, it's safer to create a Service class and inject it once.\n\n### Observer Pattern\n*   **Use When**: One change needs to notify many others.\n*   **Example**: When `Order` status becomes `SHIPPED`, notify `SMSModule`, `EmailModule`, and `InventorySystem`.\n\n### Builder Pattern\n*   **Use When**: Constructing complex objects with many optional parameters.\n*   **Example**: `Order.builder().setId(1).setItems(items).setCoupon(\"SAVE10\").build()`.\n\n---\n\n## 4. Code Quality Checklist\n\n*   **Separation of Concerns**:\n    *   **Models**: POJOs (Plain Old Java Objects) - `User`, `Ride`, `Ticket`.\n    *   **Services**: Logic - `UserService`, `BookingService`.\n    *   **Repositories (DAO)**: In-memory storage - `UserMap`, `RideList`.\n    *   **Driver/Main**: Entry point interacting with services.\n*   **Readable Naming**:\n    *   Variable: `isAvailable` (boolean), `userMap` (collection).\n    *   Methods: `calculateTotal()`, `assignDriver()`.\n*   **Extensible Structure**:\n    *   Can you add a new generic vehicle type without rewriting `ParkingLot` class?\n*   **Exception Handling**:\n    *   Don't return string \"Error\". Throw `InvalidOperationException`.\n*   **Input Validation**:\n    *   Check nulls and bounds at the start of public methods.\n\n---\n\n## 5. Data Structures for Machine Coding\n\nYou won't implementing complex trees/graphs usually. You need fast lookup and order.\n\n*   **HashMap / Dictionary**: Primary storage. O(1) retrieval by ID. `Map<String, User>`.\n*   **List / Array**: Storing collections where index matters less or simple iteration needed.\n*   **Set**: Uniqueness (e.g., `activeUserIds`).\n*   **Priority Queue**: \"Get nearest driver\", \"Get cheapest ride\" (if optimization is needed).\n\n---\n\n## 6. Common Problem Types (Practice Scenarios)\n\n1.  **Parking Lot**:\n    *   Concepts: Multilevel, different vehicle sizes, pricing strategies, filling strategies.\n2.  **Library Management**:\n    *   Concepts: Books, Copies, Users, Borrow limits, Fines, Search by Title/Author.\n3.  **Splitwise (Equation Simplifier)**:\n    *   Concepts: Users, Groups, Expenses, Exact/Percent splits, `Simplify Debt` graph algorithm.\n4.  **Elevator System**:\n    *   Concepts: Scheduling requests (SCAN/LOOK algorithm), multiple elevators, states (MOVING_UP, IDLE).\n5.  **Rate Limiter**:\n    *   Concepts: User, API-Key, sliding window/token bucket logic.\n6.  **Inventory / Order System (E-commerce)**:\n    *   Concepts: Cart, Inventory locking (concurrency simulation), Payment state machine.\n\n---\n\n## 7. Testing Mindset (The Final 15 Mins)\n\nThe interviewer wants to see your code work.\n\n1.  **Driver Class**: Create a `Main` class that acts as a simulation.\n2.  **Hardcoded Data**: Pre-populate some users/items to save demo time.\n3.  **Happy Path**: Show the standard flow working perfectly.\n4.  **Edge Cases**:\n    *   Booking when inventory is 0.\n    *   Parking a Truck in a Bike slot.\n    *   Invalid user ID.\n5.  **Bonus**: Use a simple JUnit/Test framework if comfortable, otherwise strictly structured Print statements are accepted.\n\n---\n\n## Sample Project Structure (Java/Python style conceptual)\n\n```text\n/src\n  /model\n    User.java\n    Vehicle.java\n    Ticket.java\n  /service\n    ParkingService.java (Interface)\n    ParkingServiceImpl.java (Implementation)\n    PricingStrategy.java (Interface)\n  /repository\n    InMemoryParkingRepository.java\n  /exception\n    ParkingFullException.java\n  Main.java\n```\n"}, {"name": "FAANG_Round_2_2_DSA.md", "type": "file", "content": "# FAANG DSA Guide (Round 2.2 - Problem Solving)\n\nThis guide focuses on the \"Problem Solving & Data Structures\" round (Round 2.2). The goal here is **thinking, approach, and correctness** (not just getting the right answer, but how you get there).\n\n## 1. Top 7 Algorithm Patterns (The \"Keys\" to 90% of Problems)\n\nDon't memorize problems; memorize patterns.\n\n### 1. Sliding Window\n*   **Use When**: Analyzing a contiguous subarray/substring of fixed size `K` or dynamic size based on condition.\n*   **Keywords**: \"Longest substring\", \"Max sum of subarray of size K\".\n*   **Time**: O(N) usually.\n\n### 2. Two Pointers\n*   **Use When**: Dealing with sorted arrays or finding pairs/triplets.\n*   **Variants**:\n    *   *Converging*: Left -> <- Right (e.g., Two Sum on sorted array, Palindrome check).\n    *   *Slow/Fast*: Cycle detection in Linked List, Middle of Linked List.\n\n### 3. Prefix Sum\n*   **Use When**: Need sum of sub-ranges `(i, j)` frequently.\n*   **Concept**: `P[i] = A[0] + ... + A[i]`. Sum(i, j) = `P[j] - P[i-1]`.\n*   **Keywords**: \"Sum of range\", \"Product of range\".\n\n### 4. Modified Binary Search\n*   **Use When**: Searching in Sorted (or Rotated Sorted) arrays.\n*   **Time**: O(log N).\n*   **Key**: Identify the sorted half (if rotated) and decide which side to discard.\n\n### 5. Top 'K' Elements (Heap)\n*   **Use When**: Finding \"Largest K\", \"Smallest K\", \"Most Frequent K\".\n*   **Tool**: Priority Queue (Min-Heap for largest K, Max-Heap for smallest K).\n*   **Time**: O(N log K) is better than sorting O(N log N).\n\n### 6. Depth First Search (DFS) & Backtracking\n*   **Use When**: Tree traversal, generating all combinations/permutations, solving maze/sudoku.\n*   **Concept**: Go deep, hit base case, backtrack.\n\n### 7. Breadth First Search (BFS)\n*   **Use When**: Shortest path in unweighted graph/grid, Level-order traversal of Tree.\n*   **Tool**: Queue.\n\n---\n\n## 2. Essential Data Structures\n\nKnow the **internal workings** and **time complexity** of these.\n\n| Structure | Access | Search | Insert | Delete | Use Cases |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Array** | O(1) | O(N) | O(N) | O(N) | Simple storage, indices |\n| **Stack** | O(N) | O(N) | O(1) | O(1) | Recursion, undo, parenthesis |\n| **Queue** | O(N) | O(N) | O(1) | O(1) | BFS, job scheduling |\n| **HashMap** | N/A | O(1)* | O(1)* | O(1)* | Counting Freq, caching |\n| **BST** | O(log N) | O(log N) | O(log N) | O(log N) | Sorted data, ranges |\n| **Heap** | O(1) min/max | O(N) | O(log N) | O(log N) | Priority tasks |\n\n*\\*Amortized*\n\n---\n\n## 3. Flipkart/FAANG Specific Checklist\n\n### A. Arrays & Strings\n*   [ ] **Kadane's Algorithm**: Max contiguous subarray sum.\n*   [ ] **Dutch National Flag**: Sort 0s, 1s, 2s (or Red/White/Blue).\n*   [ ] **Trapping Rain Water**: Classic hard problem (Prefix/Suffix max or Two Pointers).\n\n### B. Hashing & Maps\n*   [ ] **LRU Cache**: Design using HashMap + Doubly Linked List. (Very frequent).\n*   [ ] **Group Anagrams**: Map `{sorted_string -> list_of_strings}`.\n\n### C. Trees & Graphs\n*   [ ] **Lowest Common Ancestor (LCA)**: Recursion logic.\n*   [ ] **Level Order Traversal**: Queue based BFS.\n*   [ ] **Number of Islands**: Grid DFS/BFS.\n*   [ ] **Course Schedule**: Graph topological sort (Kahn's algo or DFS cycle detection).\n\n### D. Dynamic Programming (DP)\n*   *Don't panic on DP. Focus on 1D DP first.*\n*   [ ] **Climbing Stairs**: Fibonacci simple.\n*   [ ] **Coin Change**: Unbounded Knapsack pattern.\n*   [ ] **Longest Common Subsequence (LCS)**: String matching foundation.\n\n---\n\n## 4. The \"Interview Protocol\" (How to Pass)\n\n1.  **Clarify**: \"Does the array contain negatives?\", \"Is memory a constraint?\".\n2.  **Brute Force First**: \"I can solve this in O(N^2) using nested loops. Shall I optimize?\". -> Shows you have a baseline.\n3.  **Optimize**: \"We can bring this down to O(N) using a HashMap\".\n4.  **Dry Run**: Walk through your code with a sample case *before* the interviewer asks.\n5.  **Complexity**: Always state Time AND Space complexity at the end.\n\n## 5. Common \"Gotchas\" / Edge Cases\n\n*   **Empty Input**: Array is null or length 0.\n*   **Single Element**: Array has 1 item.\n*   **Duplicates**: All elements are the same.\n*   **Integer Overflow**: Sum exceeds `Integer.MAX_VALUE` (Use Long).\n*   **Case Sensitivity**: In string problems.\n"}, {"name": "FAANG_Round_2_3_Design.md", "type": "file", "content": "# FAANG Design Guide (Round 2.3 - System & OOD)\n\nThis round evaluates your ability to build scalable, maintainable, and robust systems. It is usually split into **Low-Level Design (LLD)** and **High-Level Design (HLD)**.\n\n---\n\n## Part A: Object-Oriented Design (LLD)\n\n**Goal**: Convert a vague requirement into a clean class diagram with correct relationships.\n\n### 1. LLD Process (The 5-Step Formula)\n1.  **Clarify Requirements**: \"Is it a paid parking lot?\", \"Do we need multiple vehicle types?\".\n2.  **Identify Actors**: \"Customer\", \"Admin\", \"System\".\n3.  **Identify Use Cases**: \"Take Ticket\", \"Pay Ticket\", \"Scan Entry\".\n4.  **Define Classes**: `ParkingLot`, `Spot`, `Vehicle`, `Ticket`, `Gate`.\n5.  **Define Relationships**:\n    *   **Composition**: `ParkingLot` *has* `Floors`.\n    *   **Inheritance**: `Car` *is a* `Vehicle`.\n    *   **Association**: `Ticket` *is associated with* `Spot`.\n\n### 2. Crucial Design Patterns for LLD\n*   **State Pattern**: Essential for Order Management, Elevator Systems, or Vending Machines. Transitioning from `Idle` -> `Moving` or `Created` -> `Paid` -> `Shipped`.\n*   **Strategy Pattern**: For changing algorithms (e.g., `PricingStrategy`, `AssignSpotStrategy`).\n*   **Factory Pattern**: To create objects like `VehicleFactory.createType(\"TRUCK\")`.\n\n### 3. Common LLD Problems\n*   **Parking Lot**: Focus on hierarchy (`ParkingLot` -> `Floor` -> `Spot`) and concurrency (two cars taking same spot).\n*   **Elevator System**: Focus on the scheduling algorithm and State management.\n*   **Tic-Tac-Toe / Chess**: Focus on `Board`, `Player`, `Move`, and `GameStatus` logic.\n\n### 4. UML Basics (Whiteboard Ready)\n*   **Box**: Class.\n*   **Solid Line + Diamond**: Composition (Strong ownership).\n*   **Dotted Line**: Dependency.\n\n---\n\n## Part B: System Design (HLD)\n\n**Goal**: Design a system that scales to millions of users.\n\n### 1. HLD Process (The Template)\n1.  **Functional Requirements**: What does the system DO? (e.g., User posts tweet, follows user).\n2.  **Non-Functional Requirements**: CAP Theorem choices. High Availability? Low Latency? Consistency?\n3.  **Capacity Estimation** (Back-of-the-envelope):\n    *   \"10M DAU, each posts 2 tweets/day = 20M tweets/day\".\n    *   \"Approx 230 tweets/sec. Peak = 2-3x = 600/sec\".\n4.  **API Design**: `POST /tweet`, `GET /feed`.\n5.  **High-Level Diagram**: Draw the boxes (LB, App Server, DB, Cache).\n6.  **Deep Dive**: Pick one component to optimize (e.g., \"How does the Feed generation work?\").\n\n### 2. Core Building Blocks\n\n#### Load Balancer (LB)\n*   Distributes traffic. Algorithms: Round Robin, Least Connections, Consistent Hashing.\n\n#### Caching (Redis/Memcached)\n*   **Read-Through**: App checks cache, if miss, checks DB and populates cache.\n*   **Eviction Policies**: LRU (Least Recently Used) is standard.\n*   *Use Case*: Storing session data, User profiles, popular tweets.\n\n#### Database Choices\n*   **SQL (RDBMS)**: Structured data, ACID transactions needed. (e.g., Payments, User Auth). *MySQL, Postgres*.\n*   **NoSQL**: Unstructured, High Write throughput, Flexible schema. (e.g., Chat logs, Activity stream). *MongoDB, Cassandra, DynamoDB*.\n\n#### Message Queues (Kafka/RabbitMQ)\n*   **Decoupling**: Producer sends message, Consumer processes it asynchronously.\n*   *Use Case*: Processing video uploads, Sending notifications, Analytics logging.\n\n### 3. Concepts to Master\n\n*   **Sharding (Partitioning)**: Splitting a large DB into smaller chunks based on a key (e.g., `User_ID`).\n    *   *Challenge*: Hot partitions (Justin Bieber problem).\n*   **Replication**: Master-Slave architecture. Write to Master, Read from Slaves.\n*   **CAP Theorem**:\n    *   **CP** (Consistency/Partition Tolerance): Banking apps.\n    *   **AP** (Availability/Partition Tolerance): Social media feeds. (It's okay if you see a like 2 seconds late, but the site shouldn't crash).\n\n### 4. Standard HLD Problems\n\n*   **Design Twitter/Facebook Feed**:\n    *   *Key*: Fan-out service (Push vs Pull model). Push to active users' timelines.\n*   **Design URL Shortener (TinyURL)**:\n    *   *Key*: Hashing strategy (Base62 encoding), Handling collisions.\n*   **Design Uber (Ride Hailing)**:\n    *   *Key*: QuadTree/Geohash for location, WebSocket for real-time updates.\n\n---\n\n## Summary Checklist for Round 2.3\n\n*   [ ] Can I draw a Class Diagram for a Parking Lot in 10 mins?\n*   [ ] Do I know when to use NoSQL vs SQL?\n*   [ ] Can I explain Consistent Hashing?\n*   [ ] Do I understand why we need a Message Queue in a system?\n"}, {"name": "Flipkart.md", "type": "file", "content": "Here is a **crisp, interview-ready topic checklist** **only** for **Flipkart SDE Phase-2** rounds **2.1, 2.2, 2.3**, exactly aligned to what they test.\n\nNo fluff. This is what you should **actually prepare**.\n\n---\n\n# \ud83d\udd39 ROUND 2.1 \u2013 MACHINE CODING (MOST IMPORTANT)\n\n> Goal: Write **working, clean, extensible code** in 90 mins.\n\n## \u2705 Core Topics\n\n### 1. Object-Oriented Programming (MUST)\n\n* Classes & objects\n* Interfaces / abstract classes\n* Inheritance vs composition\n* Encapsulation\n* Polymorphism\n\n### 2. SOLID Principles\n\n* Single Responsibility\n* Open\u2013Closed\n* Liskov Substitution\n* Interface Segregation\n* Dependency Inversion\n\n### 3. Design Patterns (Practical Use)\n\n* Factory\n* Strategy\n* Singleton (when NOT to use)\n* Observer\n* Builder (optional but good)\n\n### 4. Code Quality\n\n* Separation of concerns\n* Readable naming\n* Modular functions\n* Extensible structure\n* Exception handling\n* Input validation\n\n### 5. Data Structures Used in Code\n\n* HashMap\n* List / Array\n* Queue / Deque\n* Set\n\n### 6. Machine Coding Problem Types\n\n* Parking Lot\n* Library / Book Management\n* Splitwise (simplified)\n* Elevator system\n* Rate limiter\n* Inventory / Order system\n\n### 7. Testing Mindset\n\n* Edge cases\n* Sample test cases\n* Dry run\n* Demo readiness\n\n> \u2757 They care more about **design + extensibility** than fancy algorithms here.\n\n---\n\n# \ud83d\udd39 ROUND 2.2 \u2013 PROBLEM SOLVING & DATA STRUCTURES\n\n> Goal: Evaluate **thinking, approach, correctness**\n\n## \u2705 Data Structures\n\n* Array\n* String\n* HashMap / Dictionary\n* Stack\n* Queue / Deque\n* Linked List\n* Tree (BST, traversal)\n* Graph (BFS / DFS)\n* Heap (priority queue)\n\n## \u2705 Algorithm Patterns (VERY IMPORTANT)\n\n* Two pointers\n* Sliding window\n* Prefix sum\n* Hashing\n* Binary search\n* Recursion\n* Backtracking\n* Dynamic Programming (basic \u2192 medium)\n\n## \u2705 Topics to Master\n\n* Time & space complexity\n* Trade-offs\n* Brute force \u2192 optimized\n* Edge case handling\n\n## \u2705 Common Problem Types\n\n* Duplicate / frequency problems\n* Subarray / substring\n* Parentheses / stack problems\n* Tree traversal\n* Shortest path (basic)\n* Top-K problems\n* DP: Fibonacci, Knapsack, LCS (conceptual)\n\n> \u2757 Explain your thought process clearly. Silence kills this round.\n\n---\n\n# \ud83d\udd39 ROUND 2.3 \u2013 DESIGN ROUND (SYSTEM + OOD)\n\n## \ud83d\udd38 A. Object-Oriented Design (LLD)\n\n### Core Topics\n\n* Class responsibility\n* Relationships (IS-A vs HAS-A)\n* UML (conceptual)\n* Extensibility\n* Design principles\n\n### Problems to Practice\n\n* Parking Lot\n* Elevator System\n* Amazon Locker\n* Library Management\n* Restaurant Ordering\n\n### Key Focus\n\n* Clean class structure\n* Design patterns usage\n* Future requirements handling\n\n---\n\n## \ud83d\udd38 B. System Design (HLD)\n\n### Fundamentals\n\n* Scalability\n* Availability\n* Reliability\n* Latency vs throughput\n* Horizontal vs vertical scaling\n\n### Core Components\n\n* Load balancer\n* Cache (Redis)\n* Database (SQL vs NoSQL)\n* Message queue (Kafka / RabbitMQ)\n* CDN\n* API gateway\n\n### Concepts You MUST Know\n\n* CAP theorem\n* Caching strategies\n* Sharding & replication\n* Stateless services\n* Rate limiting\n\n### Design Problems\n\n* Design Twitter\n* URL Shortener\n* Order Management System\n* Notification system\n* Ride-hailing (high level)\n\n> \u2757 Start with **requirements gathering**. This is the most important step.\n\n---\n\n# \ud83d\udd25 Priority Order (If Time Is Limited)\n\n1\ufe0f\u20e3 Machine Coding\n2\ufe0f\u20e3 DSA Patterns (not random problems)\n3\ufe0f\u20e3 OOD\n4\ufe0f\u20e3 System Design Basics\n\n---\n\nIf you want, next I can:\n\n* Give **exact machine coding questions with solution structure**\n* Create **DSA problem list (Flipkart-style)**\n* Do **mock design walkthrough**\n* Share **mistakes that cause rejection**\n\nJust tell me what to go deeper into.\n"}, {"name": "Prepration.md", "type": "file", "content": "DSA\n------\nleet code - Medium - 100 questions\n\nSQL\n------\nleet code - Medium\n\nData Modeling\n------\n\nSystem Design\n------\n\nLeadership/ conflict management \n------\nMost challenging situation and how did you solve it"}]}, {"name": "Leetcode", "type": "directory", "children": [{"name": "Python", "type": "directory", "children": [{"name": "2sum.md", "type": "file", "content": "\n```python\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -> List[int]:\n        sum = sec=0\n        res=[]\n        for i in range(len(nums)-1): \n            sec=i+1\n            while sec <=len(nums)-1:\n                if nums[i]+nums[sec] != target:\n                    sec+=1\n                elif nums[i]+nums[sec] == target:\n                    res.append(i)\n                    res.append(sec)\n                    return res\n                \n```            \n"}, {"name": "35.Search_Insert_Position.md", "type": "file", "content": "# 35. Search Insert Position\n```python\nclass Solution:\n    def searchInsert(self, nums: List[int], target: int) -> int:\n        if target in nums:\n            return nums.index(target)\n        elif target < nums[0]:\n            return 0\n        elif target > nums[-1]:\n            return len(nums)\n        else:\n            for i in range(len(nums)):\n                if nums[i]> target:\n                    return i\n```\n\n# solution 2\n\n```python\nclass Solution:\n    def searchInsert(self, nums: List[int], target: int) -> int:\n        for num in nums:\n            if num == target:\n                return nums.index(target)\n            else:\n                nums.append(target)\n                nums.sort()\n                return nums.index(target)\n                ```"}, {"name": "3sum.md", "type": "file", "content": "# 3sum\nhttps://www.youtube.com/watch?v=6qgC-dqKElA&list=PLYAlGR1wWgUUyYZ3wX2GdnhiL-QVhAXfR&index=1\n\n```python\nclass Solution:\n    def threeSum(self, nums: List[int]) -> List[List[int]]:\n        res=[]\n        nums.sort()\n\n        length= len(nums)\n\n        for i in range(length-2):\n            if i>0 and nums[i]==nums[i-1]:\n                continue\n            l = i +1\n            r = length - 1\n\n            while l < r:\n                total=nums[i]+nums[l]+nums[r]\n                if total < 0:\n                    l= l+1\n                elif total > 0:\n                    r=r-1\n                else:\n                    res.append([nums[i],nums[l],nums[r]])\n                    while l < r and nums[l]==nums[l+1]:\n                        l=l+1\n                    while l < r and nums[r]==nums[r-1]:\n                        r=r-1\n                    l=l+1\n                    r=r-1\n        return res                \n```"}, {"name": "Remove Duplicates from Sorted Array.md", "type": "file", "content": "# Remove Duplicates from Sorted Array\n```python\nclass Solution:\n    def removeDuplicates(self, nums: List[int]) -> int:\n        if not nums:\n            return 0\n        \n        k = 1\n        for i in range(1, len(nums)):\n            if nums[i] != nums[i - 1]:\n                nums[k] = nums[i]\n                k += 1\n        \n        return k\n```"}, {"name": "happy_number.md", "type": "file", "content": "# Happy Number\n\n\n```python\nclass Solution:\n    def isHappy(self, n: int) -> bool:\n        seen = set()\n        if n < 0:\n            return False\n        while n!=1:\n            if n in seen:\n                return False\n            seen.add(n)\n            res=0\n            for ch in str(n):\n                res=res+int(ch)**2\n            n=res\n        return True\n\n                ``` "}, {"name": "haystack_needle.md", "type": "file", "content": "# Haystack Needle\n```python\nclass Solution:\n    def strStr(self, haystack: str, needle: str) -> int:\n        length = len(needle)\n        for i in range(len(haystack) - length + 1):\n            if haystack[i:i+length] == needle:\n                return i\n        return -1\n```"}, {"name": "longest_substring_with_repeat.md", "type": "file", "content": "# Longest substring without repeating characters \nhttps://www.youtube.com/watch?v=AoWiiN_bwv4&list=PLYAlGR1wWgUUyYZ3wX2GdnhiL-QVhAXfR&index=2\n\n```python\nclass Solution:\n    def lengthOfLongestSubstring(self, s: str) -> int:\n        if len(s) == 0:\n            return 0\n        map = {}\n        max_length =  start = 0\n        for i in range(len(s)):\n            if s[i] in map and start <= map[s[i]]:\n                start=map[s[i]]+1\n            else:\n                max_length=max(max_length,i-start+1)  \n            map[s[i]]=i\n        return (max_length)      \n```        "}, {"name": "palindrome.md", "type": "file", "content": "# Palindrome\n\n```python\nclass Solution:\n    def isPalindrome(self, x: int) -> bool:\n        l=r=0\n        y=str(x)\n        if x < 0:\n            return False\n        elif y==y[::-1]:\n            return True\n        return False\n```   \n\n# solution 2\n```python\ndef is_palindrome_num(x: int) -> bool:\n    if x < 0:\n        return False\n\n    original = x\n    rev = 0\n\n    while x > 0:\n        rev = rev * 10 + x % 10\n        x //= 10\n\n    return original == rev\n```\n"}]}, {"name": "SQL", "type": "directory", "children": [{"name": "176. Second Highest Salary.md", "type": "file", "content": "# Write your MySQL query statement below\n```sql\nwith q1 as\n(\n    select \nid,\nsalary,\ndense_rank() over (order by salary desc) as rk\nfrom employee\n)\n\nselect(  \n    select salary as SecondHighestSalary\nfrom q1\nwhere rk=2 limit 1\n) as SecondHighestSalary\n```"}, {"name": "550. Game Play Analysis IV.md", "type": "file", "content": "# 550. Game Play Analysis IV\nhttps://leetcode.com/problems/game-play-analysis-iv/description/\n```sql\nSELECT\n  ROUND(\n    COUNT(DISTINCT a.player_id) /\n    (SELECT COUNT(DISTINCT player_id) FROM Activity),\n    2\n  ) AS fraction\nFROM Activity a\nJOIN (\n  SELECT\n    player_id,\n    MIN(event_date) AS first_date\n  FROM Activity\n  GROUP BY player_id\n) b\nON a.player_id = b.player_id\nAND a.event_date = DATE_ADD(b.first_date, INTERVAL 1 DAY);\n```"}]}]}, {"name": "Notes", "type": "directory", "children": [{"name": "pyspark_suggestions.md", "type": "file", "content": "Create empty DF\n\nPass variable value from Data bricks to ADF\n\nParameters variable in ADF\n\nHD insights\n\nDelta Lake\n"}, {"name": "python_suggestions.md", "type": "file", "content": "Python\n###############\nStack\nQueue\n\n\u201c\u201d.join()\n\nTuple, list, dict, sets - add/remove/create empty declaration/view\n\nString replace, concat\n\nRegex\n\nHash map analogy using Dictionary\nDefault Dict\n\nSorted(strs)\nReverse=strs[::-1]\n\n\nview index of element\nfruits = [\"apple\", \"banana\", \"cherry\", \"banana\"]\n\nidx = fruits.index(\"banana\")\nprint(idx) \n# Output: 1"}, {"name": "sql_suggestions.md", "type": "file", "content": "Datediff\nDateadd\nDatepart\n\nWindow - unbounded, rows between, preceeding\n\nString concat, replace\n\nRegex\n"}]}, {"name": "Offers", "type": "directory", "children": [{"name": "Google.md", "type": "file", "content": "# Google Offer \u2013 Compensation Details (Anonymous)\n\n| Item | Details |\n| :--- | :--- |\n| **Education** | Computer Science Engineering (Tier-3 college) |\n| **Company** | Google |\n| **Role / Level** | Software Engineer (L4) |\n| **Offer Date** | Nov 2025 |\n| **Joining Date** | Jan 2026 |\n| **Location** | Bangalore |\n| **Experience** | ~3.5 YOE |\n| **Base Salary** | \u20b939.1 LPA |\n| **Performance Bonus** | \u20b95.86 LPA (15% of base) |\n| **Stocks Grant** | $80,000 (vested over 4 years) |\n| **Benefits** | \u20b93.14 LPA (standard benefits) |\n| **Total Compensation (Year 1)** | **~\u20b975 LPA** |\n| **Previous Company** | Large MNC |\n| **Previous CTC** | ~\u20b920\u201322 LPA |\n| **Previous Role** | Senior Software Engineer |\n\n## Additional Information\n*   No sign-on bonus due to company-wide compensation revision (June 2025).\n*   No relocation bonus as both previous office and Google office are in Bangalore.\n*   **Stock Vesting Schedule**: 38% in Year 1.\n\n> *I\u2019ll share my interview experience and preparation strategy in a follow-up post.*\n> *If anyone wants the detailed stock vesting breakdown for all 4 years, I can check and update.*\n"}]}, {"name": "Pyspark", "type": "directory", "children": []}, {"name": "Python", "type": "directory", "children": []}, {"name": "SQL", "type": "directory", "children": []}, {"name": "System_Design", "type": "directory", "children": [{"name": "App_Web_Design", "type": "directory", "children": [{"name": "airbnb.md", "type": "file", "content": "# Airbnb System Design\n\n## 1. Requirements\n\n### Functional\n*   **Host**: Can list properties, upload photos, set availability.\n*   **Guest**: Can search for matching properties (Location, Date, Price).\n*   **Guest**: Can book a property.\n\n### Non-Functional\n*   **High Availability**: Searching must always work.\n*   **Consistency**: Booking must be strictly consistent (No double bookings).\n*   **Latency**: Search results should be < 200ms.\n\n## 2. Capacity Estimation (Back-of-the-Envelope)\n*   **Users**: 100M active monthly.\n*   **Listings**: 10M active listings.\n*   **Search**: Highly read-heavy. 100:1 Read:Write ratio.\n*   **Storage**: 10M listings * 10KB metadata = 100GB (Metadata is small). Photos are large (stored in Blob).\n\n## 3. High-Level Design (HLD)\n\n### Architecture Components\n1.  **Load Balancer**: Distributes traffic.\n2.  **API Gateway**: Auth, Rate Limiting, Routing.\n3.  **Services**:\n    *   **User Service**: Profile mgmt.\n    *   **Listing Service**: Hosts create/edit listings. Writes to SQL (Truth) and Elasticsearch (Search).\n    *   **Search Service**: Read-only, queries Elasticsearch.\n    *   **Booking Service**: Handles reservations and payments.\n4.  **Data Stores**:\n    *   **Relational DB (Postgres/MySQL)**: For Users, Listings, Bookings (ACID required).\n    *   **Search Index (Elasticsearch/Solr)**: For geospatial and full-text search.\n    *   **Redis**: Caching hot listings.\n    *   **Blob Store (S3)**: Images/Videos.\n\n## 4. API Design\n\n*   `GET /api/v1/listings?lat=...&long=...&checkin=...&checkout=...`\n*   `POST /api/v1/bookings`\n    *   Body: `{ listing_id, checkin, checkout, payment_token }`\n*   `POST /api/v1/listings` (Host only)\n\n## 5. Deep Dive\n\n### A. Search & Indexing\n*   **Challenge**: Searching by location + availability.\n*   **Solution**:\n    *   Use **Geohash** or **Quadtree** index in Elasticsearch.\n    *   **Availability Filtering**: It's hard to index \"available dates\" efficiently.\n    *   *Approach*: Filter by location first (smaller dataset), then filter by availability in memory, or use Bitmaps for availability days in the search index.\n\n### B. Booking Consistency (Double Booking)\n*   **Problem**: Users A and B click \"Book\" for the same dates simultaneously.\n*   **Solution**: Distributed Locking or Database Row Locking.\n    *   `START TRANSACTION`\n    *   `SELECT * FROM availability WHERE listing_id=X AND date BETWEEN Y AND Z FOR UPDATE`\n    *   If any row returned is 'booked', ROLLBACK.\n    *   `UPDATE availability SET status='BOOKED'`\n    *   `COMMIT`\n\n### C. Image Storage\n*   Original high-res images uploaded to S3.\n*   Async worker (Lambda/Kafka) triggers resizing (thumbnails, mobile, desktop sizes).\n*   CDN (CloudFront) serves images to reduce latency.\n"}, {"name": "e_commerce.md", "type": "file", "content": "# E-Commerce System Design (Amazon/Shopify)\n\n## 1. Requirements\n\n### Functional\n*   Browse & Search Products.\n*   Add to Cart.\n*   Checkout & Pay.\n*   View Order History.\n\n### Non-Functional\n*   **Scalability**: Handle spikes (Black Friday).\n*   **Availability**: Catalog must always be visible.\n*   **Consistency**: Inventory must be accurate during checkout.\n\n## 2. High-Level Design (HLD)\n\n### Microservices Architecture\n1.  **Web/Mobile App**: Frontend.\n2.  **API Gateway**: Aggregation endpoint.\n3.  **Services**:\n    *   **Catalog Service**: Product metadata (Read Heavy).\n    *   **Cart Service**: Temporary state (Write Heavy).\n    *   **Inventory Service**: Stock management.\n    *   **Order Service**: Lifecycle management.\n    *   **Recommendation Service**: \"Customers/People also bought...\".\n\n## 3. Data Storage\n\n*   **Catalog**: Relational DB (MySQL) for master data + NoSQL/Elasticsearch for search attributes.\n*   **Cart**: Redis (Key=UserID, Value=List<Items>). Fast, ephemeral.\n*   **Orders**: Sharded Relational DB (by OrderID or UserID).\n*   **Historical Data**: Cassandra/HBase for analytics.\n\n## 4. Deep Dive\n\n### A. Inventory Management (The \"Oversell\" Problem)\n*   **Scenario**: 1 item left, 10 users click \"Buy\".\n*   **Option 1: Pessimistic Locking (DB)**\n    *   Lock row when user adds to cart.\n    *   *Cons*: Holds inventory for abandoned carts. Bad UX.\n*   **Option 2: Optimistic Locking (At Checkout)** \u2014 **Preferred**\n    *   Allow add to cart.\n    *   At \"Pay\" button press: `UPDATE stock SET quantity = quantity - 1 WHERE id = X AND quantity > 0`.\n    *   If rows affected = 0, inform user \"Out of Stock\".\n\n### B. Shopping Cart\n*   **Storage**: Redis is ideal for active sessions.\n*   **Persistence**: If user logs in on mobile vs desktop, cart must sync.\n    *   On login, merge Redis cart with \"Persistent Cart\" (DynamoDB/SQL).\n\n### C. Checkout Flow\n1.  **Frontend** sends `PlaceOrder` request.\n2.  **Order Service** creates PENDING order.\n3.  **Payment Gateway** (Stripe/PayPal) is called.\n4.  **Callback**:\n    *   Success: Update Order -> CONFIRMED. Trigger \"Shipping Service\".\n    *   Failure: Update Order -> FAILED. Release inventory.\n"}, {"name": "facebook.md", "type": "file", "content": "# Facebook System Design (News Feed)\n\n## 1. Requirements\n\n### Functional\n*   Post updates (Text, Image).\n*   Add Friends.\n*   View News Feed (Aggregated posts from friends).\n\n### Non-Functional\n*   Latencies: Feed generation < 200ms.\n*   Lag: Posted content should appear reasonably fast (Eventual Consistency ok).\n*   Scale: Billion users.\n\n## 2. API Design\n*   `GET /v1/feed?cursor=...`: Get timeline.\n*   `POST /v1/post`: Create content.\n\n## 3. High-Level Design\n\n### Core Services\n1.  **User Service**: Metadata.\n2.  **Graph Service**: Manages \"A follows B\".\n3.  **Post Service**: Stores content (Bigtable/Cassandra for scale).\n4.  **Feed Generation Service**: The complex engine.\n\n## 4. Deep Dive: Feed Generation\n\n### Approach 1: \"Pull\" Model (Fan-out on Load)\n*   User requests feed.\n*   System queries all ~500 friends: `Get top 5 posts from each`.\n*   Merge and Sort in memory.\n*   **Pros**: Simple storage. Real-time.\n*   **Cons**: Very slow read. N queries per feed load.\n\n### Approach 2: \"Push\" Model (Fan-out on Write) \u2014 *Standard for Standard Users*\n*   User A posts.\n*   System finds all followers of A.\n*   Insert PostID into every follower's pre-computed \"Feed List\" (Redis/Cassandra).\n*   When User B loads feed, just read their Feed List. O(1) read.\n*   **Pros**: Extremely fast reads.\n*   **Cons**: \"Justin Bieber\" problem. If a user has 100M followers, writing to 100M lists takes long.\n\n### Hybrid Approach (Winner)\n*   **Regular Users**: Use Push Model.\n*   **Celebrities**: Use Pull Model.\n*   When User B loads feed: Merge their \"Push\" feed + fetch updates from \"Celebrities\" they follow.\n\n## 5. Storage (Social Graph)\n*   **SQL**: Too many joins (`Users JOIN Friends JOIN Friends`).\n*   **Graph DB (Neo4j)**: Good for traversal.\n*   **Facebook's TAO**: Custom distributed graph store (memcache + MySQL backed) optimized for reads.\n"}, {"name": "food_delivery.md", "type": "file", "content": "# Food Delivery System Design (UberEats/DoorDash)\n\n## 1. Requirements\n*   Customer: Browse menus, Place order, Track delivery.\n*   Restaurant: Receive orders, Update status.\n*   Driver: Receive delivery requests, Navigation.\n\n## 2. High-Level Design (HLD)\n\n### Ecosystem\n*   **Customer App**: Read-heavy (menus). Write (Orders).\n*   **Restaurant App**: Tablet-based. Receives Order notifications.\n*   **Driver App**: Location tracking.\n\n### Backend Services\n1.  **Order Matching Service**: Assigns order to driver.\n2.  **Order Lifecycle Service**: Manages state (PREPARING -> READY...).\n3.  **Location Service**: Ingests driver GPS.\n\n## 3. Deep Dive\n\n### A. Driver Matching (Dispatch System)\n*   Goal: Minimize delivery time (Food ready time + Driver travel time).\n*   **Geospatial Query**: Find drivers within X km of restaurant.\n*   **Algorithm**:\n    *   Not just \"Nearest\".\n    *   Must consider: Driver direction, Current batch (can they take 2 orders?), Traffic.\n*   **Segmented Locking**: When offering an order to Driver A, lock it for 30s so Driver B doesn't grab it.\n\n### B. Real-Time Tracking\n*   **Polling**: Bad for battery and server load.\n*   **WebSockets / Server Sent Events (SSE)**:\n    *   Driver App -> sends GPS (UDP/HTTP) -> Location Service (Redis Geo).\n    *   Location Service -> publishes to Kafka \"driver_updates\".\n    *   Notification Service -> consumes Kafka -> pushes to Customer App via WebSocket.\n\n### C. Menu Management\n*   Menus changes are frequent (Sold out items).\n*   **Caching**: Heavy caching of Menus on CDNs/Redis with short TTL or explicit invalidation when Restaurant updates menu.\n"}, {"name": "netflix_streaming.md", "type": "file", "content": "# Netflix System Design (Streaming)\n\n## 1. Requirements\n*   Upload/Processing of Video (Internal).\n*   Streaming Video (Public).\n*   Search & Recommendation.\n\n## 2. Architecture\n\n### Control Plane vs Data Plane\n*   **Control Plane (AWS)**: Website, Login, Billing, Recommendations, \"Play\" button logic. Handles light traffic (JSON).\n*   **Data Plane (Open Connect CDN)**: Serves the actual Video Files. Handles massive bandwidth (Terabits/sec).\n\n### Services\n1.  **Asset Service**: Metadata (Actors, Genre).\n2.  **Transcoding Service**: Converts raw `.mov` to `.mp4` (H.264, VP9) at various bitrates (480p, 1080p, 4k).\n3.  **Recommendation Engine**: Machine Learning models (Spark/TensorFlow).\n\n## 3. Deep Dive\n\n### A. Video Processing (Transcoding)\n*   Raw master file is huge (TB).\n*   **Chunking**: Split video into 5-minute chunks.\n*   **Parallel Processing**: Process chunks in parallel on thousands of EC2 instances.\n*   **DAG Workflow**: Inspect -> Split -> Transcode(formats) -> Merge -> QCD.\n\n### B. Adaptive Bitrate Streaming (ABS)\n*   Device asks for a manifesto file (`.m3u8`).\n*   Manifesto lists URLs for different qualities (Low, Med, High).\n*   Client Player monitors bandwidth.\n    *   Slow net? Request \"Low\" chunk.\n    *   Fast net? Request \"High\" chunk.\n*   **Pre-computing**: Netflix pre-transcodes all resolutions. YouTube does some on-the-fly, but Netflix goes for quality.\n\n### C. CDN Designing (Open Connect)\n*   Netflix builds custom hardware (OCAs) placed inside ISP data centers.\n*   **Proactive Caching**: \"Stranger Things\" S2 is coming out? Push the file to the ISP boxes at 3 AM *before* launch. user fetches from local ISP box, not crossing internet backbone.\n"}, {"name": "uber.md", "type": "file", "content": "# Uber System Design (Ride Sharing)\n\n## 1. Requirements\n*   Rider: Request ride, See ETA.\n*   Driver: Accept ride, See Location.\n*   System: Match Rider to Driver efficiently.\n\n## 2. High-Level Architecture\n*   **DISCO (Dispatch System)**: The brain.\n*   **Geospatial Service**: Tracks drivers.\n*   **Trip Service**: Manages trip state.\n\n## 3. Deep Dive: Geospatial Indexing\n\n### How to handle millions of updates?\n*   Naive: `SELECT * FROM Drivers WHERE distance < 1km`. (Full table scan = Fail).\n*   **Geohash**:\n    *   Divide world into grid strings (e.g., \"dr5r...\" is NYC).\n    *   Drivers in same cell share prefix.\n    *   Lookups are string matches.\n*   **QuadTree**:\n    *   In-memory tree structure.\n    *   Root -> 4 quadrants -> 4 sub-quadrants.\n    *   Leaf node contains driver list.\n    *   If a cell gets too full (> 500 drivers), split it.\n    *   **Google S2 Library**: Maps sphere to 1D integer (Hilbert Curve). Preserves locality.\n\n### Architecture for Updates\n1.  Driver app sends Lat/Lon every 4s.\n2.  Load Balancer -> **Location Service**.\n3.  Update **Redis** (Ephemeral location).\n4.  Do *not* write every point to DB.\n5.  *Only* write to DB (Cassandra/RDBMS) when Trip Starts/Ends for billing.\n\n## 4. Handling Surge Pricing\n*   Demand > Supply in a Geohash Area.\n*   Service calculates multiplier.\n*   Update applied to area for X minutes.\n*   **Consistency**: User sees 1.5x, they must be charged 1.5x even if it drops 1 min later. Lock the price at \"Request\" time.\n"}, {"name": "youtube.md", "type": "file", "content": "# YouTube System Design\n\n## 1. Requirements\n*   Upload Video (GB sizes).\n*   View Video (Streaming).\n*   Search/Recommendations.\n\n## 2. Architecture\n\n### Components\n1.  **Web Server**: Api handling.\n2.  **Processing Queue**: Kafka.\n3.  **Encoder**: Worker nodes (Ffmpeg).\n4.  **Metadata DB**: Sharded MySQL (Vitess).\n5.  **Blob Storage**: GCS/S3.\n\n## 3. Deep Dive\n\n### A. Video Upload Flow\n1.  User sends file to **Upload Service**.\n2.  File stored in **Blob Storage** (Original raw).\n3.  Message added to **Encoding Queue**.\n4.  **Encoder** picks up job:\n    *   Checks for duplicates (Hash matching).\n    *   Generates Thumbnails.\n    *   Transcodes to .mp4, .webm, different resolutions.\n5.  New files stored in Blob Storage.\n6.  Metadata (URLs/Size) updated in DB.\n\n### B. Deduplication\n*   Compute SHA-256 of file.\n*   Complex: User uploads 10s clip of a movie.\n*   **Content ID**: Fingerprinting audio/video frames to match against Copyright DB.\n\n### C. Scaling Metadata (Vitess)\n*   YouTube uses MySQL.\n*   Trillions of videos/comments.\n*   **Sharding Key**: `VideoID`.\n*   All comments for a video sit on the same shard for fast retrieval.\n*   **Master-Slave Routing**: Writes to Master, Reads from Replicas.\n\n### D. Streaming Optimization\n*   **Range Requests**: HTTP Header `Range: bytes=0-1000`. Allows seeking to middle of video without downloading start.\n*   **Edge Caching**: Popular videos cached at ISP/Edge. Long-tail videos fetched from origin.\n"}]}, {"name": "Data_pipeline_design", "type": "directory", "children": [{"name": "airbnb.md", "type": "file", "content": "# Airbnb Data Engineering System Design\n\n## 1. Scenario\nAirbnb needs to process daily bookings, listings, and user interactions to generate business dashboards (Revenue, Occupancy Rates) and power Data Science models.\n\n## 2. Requirements\n*   **Batch Processing**: Most metrics (Revenue) are calculated daily (T-1).\n*   **Data Quality**: \"Gold\" tables must be accurate. Bad data should stop the pipeline.\n*   **Backfill**: Ability to re-process historical data if logic changes.\n*   **Evolution**: Schema evolution must be handled.\n\n## 3. Architecture: The \"Airflow\" Pattern\n\n### Architecture Flow\n`MySQL / Event Logs` --> `Sqoop` --> `S3 (Landing/Bronze)` --> `Spark (Transformation)` --> `Airflow (Orchestrator)` --> `S3 (Gold)` --> `Druid (Dashboard)`\n\n\n### Components\n1.  **Sources**:\n    *   **MySQL (RDS)**: Production DB (Listings, Bookings).\n    *   **Event Logs**: Thrift/JSON events from web servers.\n2.  **Ingestion**:\n    *   **Sqoop/DMS**: Daily snapshot of MySQL tables -> S3 (Landing Zone).\n3.  **Storage (Data Lake)**: S3 (Parquet format) partitioned by `ds` (Datestamp).\n4.  **Compute**: Apache Spark (EMR/Databricks) for transformations.\n5.  **Orchestration**: **Apache Airflow**.\n\n## 4. Pipeline Steps (DAG Design)\n\n### The \"Gold\" Dashboard Pipeline\n1.  **Sensor**: Wait for `listings_partition_2024-01-01` to arrive in S3.\n2.  **Data Quality Check (Bronze)**:\n    *   Run *Great Expectations* or simple SQL check.\n    *   `ASSERT count(*) > 0`\n    *   `ASSERT price > 0`\n3.  **Transformation (Bronze -> Silver)**:\n    *   Spark Job: Join `Bookings` with `Listings`.\n    *   De-duplicate records.\n    *   Standardize currency (USD conversion).\n4.  **Transformation (Silver -> Gold)**:\n    *   Aggregate: `SELECT sum(revenue) GROUP BY city`.\n5.  **Publish**:\n    *   Write to **Druid/Presto** for low-latency dashboarding.\n    *   Or write to **Redshift/Snowflake** for BI analysts.\n\n## 5. Deep Dive: Data Quality & Anomaly Detection\nAirbnb created \"Anomalies\" detection features in Airflow.\n*   **Logic**: If today's row count is 50% less than the 7-day average, send `SLACK_ALERT` and PAUSE the pipeline.\n*   **Partitioning**: Hive-style partitioning `s3://bucket/table/ds=YYYY-MM-DD/`. This allows atomic swaps and easy backfills.\n"}, {"name": "e_commerce.md", "type": "file", "content": "# E-Commerce Data Engineering Design\n\n## 1. Scenario\nReal-time inventory management and recommendation personalization based on user browsing history.\n\n## 2. Requirements\n*   **Latency**: Inventory updates < 10 seconds. Recommendations < 5 minutes.\n*   **Change Capture**: Capture every state change (Insert/Update/Delete) from the Order DB.\n*   **Stream & Batch**: Unified storage for historical analysis (Lambda/Kappa).\n\n## 3. Architecture: CDC & Delta Lake\n\n### Architecture Flow\n`PostgreSQL (Orders)` --> `Debezium (CDC)` --> `Kafka` --> `Spark Streaming / Flink` --> `Delta Lake (S3)` --> `Redis (Real-time View)`\n\n\n### Components\n1.  **Source Database**: PostgreSQL (Orders, Inventory).\n2.  **CDC Tool**: **Debezium**.\n    *   Reads Postgres Write-Ahead Log (WAL).\n    *   Publishes changes to Kafka topics `db.orders`, `db.inventory`.\n3.  **Streaming Platform**: **Apache Kafka**.\n4.  **Stream Processing**: **Spark Structured Streaming** or **Flink**.\n5.  **Sink/Storage**: **Delta Lake** (ACID on S3).\n\n## 4. Pipeline Design\n\n### A. Real-Time Inventory (CDC)\n1.  **Capture**: User buys item. DB updates `qty=99`.\n2.  **Ingest**: Debezium sends `{op: \"u\", before: {qty:100}, after: {qty:99}}` to Kafka.\n3.  **Process**:\n    *   Spark Stream reads Kafka.\n    *   Updates \"Real-time Stock\" Redis cache for the frontend.\n    *   triggers \"Low Stock Alert\" if `qty < threshold`.\n4.  **Archive**: Stream writes raw JSON to S3 (Delta Lake) for history.\n\n### B. Clickstream (Recommendations)\n1.  **Source**: JS Tracker sends events (Click, View, AddToCart) to API Gateway.\n2.  **Ingest**: Firehose/Kafka.\n3.  **Processing**:\n    *   Sessionize events (Window: 30 minutes).\n    *   Extract \"Interests\" (e.g., User viewed \"Shoes\" 5 times).\n    *   Update **User Feature Store** (Cassandra/DynamoDB).\n4.  **Serving**: When user visits homepage, RecSys Service queries Feature Store.\n\n## 5. Deep Dive: Schema Evolution in Streams\n*   **Problem**: Dev adds `color` column to Postgres. Debezium sends new JSON structure. Spark Job crashes.\n*   **Solution**:\n    *   **Schema Registry** (Confluent): Enforces Avro schema compatibility.\n    *   **Delta Lake Schema Evolution**: `option(\"mergeSchema\", \"true\")`. Automatically adds the new column to the parquet files.\n"}, {"name": "facebook.md", "type": "file", "content": "# Facebook Data Engineering Design\n\n## 1. Scenario\nFacebook generates Petabytes of logs per hour (Likes, Comments, Scroll depth, Errors). We need to ingest this for Ad-hoc queries, Real-time dashboards, and Abuse detection.\n\n## 2. Requirements\n*   **Scale**: Handle Trillions of events/day.\n*   **Latency**: Query recent data within seconds.\n*   **Storage**: Efficient compression and tiered storage.\n\n## 3. Architecture: The Log Aggregation Pipeline\n\n### Architecture Flow\n`App Logs` --> `Scribe (Collector)` --> `Kafka (Buffer)` --> `Druid (Real-time OLAP)` & `HDFS (Batch/Hive)` --> `Superset (Analytics)`\n\n\n### Components\n1.  **Ingestion**: **Scribe** (Facebook's internal log collector, similar to Fluentd).\n    *   Agents on every web server push logs to Scribe aggregators.\n2.  **Buffer**: **Kafka** (or FB's LogDevice).\n    *   Decouples producers from consumers.\n    *   Topics: `fb_clicks`, `fb_errors`.\n3.  **Real-Time Layer**: **Apache Druid** / **Pinot**.\n    *   Consumes from Kafka.\n    *   Builds OLAP cubes for sub-second slicing/dicing.\n4.  **Batch Layer**: **HDFS / Hive**.\n    *   Kafka -> Secor/Gobblin -> HDFS (ORC Format).\n    *   Used for daily ETL and heavy ML training.\n\n## 4. Pipeline Design\n\n### A. Real-Time Ad Analytics\n*   **Goal**: Advertiser wants to see \"Clicks vs Impressions\" right now.\n*   **Flow**:\n    1.  App logs `AdImpression` event.\n    2.  Scribe pushes to `ads_stream` Kafka topic.\n    3.  **Druid** consumes stream. roll-up enabled (Store sum, not raw rows).\n    4.  Advertiser Dashboard queries Druid: `SELECT sum(clicks) FROM ads WHERE advertiser_id=123`.\n\n### B. Graph Processing (Friends Recommendation)\n*   **Tool**: **Apache Giraph** (or Spark GraphX).\n*   **Flow**:\n    1.  Daily snapshot of \"Friendship\" table (Edge List).\n    2.  Run PageRank or Label Propagation algorithm.\n    3.  Output \"Suggested Friends\" to Key-Value Store (RocksDB) for serving.\n\n## 5. Deep Dive: Handling \"The Justin Bieber\" Partition\n*   **Problem**: Data Skew. A celebrity has millions of interactions. One Kafka partition gets flooded, causing lag.\n*   **Solution**:\n    *   **Salting**: Add a random number `(0-9)` to the partition key for hot keys. Distribution becomes even.\n    *   **Partial Aggregation**: Aggregate in memory (Producer side) before sending network packets.\n"}, {"name": "food_delivery.md", "type": "file", "content": "# Food Delivery Data Engineering Design (UberEats)\n\n## 1. Scenario\nCalculate dynamic \"Estimated Time of Arrival\" (ETA) and \"Delivery Fees\" based on real-time driver locations, traffic, and restaurant state.\n\n## 2. Requirements\n*   **Stateful Processing**: Need to remember \"Driver A is currently on Order #123\".\n*   **Geospatial**: Efficient ingestion of Lat/Lon streams.\n*   **ML Integration**: Features must be served to ML models instantly.\n\n## 3. Architecture: Stream Processing (Flink)\n\n### Architecture Flow\n`Driver GPS` --> `Kafka (Ingestion)` --> `Flink (Stateful Processing)` --> `Redis (ETA Cache)` --> `Iceberg (Data Lake)`\n\n\n### Components\n1.  **Source**: Driver GPS stream ~ every 5 seconds per driver (MQTT/WebSocket -> Kafka).\n2.  **Processing Engine**: **Apache Flink**.\n    *   True streaming (Event time processing).\n    *   Stateful (RocksDB backend).\n3.  **Serving Layer**: **Redis** (Hot state), **Cassandra** (Order history).\n4.  **Data Lake**: **Apache Iceberg** (Geospatial partitioning).\n\n## 4. Pipeline Design\n\n### A. Dynamic ETA Calculation\n1.  **Stream 1**: `DriverLocations` (DriverID, Lat, Lon, Time).\n2.  **Stream 2**: `Orders` (OrderID, RestaurantLat, Lon, Status).\n3.  **Flink Job**:\n    *   Window Join (Interval Join).\n    *   Calculate distance (Haversine or Mapbox Router).\n    *   Apply ML Model (Traffic factor).\n    *   Emit `PredictedETA`.\n4.  **Sink**: Update Customer App (Websocket) and write to `OrderEvents` table.\n\n### B. Sessionization (Driver Efficiency)\n*   **Goal**: Calculate how much time a driver waits at restaurants per day.\n*   **Logic**:\n    *   Identify \"Arrival\" event (Driver enters Geofence).\n    *   Identify \"Pickup\" event (Driver swipes 'Picked Up').\n    *   `WaitTime = PickupTime - ArrivalTime`.\n    *   Flink Session Window key functionality.\n\n## 5. Deep Dive: Backfill in Streaming (Kappa)\n*   **Scenario**: We improved the ETA algorithm. We need to re-process last month's data to test it.\n*   **Batch as Stream**:\n    *   Read historical data from Iceberg Lake.\n    *   Pipe it through the *same* Flink job used for production.\n    *   Sink to a \"Shadow Topic\" for verification.\n    *   No separate \"Batch Codebase\" needed (Unified API).\n"}, {"name": "netflix_streaming.md", "type": "file", "content": "# Netflix Data Engineering Design\n\n## 1. Scenario\nNetflix (\"Keystone\" Platform) handles routing of data events from every device (TV, Mobile) to sinks (S3, Elastic, Kafka) for billing, personalization, and operational monitoring.\n\n## 2. Requirements\n*   **Routing**: Dynamic routing. \"Billings logs go to S3 Bucket A\", \"Error logs go to Elasticsearch\".\n*   **Scale**: 500 Billion events/day.\n*   **Partitioning**: Efficient lake storage (Iceberg).\n\n## 3. Architecture: The Keystone Pipeline\n\n### Architecture Flow\n`User Events` --> `Keystone (Kafka)` --> `Flink (Router & Masking)` --> `S3 (Iceberg Table)` --> `Trino (Interactive Query)`\n\n\n### Components\n1.  **Fronting**: REST Proxy / DG (Data Gateway) accepts events.\n2.  **Messaging**: **Kafka** (Partitioned by UserID or DeviceID).\n3.  **Routing**: **Flink / Kafka Connect**.\n    *   Projector: Filters/Masks PII.\n    *   Router: Decides destination.\n4.  **Data Lake**: S3 + **Apache Iceberg**.\n    *   Iceberg provides ACID transactions on S3 (fixes \"File-listing\" slowness).\n\n## 4. Pipeline Design\n\n### A. The \"Play\" Event Pipeline\n1.  User clicks Play. App sends event `{evtType: \"P\", movieId: 123, profileId: 456, time: ...}`.\n2.  **Ingest**: Kafka Topic `user_interactions`.\n3.  **Route 1 (Personalization)**:\n    *   Flink job aggregates \"Watch Duration\".\n    *   Writes to Cassandra (Profile Store).\n    *   RecSys updates \"Continue Watching\" list.\n4.  **Route 2 (Data Warehouse)**:\n    *   Writes parquet files to S3 `s3://warehouse/interaction_logs/date=...`.\n    *   Iceberg commits the metadata.\n\n### B. Handling Late Data\n*   **Scenario**: User watches offline (Airplane). Mobile uploads logs 5 hours later.\n*   **Solution**:\n    *   Iceberg handles late-arriving data via hidden partitioning.\n    *   Query `SELECT * FROM logs` sees the data immediately after commit, even if it belongs to a past partition.\n\n## 5. Deep Dive: Data Mesh & Standardization\nNetflix pioneered \"Paved Road\" (Platform Engineering).\n*   **Genie**: A federated job execution service.\n*   **Metacat**: Unified Metadata catalog (Hive + RDS + Teradata).\n*   **Audit**: Automate PII detection (Regex for credit cards) in the stream before writing to Lake.\n"}, {"name": "uber.md", "type": "file", "content": "# Uber Data Engineering Design\n\n## 1. Scenario\nUber manages \"Trip\" audits, billing updates, and rider history. A trip has a lifecycle (Request -> Driver Arrived -> Start -> End -> Tip Added). The \"Tip Added\" event happens hours after the trip ends.\n\n## 2. Requirements\n*   **Upserts (Update/Insert)**: We must update the \"Trip\" record in the Data Lake when a Tip arrives. Standard S3 (Parquet) is immutable (Write Once).\n*   **Incremental Processing**: Downstream jobs should only process \"New/Changed\" trips, not scan the whole table.\n\n## 3. Architecture: Apache Hudi & Kappa\n\n### Architecture Flow\n`Trip Events` --> `Kafka` --> `Hudi (Kappa Upserts)` --> `HDFS/S3` --> `Hive / Presto (Serving)`\n\n\n### Components\n1.  **Ingestion**: Kafka.\n2.  **Lake Format**: **Apache Hudi** (Hadoop Upsert Delete and Incremental).\n    *   Enables `UPDATE` on S3.\n3.  **Processing**: Spark / Flink.\n4.  **Serving**: Presto / Hive.\n\n## 4. Pipeline Design\n\n### A. The Trip Lifecycle (Upsert Problem)\n1.  **Event 1 (Trip End)**: `{trip_id: 100, status: \"ENDED\", fare: $20}`.\n    *   Hudi inserts new record.\n2.  **Event 2 (Tip Added)**: `{trip_id: 100, tip: $5, total: $25}`.\n    *   **Standard Lake**: Would require rewriting the whole partition or creating a duplicate row.\n    *   **Hudi**: Locates the file containing `trip_id: 100` and creates a new version (Copy-On-Write or Merge-On-Read).\n3.  **Query**: `SELECT * FROM trips WHERE trip_id=100` returns the latest state ($25).\n\n### B. Incremental ETL\n*   Daily Revenue Report needs to process only today's trips + yesterday's trips that got adjusted today.\n*   **Hudi functionality**: `hoodie_table.incremental_query(start_timestamp)`.\n*   Spark Job reads *only* changed rows. No full table scan.\n\n## 5. Deep Dive: Hudi Indexing\n*   How does Hudi know which file contains `trip_id: 100`?\n    *   **Bloom Filter Index**: Kept in the footer of parquet files. Fast check \"Maybe in this file\".\n    *   **HBase Index**: External KV store mapping Key -> FileID.\n"}, {"name": "youtube.md", "type": "file", "content": "# YouTube Data Engineering Design\n\n## 1. Scenario\nCorrectly counting \"Views\" is critical (Monetization). We need a system that is fast (User satisfaction) and accurate (Billing).\n\n## 2. Requirements\n*   **Accuracy**: Ads are paid per 1000 views. No over/under counting.\n*   **Deduplication**: Spam protection (bot farms).\n*   **Latency**: View count should update reasonably fast (e.g., \"301 views\" phenomenon).\n\n## 3. Architecture: Lambda Architecture\n\n### Architecture Flow\n`Video Upload` --> `Blob Store` --> `Kafka (Event)` --> `Spark Streaming (Speed)` & `MapReduce (Batch)` --> `Serving Layer (Views DB)`\n\n\n### Components\n1.  **Speed Layer (Approximate)**:\n    *   Kafka -> Spark Streaming / Storm.\n    *   Lossy counting or simple aggregation per minute.\n    *   Updates Redis/Cassandra.\n    *   *Result*: \"301 Views\" (Fast, but maybe innacurate/duplicated).\n2.  **Batch Layer (Accurate)**:\n    *   Raw logs -> S3.\n    *   MapReduce / Spark Batch.\n    *   Complex Spam Filtering Algorithms (Graph analysis, IP reputation).\n    *   *Result*: \"Verified Views\" (Computed daily/hourly).\n3.  **Serving Layer**:\n    *   Merges Speed + Batch.\n    *   If Batch > Speed (Batch caught up), show Batch. Else show Speed.\n\n## 4. Pipeline Design\n\n### A. View Counting Pipeline\n1.  **Log**: Client sends `ViewEvent` heartbeat every 30s.\n2.  **Deduplication (Batch)**:\n    *   Group by `(video_id, user_ip, session_id)`.\n    *   If > 50 views from same IP in 1 hour -> Mark as Spam.\n3.  **Aggregation**:\n    *   `SELECT video_id, count(*) FROM legitimate_views GROUP BY video_id`.\n4.  **Publication**:\n    *   Update `VideoMetadata` table (Sharded MySQL/Vitess).\n\n### B. Thumbnail Generation (ETL for Binary Data)\n1.  **Trigger**: Video Upload to Blob Store.\n2.  **Workflow (Airflow)**:\n    *   Extract frame at timestamp 00:10.\n    *   Resize to 1080p, 720p, 480p.\n    *   Upload images to CDN Origin.\n    *   Update Metadata DB with Image URLs.\n\n## 5. Deep Dive: \"The 301 Views\"\n*   Why did YouTube pause at 301?\n*   Because the **Switchover** from Speed Layer to Batch Layer was manual or hard-coded at that threshold.\n*   The system pauses public updates while the Batch Layer performs a rigorous spam check. Once verified, the counter unlocks.\n"}]}]}, {"name": "airbnb_interview_answers.md", "type": "file", "content": "# Airbnb Data Engineering Interview Answers\n*Based on Senior DE Competencies*\n\nDetailed, architectural, and production-ready answers for Airbnb-style Data Engineering interview questions.\n\n---\n\n## 1\ufe0f\u20e3 Data Pipeline Design & Architecture\n\n### 1. Merchandising Optimization Pipeline\n**Q:** Design an end-to-end pipeline to capture user interactions (clicks, searches) for merchandising.\n**A:**\n*   **Ingestion:**\n    *   **Source:** Event Logging Service (Backend APIs) emits events to **Kafka** topics (partitioned by `user_id` to guarantee ordering).\n    *   *Why Kafka?* High throughput, decoupling, and playback capability.\n*   **Processing (Lambda Architecture):**\n    *   **Speed Layer (Real-time):** **Spark Structured Streaming** or **Flink** reads from Kafka -> Aggregates clicks/views per Listing ID (1-min windows) -> Writes to **Redis/Cassandra** for immediate ranking updates.\n    *   **Batch Layer (Accurate):** **Secor/Kafka Connect** dumps raw events to S3 (Bronze). **Airflow** triggers nightly Spark jobs to deduplicate, join with Dimensions (User demographics), and aggregate metrics (Silver/Gold) -> Writes to **Data Warehouse (Delta Lake/Iceberg)**.\n*   **Serving:**\n    *   Search Services query Redis for real-time signals.\n    *   DS/BI tools query Data Warehouse for historical analysis and Model Training.\n\n### 2. Handling Late-Arriving Events\n**Q:** How do you handle late-arriving events?\n**A:**\n*   **Streaming:** Use **Watermarking** in Spark/Flink. Define a threshold (e.g., \"accept data up to 1 hour late\").\n    *   *Within threshold:* Re-aggregate and update the state.\n    *   *Beyond threshold:* Drop or send to a **Dead Letter Queue (DLQ)** (S3 bucket) for manual inspection/backfill.\n*   **Batch:** Partition data by **Event Date** (business date), not Processing Date.\n    *   If yesterday's data arrives today, it lands in `date=2023-10-25`.\n    *   Rerun the DAG for `2023-10-25` to include the late data (Idempotent replay).\n\n### 3. Idempotency strategies\n**Q:** How to make pipelines idempotent?\n**A:**\n*   **Definition:** Running the pipeline multiple times produces the same result.\n*   **Strategies:**\n    1.  **Overwriting Partitions:** Instead of `INSERT INTO`, use `INSERT OVERWRITE partition(date='...')`.\n    2.  **Upserts (Merge):** Use unique keys (e.g., `event_id`) to Merge data. `MERGE INTO target USING source ON t.id = s.id ...`.\n    3.  **Deduplication:** Always deduplicate raw data by unique ID in the first transformation step.\n\n---\n\n## 2\ufe0f\u20e3 Spark, SparkSQL & Scala\n\n### 1. Execution Model (DAG, Stages, Tasks)\n**Q:** Explain Spark's execution model.\n**A:**\n*   **Application:** User program (Driver + Executors).\n*   **Job:** Triggered by an **Action** (e.g., `.count()`, `.write()`).\n*   **Stage:** Jobs are divided into Stages by **Shuffle Boundaries** (Wide transformations like `groupBy`, `join`).\n    *   *Optimization:* Too many stages = too much I/O.\n*   **Task:** The smallest unit of work. One task per Partition. Sent to Executors.\n*   **Pipeline:** Spark chains Narrow Transformations (Map, Filter) into a single task (Pipelining) so data isn't written to disk between them.\n\n### 2. Catalyst Optimizer\n**Q:** How does it work?\n**A:**\n1.  **Analysis:** Validates column names/references against the Catalog.\n2.  **Logical Optimization:** Applies heuristics:\n    *   *Predicate Pushdown* (Filter before Join).\n    *   *Column Pruning* (Read only needed columns).\n    *   *Constant Folding* (`1+1` -> `2`).\n3.  **Physical Planning:** Chooses strategies (HashJoin vs SortMergeJoin) based on cost/stats.\n4.  **Code Generation:** Generates optimized Java bytecode (Project Tungsten) to minimize CPU instructions.\n\n### 3. Handling Data Skew\n**Q:** How do you handle High Skew (e.g., one 'Guest' clicking 1M times)?\n**A:**\n*   **Symptom:** One task runs forever; others finish fast. OOM errors.\n*   **Fix 1: Salting (For Large Joins):**\n    *   Add a random number (0-9) to the skew key in the large table.\n    *   Explode the joining key in the small table (replicate rows 0-9).\n    *   Join on Key + Salt. This disperses the skew bucket into 10 smaller buckets.\n*   **Fix 2: Broadcast Join:**\n    *   If one table is small, broadcast it to avoid the shuffle entirely.\n*   **Fix 3: Filter Nulls:** Often skew is caused by `null` keys. Filter them out first.\n\n---\n\n## 3\ufe0f\u20e3 Airflow & Orchestration\n\n### 1. Airflow Architecture\n**Q:** How does Airflow work?\n**A:**\n*   **Scheduler:** The brain. Parses DAGs, checks schedules, checks dependencies, creates \"Task Instances\" in the DB.\n*   **Executor (Kubernetes/Celery):** Picks up \"Queued\" tasks and assigns them to Workers.\n*   **Worker:** Executes the actual code (or submits the Spark Application).\n*   **Metastore (Postgres):** Stores the state of every DAG run and Task instance.\n\n### 2. Backfills\n**Q:** How to implement backfills safely?\n**A:**\n*   **Idempotency is Key:** Ensure re-running a date range overwrites cleanly.\n*   **Catchup:** Set `catchup=False` generally to avoid accidental floods. For intentional backfill, instantiate DAG runs for the historical period using CLI `airflow dags backfill ...`.\n*   **Scaling:** Limit concurrency (`max_active_runs`) during backfill to avoid DDOSing systems.\n\n---\n\n## 4\ufe0f\u20e3 Data Modeling (Merchandising)\n\n### 1. Modeling Listing Interactions\n**Q:** Model data for Listing Merchandising.\n**A:**\n*   **Fact Table:** `fct_listing_views`\n    *   `view_id` (PK), `listing_id` (FK), `viewer_id` (FK), `timestamp`, `platform`, `duration_sec`, `is_booked` (Boolean).\n*   **Dimension Table:** `dim_listings` (SCD Type 2)\n    *   `listing_id`, `host_id`, `neighborhood`, `room_type`, `price`, `active_from`, `active_to`, `is_current`.\n*   **Aggregated Table (Merchandising):** `agg_listing_daily_metrics`\n    *   `listing_id`, `date`, `total_views`, `conversion_rate`, `avg_time_on_page`.\n    *   *Usage:* Used by the Ranking Algorithm to promote high-converting listings.\n\n### 2. SCD Type 2\n**Q:** How to handle Price Changes over time?\n**A:**\nUse SCD Type 2 on the Listing Dimension.\n*   **Columns:** `price`, `valid_from`, `valid_to`, `is_current`.\n*   **Update Logic:** When price changes from $100 to $120:\n    1.  Update old row: `valid_to` = Now, `is_current` = False.\n    2.  Insert new row: `price` = 120, `valid_from` = Now, `valid_to` = 9999-12-31, `is_current` = True.\n\n---\n\n## 5\ufe0f\u20e3 Data Quality & Reliability\n\n### 1. Data Contracts & Validation\n**Q:** How do you design data validation?\n**A:**\n*   **Shift Left:** Validate schema at the source (Schema Registry).\n*   **Write-Audit-Publish:**\n    *   Write data to a *Staging* table.\n    *   Run checks (Great Expectations / dbt tests): `count > 0`, `unique_id`, `null_rate < 1%`.\n    *   If Pass -> Swap/Merge to Production.\n    *   If Fail -> Alert and Halt. Do NOT corrupt Prod.\n*   **Contracts:** Use explicit YAML contracts (e.g., OpenLineage) defining schema and ownership between Producers and Data Engineering.\n\n---\n\n## 6\ufe0f\u20e3 Distributed Systems & Scale\n\n### 1. Kafka Ordering\n**Q:** How does Kafka guarantee ordering?\n**A:**\n*   Kafka guarantees order **only within a Partition**, NOT across the whole topic.\n*   **Design:** Producer must hash the ordering key (e.g., `user_id`) so all events for User A go to Partition 5.\n*   Consumer reads Partition 5 sequentially, preserving the order of user actions.\n\n### 2. Hot Partitions\n**Q:** How to handle hot partitions (one partition taking too much traffic)?\n**A:**\n*   **Cause:** Poor partition key choice (e.g., partitioning by 'Country' and 'USA' is 90% of traffic).\n*   **Solution:**\n    *   Change Key: Use finer grain (e.g., `user_id` instead of `country`).\n    *   **Random Partitioning:** If ordering isn't strictly required at ingestion, randomize partitioning to distribute load, then re-sort downstream.\n\n---\n\n## 7\ufe0f\u20e3 Behavioral & Product SRE\n\n### 1. Working with DS/PM\n**Q:** How do you prioritize Requests?\n**A:**\n*   \"I prioritize based on **Business Impact vs Effort** matrix.\"\n*   *Example:* \"PM wanted a real-time dashboard. I asked 'What decision will you make in 5 minutes that you can't make in 1 hour?'. They realized hourly batch was sufficient. This saved 3 weeks of engineering time vs building a streaming pipeline.\"\n\n### 2. Tech Debt\n**Q:** How do you handle Tech Debt?\n**A:**\n*   \"I follow the 'Boy Scout Rule': Leave the code better than you found it.\"\n*   \"I advocate for 20% innovation/cleanup time in Sprint planning to migrate legacy pipelines.\"\n\n---\n\n## \ud83d\udd1f Airbnb System Design: Merchandising Platform\n\n**Goal:** Real-time & Batch optimization for Listings.\n\n**1. Data Sources:**\n*   User Activity (Kafka): Clicks, Bookings.\n*   Listing Metadata (MySQL/CDC): Price, Description changes.\n\n**2. Ingestion:**\n*   **Debezium:** Captures Listing DB changes (CDC) -> Kafka.\n*   **Event Collector:** App sends Clicks -> Kafka.\n\n**3. Processing:**\n*   **Real-time (Flink/Spark Streaming):** \n    *   Ingest Clicks & CDC.\n    *   Join Click + Listing Metadata (Enrichment). Use State Management/Broadcast State for Metadata.\n    *   Calc \"Hot Listings\" (Views last 10 mins).\n    *   Sink -> **Redis** (Feature Store) for Search Ranker API to read ms-latency.\n*   **Batch (Spark/Airflow):**\n    *   Ingest raw Kafka to Data Lake (S3).\n    *   Calculate complex metrics (e.g., \"7-day Conversion Rate vs Neighborhood Average\").\n    *   Sink -> **Hive/Iceberg**.\n\n**4. Storage:**\n*   **Bronze:** Raw JSON (S3).\n*   **Silver:** Cleaned, Deduplicated (Delta Lake).\n*   **Gold:** Aggregated Metrics (Delta Lake).\n*   **Feature Store:** Redis (Online), Iceberg (Offline).\n\n**5. Consumer:**\n*   **Search Service:** Queries Redis to boost \"Hot\" listings.\n*   **Merchandising DS Team:** Queries Gold tables to train \"Ranking Model\".\n\n**6. Quality:**\n*   Circuit Breakers on Spark Jobs (Validation).\n*   Lag monitoring on Kafka Consumers.\n"}, {"name": "azure_data_factory_interview_questions.md", "type": "file", "content": "# Azure Data Factory (ADF) Interview Questions\n*Mid to Senior Level*\n\nA comprehensive guide to Azure Data Factory scenarios, focusing on orchestration, data movement, and integration challenges.\n\n---\n\n## Table of Contents\n1. [Core Components & Architecture](#core-components--architecture)\n2. [Integration Runtimes (IR)](#integration-runtimes-ir)\n3. [Data Flows & Transformation](#data-flows--transformation)\n4. [Triggers & Scheduling](#triggers--scheduling)\n5. [Scenarios & Performance](#scenarios--performance)\n\n---\n\n## Core Components & Architecture\n\n### 1. Linked Service vs Dataset\n**Q:** What is the relationship between a Linked Service and a Dataset?\n**A:**\n*   **Linked Service:** The connection string. It defines *how* to connect to an external source (e.g., \"Connect to Azure Blob Storage using this Key\"). Like a connection string in code.\n*   **Dataset:** The specific data structure. It points to a specific file or table *within* that Linked Service (e.g., \"The file `data.csv` in container `raw`).\n*   *Analogy:* Linked Service is the door to the house; Dataset is a specific box inside the house.\n\n### 2. Variables vs Parameters\n**Q:** When do you use Pipeline Parameters vs Variables?\n**A:**\n*   **Parameters:** External inputs passed *into* the pipeline at runtime (e.g., `RunDate`, `FilePath`). They are constant throughout the pipeline run.\n*   **Variables:** Internal values that can store temporary data and change *during* the pipeline execution using the `Set Variable` activity (e.g., storing a loop counter or a flag).\n\n### 3. Control Flow Activities\n**Q:** Explain the difference between `ForEach` and `Until` activities.\n**A:**\n*   **ForEach:** Iterates over a fixed collection of items (e.g., list of filenames). Can run sequentially or in parallel (default).\n*   **Until:** A do-while loop that runs repeatedly until a specific condition evaluates to true (e.g., wait until a file appears or status becomes 'Success').\n\n---\n\n## Integration Runtimes (IR)\n\n### 4. Types of Integration Runtimes\n**Q:** Name the three types of IR and when to use each.\n**A:**\n1.  **Azure IR (AutoResolve):** The default, fully managed, serverless compute in Azure. Use for cloud-to-cloud movement (e.g., Blob to SQL).\n2.  **Self-Hosted IR (SHIR):** Installed on a local VM or on-premise machine. Required to access **private/on-premise** networks (e.g., local SQL Server behind a firewall).\n3.  **Azure-SSIS IR:** Dedicated cluster of VMs to execute legacy SSIS packages in the cloud (Lift and Shift).\n\n### 5. Self-Hosted IR High Availability\n**Q:** How do you ensure High Availability (HA) for a Self-Hosted IR?\n**A:**\nInstall the SHIR agent on multiple (up to 4) separate nodes/VMs and register them with the same Authentication Key. ADF automatically load balances tasks across these nodes and handles failover if one node goes down.\n\n---\n\n## Data Flows & Transformation\n\n### 6. Mapping Data Flow vs Wrangling Data Flow\n**Q:** What is the difference?\n**A:**\n*   **Mapping Data Flow:** Visual interface to build transformation logic (Join, Aggregate, Pivot) that runs on a managed **Spark** cluster. Designed for data engineers.\n*   **Wrangling Data Flow:** Uses the **Power Query M** interface (Excel-like) for lightweight data preparation. Designed for citizen integrators/analysts.\n\n### 7. Schema Drift\n**Q:** What is Schema Drift in Mapping Data Flows and how do you handle it?\n**A:**\n*   *Definition:* When source fields change (add/remove columns) or data types change dynamically.\n*   *Handling:* Enable the \"Allow Schema Drift\" checkbox in the Source transformation. Use \"Drifted Column\" patterns or `byName()` mapping to handle columns that aren't defined in the dataset explicitly.\n\n---\n\n## Triggers & Scheduling\n\n### 8. Tumbling Window vs Schedule Trigger\n**Q:** Why use a Tumbling Window trigger instead of a Schedule trigger?\n**A:**\n*   **Schedule Trigger:** \"Fire at 10:00 AM\". If the 9:00 AM run fails, the 10:00 AM run still fires independently. No built-in backfill.\n*   **Tumbling Window:** \"Process data for window 9:00-10:00\".\n    *   **Dependency:** Can wait for the previous window to succeed.\n    *   **Retry:** Built-in retry policy.\n    *   **Backfill:** Easily rerun historical windows.\n    *   **Concurrency:** Can forbid overlapping windows (ensure sequential processing).\n\n### 9. Event-Based Trigger\n**Q:** Give a use case for an Event-Based Trigger.\n**A:**\nStarting a pipeline immediately when a file lands in an S3 bucket or Azure Blob Storage (`BlobCreated` event). Eliminates polling/waiting.\n\n---\n\n## Scenarios & Performance\n\n### 10. Incremental Loading\n**Q:** How do you design a pipeline to incrementally load data from a SQL DB to a Data Lake?\n**A:**\n1.  **Lookup (Old):** Get the `LastModifiedDate` (Watermark) from a metadata table/file.\n2.  **Lookup (New):** Get the `MAX(LastModifiedDate)` from the Source Table.\n3.  **Copy Data:** Source Query: `SELECT * FROM Source WHERE Date > @OldWatermark AND Date <= @NewWatermark`.\n4.  **Stored Procedure:** Update the metadata table with the `@NewWatermark` only if the Copy succeeds.\n\n### 11. Performance Tuning (DIUs)\n**Q:** The Copy Activity is slow. How do you tune it?\n**A:**\n1.  **DIU (Data Integration Units):** Increase DIUs (power units) for Azure IR.\n2.  **Degree of Parallelism:** Increase the parallel file copy count settings.\n3.  **Staging:** Enable \"Staging\" (via Blob Storage) when loading into Azure Synapse (PolyBase) or Snowflake for massive throughput gains.\n4.  **Partitioning:** Ensure the source database is read in parallel using \"Source Partitioning\" (e.g., partition by ID).\n\n### 12. Handling API Pagination\n**Q:** How do you handle a Rest API that returns paginated results?\n**A:**\nIn the Copy Activity Source settings, configure **Pagination Rules**.\n*   Map the `nextLink` or `offset` from the response body/header to the next request's URL or query parameter.\n*   ADF will automatically loop until the API stops returning a next page token.\n"}, {"name": "beginner_python_pyspark_questions.md", "type": "file", "content": "# Beginner Python & PySpark Interview Questions\n*Entry Level*\n\nA guide for beginners focusing on core DataFrame concepts in Pandas and PySpark, illustrating the transition from single-machine to distributed data processing.\n\n---\n\n## Table of Contents\n1. [Pandas Basics (Single Machine)](#pandas-basics)\n2. [PySpark Basics (Distributed)](#pyspark-basics)\n3. [Key Concepts & Differences](#key-concepts--differences)\n4. [Common Coding Scenarios](#common-coding-scenarios)\n5. [Additional PySpark Questions](#additional-pyspark-questions)\n\n---\n\n## Pandas Basics\n\n### 1. Reading and Inspecting Data\n**Q:** How do you read a CSV file in Pandas and check the first 5 rows?\n**A:**\n```python\nimport pandas as pd\n\n# Read CSV\ndf = pd.read_csv('data.csv')\n\n# View first 5 rows\nprint(df.head())\n\n# Check data types and non-null counts\nprint(df.info())\n```\n\n### 2. Selecting and Filtering\n**Q:** How do you select specific columns and filter rows based on a condition?\n**A:**\n```python\n# Select single column (returns Series)\nnames = df['Name']\n\n# Select multiple columns (returns DataFrame)\nsubset = df[['Name', 'Age']]\n\n# Filter: People older than 25\nadults = df[df['Age'] > 25]\n\n# Filter: Multiple conditions (AND=&, OR=|)\n# Note: Parentheses are mandatory!\ntarget = df[(df['Age'] > 25) & (df['Department'] == 'IT')]\n```\n\n### 3. Handling Missing Data\n**Q:** How do you handle null/missing values in Pandas?\n**A:**\n1.  **Check for nulls:** `df.isnull().sum()`\n2.  **Drop rows with nulls:** `df.dropna()`\n3.  **Fill nulls:** `df.fillna(0)` or `df['Age'].fillna(df['Age'].mean())`\n\n### 4. GroupBy and Aggregation\n**Q:** Calculate the average salary per department.\n**A:**\n```python\n# Group by 'Department' and calculate mean of 'Salary'\navg_salary = df.groupby('Department')['Salary'].mean()\n\n# Multiple aggregations\nsummary = df.groupby('Department').agg({\n    'Salary': ['min', 'max', 'mean'],\n    'Age': 'mean'\n})\n```\n\n---\n\n## PySpark Basics\n\n### 5. SparkSession\n**Q:** What is a `SparkSession`?\n**A:**\nIt is the entry point to programming Spark with the Dataset and DataFrame API. You need it to create DataFrames.\n```python\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"BeginnerApp\").getOrCreate()\n```\n\n### 6. PySpark DataFrame Operations\n**Q:** How do you perform basic selection and filtering in PySpark?\n**A:**\nIn PySpark, operations are methods chained together.\n```python\n# Read\ndf = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n\n# Select\ndf_subset = df.select(\"Name\", \"Age\")\n\n# Filter (can use SQL-like string or Column conditions)\ndf_adults = df.filter(\"Age > 25\")\n# OR\nfrom pyspark.sql.functions import col\ndf_adults = df.filter(col(\"Age\") > 25)\n\n# Adding a new column (Transformation)\ndf_new = df.withColumn(\"Age_in_10_Years\", col(\"Age\") + 10)\n\n# Renaming a column\ndf_renamed = df.withColumnRenamed(\"Name\", \"Full_Name\")\n```\n\n### 7. Actions vs Transformations\n**Q:** What is the difference between `df.select()` and `df.count()`?\n**A:**\n*   **Transformation (`select`, `filter`, `withColumn`):** Lazy. It just builds a \"recipe\" (execution plan) but executes nothing. Returns a new DataFrame.\n*   **Action (`count`, `show`, `collect`, `write`):** Triggers the actual computation. Spark looks at the recipe, optimizes it, and runs it.\n\n---\n\n## Key Concepts & Differences\n\n### 8. Pandas vs PySpark\n**Q:** When should you use PySpark over Pandas?\n**A:**\n| Feature | Pandas | PySpark |\n|---------|--------|---------|\n| **Data Size** | Small/Medium (Must fit in RAM) | Huge (Terabytes/Petabytes) |\n| **Architecture** | Single Machine | Distributed (Cluster of machines) |\n| **Execution** | Eager (Runs immediately) | Lazy (Runs only when triggered) |\n| **Memory** | Can error with \"Out of Memory\" | Spills to disk if RAM is full |\n\n### 9. Lazy Evaluation\n**Q:** Why is Lazy Evaluation useful?\n**A:**\nIt allows Spark to **optimize** the query before running it.\n*Example:* If you load a 1TB file, filter it to 10 rows, and then select columns, Spark notices this. It pushes the filter down to the read source (CSV/Parquet) and only reads the relevant rows instead of loading the whole 1TB into memory first.\n\n---\n\n## Common Coding Scenarios\n\n### 10. Distinct Counts\n**Q:** Count the number of unique departments.\n**Pandas:**\n```python\ncount = df['Department'].nunique()\n```\n**PySpark:**\n```python\nfrom pyspark.sql.functions import countDistinct\ndf.select(countDistinct(\"Department\")).show()\n```\n\n### 11. Sorting\n**Q:** Sort data by Age in descending order.\n**Pandas:**\n```python\ndf.sort_values(by='Age', ascending=False)\n```\n**PySpark:**\n```python\nfrom pyspark.sql.functions import desc\ndf.orderBy(desc(\"Age\")).show()\n```\n\n---\n\n## Additional PySpark Questions\n\n### 12. Schema Definitions (StructType)\n**Q:** How do you strictly define a schema instead of using `inferSchema`?\n**A:**\n```python\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\nschema = StructType([\n    StructField(\"Name\", StringType(), True),\n    StructField(\"Age\", IntegerType(), True)\n])\n\ndf = spark.read.schema(schema).csv(\"data.csv\")\n```\n*Why?* It is faster (no reading file to guess types) and safer (prevents type errors).\n\n### 13. Data Types Check\n**Q:** How do you check column types in PySpark?\n**A:**\n*   `df.printSchema()`: Prints a tree structure.\n*   `df.dtypes`: Returns a list of tuples `[('Name', 'string'), ('Age', 'int')]`.\n\n### 14. Dropping Columns\n**Q:** How do you remove a column?\n**A:**\n```python\ndf_dropped = df.drop(\"ColumnName\")\n```\n\n### 15. Handling Duplicates\n**Q:** How do you remove duplicate rows?\n**A:**\n```python\n# Remove fully duplicate rows\ndf_unique = df.dropDuplicates()\n\n# Remove duplicates based on specific columns (keep first occurrence)\ndf_unique_names = df.dropDuplicates([\"Name\"])\n```\n\n### 16. Filling Nulls\n**Q:** How do you replace null values?\n**A:**\n```python\n# Replace all nulls with 0\ndf.na.fill(0)\n\n# Replace nulls in specific columns\ndf.na.fill({\"Name\": \"Unknown\", \"Age\": 0})\n```\n\n### 17. Dropping Rows with Nulls\n**Q:** How do you drop rows containing null values?\n**A:**\n```python\n# Drop row if ANY column is null\ndf.na.drop(how=\"any\")\n\n# Drop row if ALL columns are null\ndf.na.drop(how=\"all\")\n```\n\n### 18. Literal Values\n**Q:** How do you add a static value column (e.g., Country=\"USA\")?\n**A:**\nYou must use the `lit` function. pure strings are interpreted as column names.\n```python\nfrom pyspark.sql.functions import lit\ndf = df.withColumn(\"Country\", lit(\"USA\"))\n```\n\n### 19. Column Aliasing\n**Q:** How do you rename a column during selection?\n**A:**\n```python\ndf.select(col(\"Name\").alias(\"Full_Name\"))\n```\n\n### 20. Type Casting\n**Q:** How do you convert a String column to Integer?\n**A:**\n```python\ndf = df.withColumn(\"Age\", col(\"Age\").cast(\"integer\"))\n```\n\n### 21. String Filtering\n**Q:** Filter for names starting with \"A\".\n**A:**\n```python\ndf.filter(col(\"Name\").startswith(\"A\"))\n# OR\ndf.filter(col(\"Name\").like(\"A%\"))\n```\n\n### 22. List Filtering (IS IN)\n**Q:** Filter for Department in IT or HR.\n**A:**\n```python\ndf.filter(col(\"Department\").isin([\"IT\", \"HR\"]))\n```\n\n### 23. Substring\n**Q:** Extract the first 3 characters of a Name.\n**A:**\n```python\nfrom pyspark.sql.functions import substring\ndf.select(substring(col(\"Name\"), 0, 3))\n```\n\n### 24. Conditional Logic (Case When)\n**Q:** Create a column \"Status\": \"Adult\" if Age > 18, else \"Minor\".\n**A:**\n```python\nfrom pyspark.sql.functions import when\n\ndf = df.withColumn(\"Status\", \n    when(col(\"Age\") > 18, \"Adult\")\n    .otherwise(\"Minor\")\n)\n```\n\n### 25. Date Addition\n**Q:** Add 5 days to a date column.\n**A:**\n```python\nfrom pyspark.sql.functions import date_add\ndf.withColumn(\"Next_Week_Date\", date_add(col(\"DateColumn\"), 7))\n```\n\n### 26. Current Date & Time\n**Q:** How do you get the current date and timestamp?\n**A:**\n```python\nfrom pyspark.sql.functions import current_date, current_timestamp\ndf.select(current_date(), current_timestamp())\n```\n\n### 27. Union vs UnionByName\n**Q:** What is the difference?\n**A:**\n*   `union()`: Merges by **position**. Column 1 matches Column 1. Dangerous if schemas differ order.\n*   `unionByName()`: Merges by **column name**. Safer.\n\n### 28. Joins\n**Q:** Syntax for joining two DataFrames.\n**A:**\n```python\n# Inner join (default)\ndf1.join(df2, \"id\")\n\n# Left join\ndf1.join(df2, df1[\"id\"] == df2[\"u_id\"], \"left\")\n```\n\n### 29. SQL Temp Views\n**Q:** How do you run SQL queries on a DataFrame?\n**A:**\nYou must register it as a view first.\n```python\ndf.createOrReplaceTempView(\"people\")\nspark.sql(\"SELECT * FROM people WHERE age > 20\").show()\n```\n\n### 30. Writing to Parquet (Modes)\n**Q:** How do you write a DataFrame overwriting existing files?\n**A:**\n```python\ndf.write.mode(\"overwrite\").parquet(\"output_path\")\n```\n\n### 31. Coalesce vs Repartition\n**Q:** How do you reduce the number of output files to 1?\n**A:**\n```python\n# coalesce is more efficient than repartition for reducing counts\ndf.coalesce(1).write.csv(\"output\")\n```\n"}, {"name": "data_engineering_interview_questions.md", "type": "file", "content": "# Data Engineering Interview Questions\n\nA comprehensive collection of data engineering interview questions covering PySpark, SQL, and data processing concepts.\n\n---\n\n## Table of Contents\n- [Deloitte Interview Questions](#deloitte-interview-questions)\n- [Coforge Interview Questions](#coforge-interview-questions)\n- [Additional PySpark Questions](#additional-pyspark-questions)\n- [Additional SQL Questions](#additional-sql-questions)\n- [Data Processing Concepts](#data-processing-concepts)\n\n---\n\n## Deloitte Interview Questions\n\n### 1. Splitting Large Files While Preserving Order\n\n**Question:** I have 1 million records and want to split them into 10 files (100K each) without changing the order. How would you do this in PySpark?\n\n**Answer:**\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, monotonically_increasing_id\n\nspark = SparkSession.builder.appName(\"SplitRecords\").getOrCreate()\n\n# Read the CSV file\ndf = spark.read.csv(\"path/filename.csv\", header=True)\n\n# Add row numbers to preserve order\ndf_with_row = df.withColumn(\"row_id\", monotonically_increasing_id())\nwindow = Window.orderBy(\"row_id\")\ndf2 = df_with_row.withColumn(\"rownum\", row_number().over(window))\n\n# Split and write each partition\nfor i in range(10):\n    start = i * 100000 + 1\n    end = (i + 1) * 100000\n    df_part = df2.filter((df2.rownum >= start) & (df2.rownum <= end))\n    df_part.drop(\"row_id\", \"rownum\").write.format(\"orc\").save(f\"output/file_{i+1}\")\n```\n\n**Key Points:**\n- `monotonically_increasing_id()` generates unique IDs but not consecutive\n- `row_number()` with a window provides sequential numbering\n- Always drop helper columns before writing\n\n---\n\n### 2. Handling Corrupt/Bad Data in Files\n\n**Question:** Examine the following file with corrupt data. How will you handle and import it into a Spark DataFrame?\n\n```\nEmp_no, Emp_name, Department\n101, Murugan, HealthCare Invalid Entry, Description: Bad Record entry \n102, Kannan, Finance \n103, Mani, IT Connection lost, Description: Poor Connection \n104, Pavan, HR Bad Record, Description: Corrupt record\n```\n\n**Answer:**\n\n```python\n# Method 1: Using PERMISSIVE mode with corrupt record column\ndf = spark.read \\\n    .option(\"mode\", \"PERMISSIVE\") \\\n    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n    .csv(\"path/to/file.csv\", header=True)\n\n# Separate clean and corrupt records\nclean_df = df.filter(df._corrupt_record.isNull())\ncorrupt_df = df.filter(df._corrupt_record.isNotNull())\n\n# Method 2: Using badRecordsPath (Spark 2.3+)\ndf = spark.read \\\n    .option(\"mode\", \"PERMISSIVE\") \\\n    .option(\"badRecordsPath\", \"/path/to/bad_records\") \\\n    .csv(\"path/to/file.csv\", header=True)\n\n# Method 3: Using DROPMALFORMED mode (drops bad records silently)\ndf = spark.read \\\n    .option(\"mode\", \"DROPMALFORMED\") \\\n    .csv(\"path/to/file.csv\", header=True)\n```\n\n**Read Modes:**\n| Mode | Behavior |\n|------|----------|\n| `PERMISSIVE` | Sets malformed fields to null, can capture corrupt records |\n| `DROPMALFORMED` | Drops rows with malformed records |\n| `FAILFAST` | Throws exception on malformed records |\n\n---\n\n### 3. Merging Files with Different Schemas\n\n**Question:** How will you merge File1 and File2 into a single DataFrame if they have different schemas?\n\n```\nFile-1: Name|Age\nAzarudeen, Shahul|25\nMichel, Clarke|26\n\nFile-2: Name|Age|Gender\nRabindra, Tagore|32|Male\nMadona, Laure|59|Female\n```\n\n**Answer:**\n\n```python\n# Read both files\ndf1 = spark.read.option(\"delimiter\", \"|\").csv(\"file1.csv\", header=True)\ndf2 = spark.read.option(\"delimiter\", \"|\").csv(\"file2.csv\", header=True)\n\n# Method 1: unionByName with allowMissingColumns (Spark 3.1+)\nmerged_df = df1.unionByName(df2, allowMissingColumns=True)\n\n# Method 2: Manual schema alignment (older Spark versions)\nfrom pyspark.sql.functions import lit\n\n# Add missing column to df1\ndf1_aligned = df1.withColumn(\"Gender\", lit(None).cast(\"string\"))\n\n# Now union\nmerged_df = df1_aligned.unionByName(df2)\n\n# Method 3: Using SQL\ndf1.createOrReplaceTempView(\"table1\")\ndf2.createOrReplaceTempView(\"table2\")\n\nmerged_df = spark.sql(\"\"\"\n    SELECT Name, Age, NULL as Gender FROM table1\n    UNION ALL\n    SELECT Name, Age, Gender FROM table2\n\"\"\")\n```\n\n---\n\n### 4. Finding Duplicate Emails\n\n**Question:** Write a SQL query to find all duplicate emails in a table named Person.\n\n```\n+----+---------+\n| Id | Email   |\n+----+---------+\n| 1  | a@b.com |\n| 2  | c@d.com |\n| 3  | a@b.com |\n+----+---------+\n```\n\n**Answer:**\n\n```sql\n-- Method 1: Using GROUP BY and HAVING\nSELECT Email\nFROM Person\nGROUP BY Email\nHAVING COUNT(*) > 1;\n\n-- Method 2: Using self-join\nSELECT DISTINCT p1.Email\nFROM Person p1\nINNER JOIN Person p2 \n    ON p1.Email = p2.Email \n    AND p1.Id <> p2.Id;\n\n-- Method 3: Using subquery\nSELECT DISTINCT Email\nFROM Person\nWHERE Email IN (\n    SELECT Email\n    FROM Person\n    GROUP BY Email\n    HAVING COUNT(*) > 1\n);\n```\n\n---\n\n## Coforge Interview Questions\n\n### 1. Deleting Duplicate Records\n\n**Question:** Delete duplicate records from a temp table, keeping only one occurrence.\n\n**Answer:**\n\n```sql\n-- Method 1: Using ROW_NUMBER() with CTE\nWITH RankedRecords AS (\n    SELECT name, \n           ROW_NUMBER() OVER (PARTITION BY name ORDER BY name) AS rnk\n    FROM temp_table\n)\nDELETE FROM RankedRecords WHERE rnk > 1;\n\n-- Method 2: Using MIN/MAX to keep specific record\nDELETE FROM temp_table\nWHERE id NOT IN (\n    SELECT MIN(id)\n    FROM temp_table\n    GROUP BY name\n);\n\n-- Method 3: In PySpark\ndf = spark.read.table(\"temp_table\")\ndf_deduplicated = df.dropDuplicates([\"name\"])\n```\n\n---\n\n### 2. Reading Tab-Separated Files\n\n**Question:** How do you read a tab-separated file in PySpark?\n\n**Answer:**\n\n```python\n# Method 1: Using sep parameter\ndf = spark.read.csv(\"filepath/filename.tsv\", sep=\"\\t\", header=True)\n\n# Method 2: Using option\ndf = spark.read \\\n    .option(\"delimiter\", \"\\t\") \\\n    .option(\"header\", \"true\") \\\n    .csv(\"filepath/filename.tsv\")\n\n# Method 3: Using format\ndf = spark.read \\\n    .format(\"csv\") \\\n    .option(\"sep\", \"\\t\") \\\n    .option(\"header\", \"true\") \\\n    .load(\"filepath/filename.tsv\")\n```\n\n---\n\n## Additional PySpark Questions\n\n### 5. Difference Between `cache()` and `persist()`\n\n**Question:** What is the difference between `cache()` and `persist()` in Spark?\n\n**Answer:**\n\n| Feature | `cache()` | `persist()` |\n|---------|-----------|-------------|\n| Storage Level | MEMORY_ONLY (default) | Configurable |\n| Flexibility | Fixed | Can use MEMORY_AND_DISK, DISK_ONLY, etc. |\n\n```python\nfrom pyspark import StorageLevel\n\n# cache() - stores in memory only\ndf.cache()\n\n# persist() - configurable storage\ndf.persist(StorageLevel.MEMORY_AND_DISK)\ndf.persist(StorageLevel.DISK_ONLY)\ndf.persist(StorageLevel.MEMORY_ONLY_SER)  # Serialized\n```\n\n---\n\n### 6. Broadcast Variables\n\n**Question:** What are broadcast variables and when would you use them?\n\n**Answer:**\n\n```python\nfrom pyspark.sql.functions import broadcast\n\n# Small lookup table\nlookup_df = spark.read.csv(\"small_lookup.csv\", header=True)  # < 10MB\n\n# Large fact table\nfact_df = spark.read.parquet(\"large_facts.parquet\")\n\n# Broadcast join - sends small table to all executors\nresult = fact_df.join(broadcast(lookup_df), \"key_column\")\n```\n\n**Use When:**\n- Joining large DataFrame with small DataFrame (< 10MB default)\n- Reducing shuffle operations\n- Lookup tables that fit in memory\n\n---\n\n### 7. Handling Skewed Data\n\n**Question:** How do you handle data skew in Spark joins?\n\n**Answer:**\n\n```python\n# Method 1: Salting technique\nfrom pyspark.sql.functions import lit, rand, concat, col\n\n# Add salt to skewed key\nsalt_range = 10\ndf_salted = df.withColumn(\"salted_key\", \n    concat(col(\"key\"), lit(\"_\"), (rand() * salt_range).cast(\"int\")))\n\n# Method 2: Broadcast smaller table\nresult = large_df.join(broadcast(small_df), \"key\")\n\n# Method 3: Adaptive Query Execution (Spark 3.0+)\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n```\n\n---\n\n### 8. Difference Between `repartition()` and `coalesce()`\n\n**Question:** Explain the difference between `repartition()` and `coalesce()`.\n\n**Answer:**\n\n| Feature | `repartition()` | `coalesce()` |\n|---------|-----------------|--------------|\n| Shuffle | Full shuffle | Minimizes shuffle |\n| Increase partitions | \u2705 Yes | \u274c No |\n| Decrease partitions | \u2705 Yes | \u2705 Yes (preferred) |\n| Use case | Even distribution | Reducing partitions |\n\n```python\n# Increase partitions (use repartition)\ndf = df.repartition(100)\n\n# Decrease partitions (use coalesce - more efficient)\ndf = df.coalesce(10)\n\n# Repartition by column (for partitioned writes)\ndf = df.repartition(\"date_column\")\n```\n\n---\n\n### 9. Window Functions in PySpark\n\n**Question:** Calculate running total and rank by department.\n\n**Answer:**\n\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import sum, row_number, rank, dense_rank\n\n# Define window specifications\nwindow_dept = Window.partitionBy(\"department\").orderBy(\"salary\")\nwindow_running = Window.partitionBy(\"department\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\ndf = df.withColumn(\"row_num\", row_number().over(window_dept)) \\\n       .withColumn(\"rank\", rank().over(window_dept)) \\\n       .withColumn(\"dense_rank\", dense_rank().over(window_dept)) \\\n       .withColumn(\"running_total\", sum(\"amount\").over(window_running))\n```\n\n---\n\n## Additional SQL Questions\n\n### 10. Second Highest Salary\n\n**Question:** Find the second highest salary from an Employee table.\n\n**Answer:**\n\n```sql\n-- Method 1: Using LIMIT and OFFSET\nSELECT DISTINCT salary\nFROM Employee\nORDER BY salary DESC\nLIMIT 1 OFFSET 1;\n\n-- Method 2: Using subquery\nSELECT MAX(salary) AS SecondHighestSalary\nFROM Employee\nWHERE salary < (SELECT MAX(salary) FROM Employee);\n\n-- Method 3: Using DENSE_RANK\nSELECT salary\nFROM (\n    SELECT salary, DENSE_RANK() OVER (ORDER BY salary DESC) AS rnk\n    FROM Employee\n) ranked\nWHERE rnk = 2;\n\n-- Method 4: Handle NULL if no second highest\nSELECT (\n    SELECT DISTINCT salary\n    FROM Employee\n    ORDER BY salary DESC\n    LIMIT 1 OFFSET 1\n) AS SecondHighestSalary;\n```\n\n---\n\n### 11. Nth Highest Salary\n\n**Question:** Write a function to get the Nth highest salary.\n\n**Answer:**\n\n```sql\n-- Using DENSE_RANK\nCREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT\nBEGIN\n    RETURN (\n        SELECT DISTINCT salary\n        FROM (\n            SELECT salary, DENSE_RANK() OVER (ORDER BY salary DESC) AS rnk\n            FROM Employee\n        ) ranked\n        WHERE rnk = N\n    );\nEND;\n\n-- Using LIMIT OFFSET (N-1 because OFFSET is 0-indexed)\nSELECT DISTINCT salary\nFROM Employee\nORDER BY salary DESC\nLIMIT 1 OFFSET N-1;\n```\n\n---\n\n### 12. Consecutive Numbers\n\n**Question:** Find all numbers that appear at least three times consecutively.\n\n```\n+----+-----+\n| Id | Num |\n+----+-----+\n| 1  | 1   |\n| 2  | 1   |\n| 3  | 1   |\n| 4  | 2   |\n| 5  | 1   |\n| 6  | 2   |\n| 7  | 2   |\n+----+-----+\n```\n\n**Answer:**\n\n```sql\n-- Method 1: Self-join\nSELECT DISTINCT l1.Num AS ConsecutiveNums\nFROM Logs l1\nJOIN Logs l2 ON l1.Id = l2.Id - 1\nJOIN Logs l3 ON l2.Id = l3.Id - 1\nWHERE l1.Num = l2.Num AND l2.Num = l3.Num;\n\n-- Method 2: Using LAG/LEAD\nSELECT DISTINCT Num AS ConsecutiveNums\nFROM (\n    SELECT Num,\n           LAG(Num, 1) OVER (ORDER BY Id) AS prev1,\n           LAG(Num, 2) OVER (ORDER BY Id) AS prev2\n    FROM Logs\n) t\nWHERE Num = prev1 AND Num = prev2;\n```\n\n---\n\n### 13. Department Top 3 Salaries\n\n**Question:** Find employees who earn top 3 salaries in each department.\n\n**Answer:**\n\n```sql\nSELECT Department, Employee, Salary\nFROM (\n    SELECT \n        d.Name AS Department,\n        e.Name AS Employee,\n        e.Salary,\n        DENSE_RANK() OVER (PARTITION BY e.DepartmentId ORDER BY e.Salary DESC) AS rnk\n    FROM Employee e\n    JOIN Department d ON e.DepartmentId = d.Id\n) ranked\nWHERE rnk <= 3;\n```\n\n---\n\n## Data Processing Concepts\n\n### 14. ETL vs ELT\n\n**Question:** What is the difference between ETL and ELT?\n\n**Answer:**\n\n| Aspect | ETL | ELT |\n|--------|-----|-----|\n| **Process** | Extract \u2192 Transform \u2192 Load | Extract \u2192 Load \u2192 Transform |\n| **Transform Location** | Staging area / ETL tool | Target data warehouse |\n| **Best For** | On-premise, structured data | Cloud data warehouses |\n| **Examples** | Informatica, Talend, SSIS | Snowflake, BigQuery, Databricks |\n| **Scalability** | Limited by ETL server | Leverages cloud compute |\n\n---\n\n### 15. Slowly Changing Dimensions (SCD)\n\n**Question:** Explain SCD Types 1, 2, and 3.\n\n**Answer:**\n\n| Type | Description | Example |\n|------|-------------|---------|\n| **SCD Type 1** | Overwrite old data | Update address directly |\n| **SCD Type 2** | Add new row with versioning | Keep history with effective dates |\n| **SCD Type 3** | Add new column for previous value | current_address, previous_address |\n\n```sql\n-- SCD Type 2 Implementation\nINSERT INTO dim_customer (customer_id, name, address, effective_from, effective_to, is_current)\nVALUES (101, 'John', 'New York', '2024-01-01', '9999-12-31', 1);\n\n-- When address changes:\nUPDATE dim_customer \nSET effective_to = CURRENT_DATE - 1, is_current = 0\nWHERE customer_id = 101 AND is_current = 1;\n\nINSERT INTO dim_customer (customer_id, name, address, effective_from, effective_to, is_current)\nVALUES (101, 'John', 'Los Angeles', CURRENT_DATE, '9999-12-31', 1);\n```\n\n---\n\n### 16. Data Quality Checks\n\n**Question:** What data quality checks would you implement in a data pipeline?\n\n**Answer:**\n\n```python\nfrom pyspark.sql.functions import col, count, when, isnan, isnull\n\n# 1. Null check\nnull_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n\n# 2. Duplicate check\nduplicate_count = df.count() - df.dropDuplicates().count()\n\n# 3. Schema validation\nexpected_schema = [\"id\", \"name\", \"amount\", \"date\"]\nassert df.columns == expected_schema, \"Schema mismatch!\"\n\n# 4. Range validation\ninvalid_amounts = df.filter((col(\"amount\") < 0) | (col(\"amount\") > 1000000)).count()\n\n# 5. Referential integrity\norphan_records = fact_df.join(dim_df, \"key\", \"left_anti\").count()\n\n# 6. Freshness check\nfrom datetime import datetime, timedelta\nmax_date = df.agg({\"date\": \"max\"}).collect()[0][0]\nassert max_date >= datetime.now() - timedelta(days=1), \"Data is stale!\"\n```\n\n---\n\n### 17. Partitioning Strategies\n\n**Question:** How do you decide on partitioning strategy for a data lake?\n\n**Answer:**\n\n```python\n# Time-based partitioning (most common)\ndf.write \\\n    .partitionBy(\"year\", \"month\", \"day\") \\\n    .parquet(\"s3://bucket/table/\")\n\n# Category-based partitioning\ndf.write \\\n    .partitionBy(\"region\", \"category\") \\\n    .parquet(\"s3://bucket/table/\")\n\n# Bucketing (for join optimization)\ndf.write \\\n    .bucketBy(100, \"user_id\") \\\n    .sortBy(\"user_id\") \\\n    .saveAsTable(\"bucketed_table\")\n```\n\n**Guidelines:**\n- Partition by frequently filtered columns\n- Avoid too many small files (< 128MB)\n- Avoid too few large partitions\n- Ideal partition size: 128MB - 1GB\n\n---\n\n### 18. Spark Optimization Techniques\n\n**Question:** List common Spark optimization techniques.\n\n**Answer:**\n\n1. **Use appropriate file formats:** Parquet, ORC over CSV/JSON\n2. **Broadcast small tables:** `broadcast(small_df)`\n3. **Cache/persist intermediate results:** `df.cache()`\n4. **Avoid UDFs when possible:** Use built-in functions\n5. **Repartition for parallelism:** `df.repartition(n)`\n6. **Use coalesce to reduce partitions:** `df.coalesce(n)`\n7. **Enable AQE:** `spark.sql.adaptive.enabled = true`\n8. **Optimize joins:** Broadcast joins, bucketing\n9. **Filter early:** Push down predicates\n10. **Use appropriate data types:** Avoid strings for numbers\n\n---\n\n## Quick Reference\n\n### Common PySpark Imports\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\n```\n\n### SparkSession Creation\n\n```python\nspark = SparkSession.builder \\\n    .appName(\"MyApp\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.executor.memory\", \"4g\") \\\n    .getOrCreate()\n```\n\n### Common Read/Write Operations\n\n```python\n# Read\ndf = spark.read.parquet(\"path/to/data\")\ndf = spark.read.csv(\"path/to/data\", header=True, inferSchema=True)\ndf = spark.read.json(\"path/to/data\")\n\n# Write\ndf.write.mode(\"overwrite\").parquet(\"path/to/output\")\ndf.write.mode(\"append\").partitionBy(\"date\").parquet(\"path/to/output\")\n```\n\n---\n\n*Last Updated: January 2026*\n"}, {"name": "data_modeling_interview_questions.md", "type": "file", "content": "# Data Modeling Interview Questions & Concepts\n\nA guide to core data modeling concepts and a practical schema design example.\n\n## 1. Core Concepts\n\n### Normalization vs. Denormalization\n- **Normalization (OLTP)**: Organizing measurements to minimize redundancy.\n    - **1NF**: Atomic values, no repeating groups.\n    - **2NF**: 1NF + Partial dependencies removed (attributes depend on whole PK).\n    - **3NF**: 2NF + Transitive dependencies removed (attributes depend *only* on PK).\n    - **Goal**: Write-heavy systems, data integrity, optimize storage.\n- **Denormalization (OLAP)**: Adding redundancy to speed up reads.\n    - **Goal**: Read-heavy systems (Data Warehouses), minimize joins.\n\n### Fact vs. Dimension Tables\n- **Fact Table**: Contains quantitative data (metrics, measures) and foreign keys to dimensions.\n    - **Transaction Fact**: One row per event (e.g., Order Line Item).\n    - **Snapshot Fact**: One row per period (e.g., Monthly Account Balance).\n    - **Accumulating Snapshot**: Updates over time (e.g., Order processing workflow).\n- **Dimension Table**: Contains descriptive attributes (who, what, where, when).\n    - **Conformed Dimension**: Shared across multiple facts (e.g., Date, Customer).\n    - **Junk Dimension**: Collection of low-cardinality flags/codes.\n\n### Schema Types\n- **Star Schema**: Central fact table connected directly to dimensions. Simpler, faster joins.\n- **Snowflake Schema**: Dimensions are normalized (split into sub-dimensions). Saves space, but more complex joins.\n- **Galaxy Schema**: Multiple fact tables sharing conformed dimensions.\n\n## 2. Slowly Changing Dimensions (SCD)\nHow to handle updates to dimension attributes (e.g., User changes address).\n\n- **Type 0 (Retain Original)**: No changes allowed.\n- **Type 1 (Overwrite)**: Update the record. History is lost.\n- **Type 2 (Add Row)**: Create a new record with effective dates (`start_date`, `end_date`) and `is_current` flag. Preserves full history. (Most common in DW).\n- **Type 3 (Add Column)**: Add a `previous_value` column. Keeps limited history (current + previous).\n\n## 3. Practical Example: E-commerce Schema Design\n\n**Scenario**: Design a database for an Amazon-like E-commerce platform.\n\n### Requirements\n- Users can sign up and manage profile.\n- Products have categories and inventory.\n- Users place orders containing multiple items.\n- Payments and Shipping tracking.\n\n### Conceptual / Logical Data Model (OLTP)\n\n#### 1. Users Table\n- `user_id` (PK)\n- `email` (Unique, Index)\n- `password_hash`\n- `first_name`, `last_name`\n- `created_at`\n\n#### 2. Products Table\n- `product_id` (PK)\n- `name`, `description`\n- `price` (Decimal)\n- `category_id` (FK) -> Categories Table\n- `sku` (Unique Stock Keeping Unit)\n- `created_at`\n\n#### 3. Inventory Table\n(Separated from Products to handle warehouses or real-time availability)\n- `inventory_id` (PK)\n- `product_id` (FK)\n- `quantity_on_hand`\n- `warehouse_location`\n- `last_updated` (Timestamp for concurrency checks)\n\n#### 4. Orders Table (Header)\n- `order_id` (PK)\n- `user_id` (FK)\n- `status` (Enum: Pending, Shipped, Delivered, Canceled)\n- `total_amount`\n- `shipping_address_id` (FK)\n- `created_at`\n\n#### 5. Order_Items Table (Details - The \"Line Items\")\n- `order_item_id` (PK)\n- `order_id` (FK)\n- `product_id` (FK)\n- `quantity`\n- `unit_price` (Price *at time of purchase*, critical for history!)\n\n#### 6. Payments Table\n- `payment_id` (PK)\n- `order_id` (FK)\n- `amount`\n- `payment_method` (CC, PayPal)\n- `status` (Success, Failed)\n- `transaction_date`\n\n### Key Considerations for Interview\n\n1.  **Price History**: Why store `unit_price` in `Order_Items` when it's in `Products`?\n    *   **Answer**: Product prices change. You must lock in the price at the moment of purchase for historical accuracy and refund calculations.\n\n2.  **Concurrency / Inventory Management**: What happens if two users buy the last item simultaneously?\n    *   **Answer**: Use Database Transactions (ACID).\n    *   *Optimistic Locking*: Check `last_updated` timestamp before write.\n    *   *Pessimistic Locking*: `SELECT ... FOR UPDATE` to lock the row.\n    *   *Check Constraint*: Ensure `quantity >= 0`.\n\n3.  **Scalability**: How to handle 1 billion orders?\n    *   **Sharding**: Partition the `Orders` and `Order_Items` tables by `user_id` (keeps all user data on one shard) or `order_date` (for archival).\n    *   **Indexing**: Index commonly queried columns (`user_id`, `status`, `product_id`).\n\n4.  **Analytics (OLAP) Transformation**:\n    *   If moving to a Data Warehouse (Redshift/Snowflake), this schema would become a **Star Schema**:\n        *   **Fact**: `Fact_Orders` (grain: one row per line item).\n        *   **Dimensions**: `Dim_User`, `Dim_Product` (SCD Type 2), `Dim_Date`, `Dim_Payment`.\n"}, {"name": "databricks_interview_questions.md", "type": "file", "content": "# Databricks Interview Questions & Concepts\n*Mid to Senior Level*\n\nA comprehensive guide to Databricks-specific features, architecture, and optimization techniques, focusing on the Lakehouse platform.\n\n---\n\n## Table of Contents\n1. [Architecture & Core Concepts](#architecture--core-concepts)\n2. [Delta Lake Internals](#delta-lake-internals)\n3. [Unity Catalog & Governance](#unity-catalog--governance)\n4. [Performance Optimization (Databricks Specific)](#performance-optimization-databricks-specific)\n5. [Data Engineering Features (DLT, Auto Loader)](#data-engineering-features)\n\n---\n\n## Architecture & Core Concepts\n\n### 1. Control Plane vs Data Plane\n**Q:** Explain the difference between the Control Plane and Data Plane in Databricks architecture.\n**A:**\n*   **Control Plane:** Managed by Databricks (in their AWS/Azure account). It hosts the UI, Cluster Manager, Jobs service, Notebooks, and the Web Application.\n*   **Data Plane:** Resides in **your** customer cloud account (VPC/VNet). This is where the actual compute (clusters) runs and where data is processed. Data generally stays in your plane (security requirement).\n*   **Serverless Data Plane:** A newer model where compute runs in Databricks' account but is strictly isolated, allowing for instant startup (SQL Warehouses).\n\n### 2. High Concurrency vs Standard Cluster\n**Q:** When would you choose a High Concurrency cluster?\n**A:**\n*   **High Concurrency:** Optimized for multiple users sharing resources (e.g., a BI/Analyst team running SQL in notebooks). It uses features like *Task Preemption* to ensure small queries aren't blocked by long-running ones. It forces isolation (Python/SQL) to prevent users from interfering with each other's state.\n*   **Standard (Single User):** Best for single users or automated jobs. Supports all languages (Scala, R, Python, SQL) without restriction.\n\n### 3. Photon Engine\n**Q:** What is Photon and when should you enable it?\n**A:**\n*   **What:** A vectorized query engine written in C++ (instead of JVM) that replaces Spark's existing execution engine.\n*   **When:** Best for SQL-heavy workloads, aggressive aggregations, extensive joins, and Delta Lake operations (Merge/Delete).\n*   **Cost:** It costs more DBUs (Databricks Units) per hour, so it should be benchmarked to ensure the speedup justifies the cost.\n\n---\n\n## Delta Lake Internals\n\n### 4. ACID Transactions in Delta\n**Q:** How does Delta Lake achieve ACID properties on top of S3/ADLS (which are eventually consistent)?\n**A:**\nIt uses a **Transaction Log (`_delta_log`)**:\n*   **Atomicity:** Changes are recorded in JSON files in the log. A commit fails or succeeds completely.\n*   **Consistency:** Readers verify the log state to ensure they read a consistent snapshot.\n*   **Isolation:** Uses Optimistic Concurrency Control (OCC). If two writers try to update the same file, one fails with a concurrency exception.\n*   **Durability:** Data is stored in persistent object storage (Parquet).\n\n### 5. Time Travel & Restore\n**Q:** How do you query a table as it existed yesterday? How do you rollback an accidental delete?\n**A:**\n```sql\n-- Query older version\nSELECT * FROM my_table TIMESTAMP AS OF '2023-10-25 10:00:00';\nSELECT * FROM my_table VERSION AS OF 5;\n\n-- Rollback (Restore)\nRESTORE TABLE my_table TO VERSION AS OF 4;\n```\n*Mechanism:* Delta keeps old parquet files (tombstoned) until `VACUUM` is run.\n\n### 6. VACUUM\n**Q:** What does `VACUUM` do and what is the risk?\n**A:**\n*   Removes physical files that are no longer referenced by the current state of the transaction log and are older than the retention period (default 7 days).\n*   **Risk:** Once vacuumed, you **cannot** Time Travel back to versions requiring those files.\n\n---\n\n## Unity Catalog & Governance\n\n### 7. Unity Catalog vs Hive Metastore\n**Q:** Why migrate to Unity Catalog (UC)?\n**A:**\n1.  **Centralized Governance:** Single metastore across specific workspaces/regions (unlike workspace-local Hive metastores).\n2.  **Data Lineage:** Automated column-level lineage tracking.\n3.  **Audit Logs:** Centralized auditing of who accessed what data.\n4.  **External Locations/Credentials:** Securely manages access to S3/ADLS without exposing keys in notebooks.\n5.  **3-Level Namespace:** `Catalog.Schema.Table` hierarchy.\n\n### 8. Managed vs External Tables in UC\n**Q:** Difference between Managed and External tables in Unity Catalog?\n**A:**\n*   **Managed Table:** Unity Catalog manages the lifecycle AND the data location (in a managed storage bucket). Dropping the table **deletes** the underlying data.\n*   **External Table:** You manage the storage location. Dropping the table only deletes the metadata; the files persist in cloud storage.\n\n---\n\n## Performance Optimization (Databricks Specific)\n\n### 9. OPTIMIZE & Z-ORDER\n**Q:** Explain `OPTIMIZE` and `Z-ORDER BY`. How is it different from Partitioning?\n**A:**\n*   **Partitioning:** Physical directory separation (e.g., `Year=2023`). Good for low-cardinality columns. Bad if >10k partitions.\n*   **OPTIMIZE (Bin-packing):** Coalesces small files (100MB - 1GB) into larger ones to fix the \"Small File Problem\".\n*   **Z-ORDER (Multi-dimensional Clustering):** Co-locates related data within the same set of files.\n    *   *Scenario:* If you frequently filter by `CustomerID` (high cardinality), Partitioning is bad. Z-Ordering by `CustomerID` allows Delta to skip huge chunks of data (Data Skipping) efficiently.\n\n```sql\nOPTIMIZE events_table ZORDER BY (user_id, event_type);\n```\n\n### 10. Cache vs Disk Spec\n**Q:** Databricks uses \"Delta Cache\" (now \"Disk Cache\"). How is it different from Spark Cache?\n**A:**\n*   **Spark Cache (`.cache()`):** Stores data in RAM (Executor memory). Can cause OOM if too large.\n*   **Disk Cache (IO Cache):** Automatically creates copies of remote Parquet files on the local SSDs of the worker nodes. It is entirely separate from JVM memory. It accelerates read speeds significantly and does **not** cause OOMs.\n\n---\n\n## Data Engineering Features\n\n### 11. Auto Loader\n**Q:** What is Auto Loader (`cloudFiles`)?\n**A:**\nAn optimized streaming source to ingest files from S3/ADLS.\n*   **Benefits:** More efficient and cheaper than standard `spark.readStream` file listing.\n*   **Mechanism:** Can works in two modes:\n    1.  **Directory Listing:** Good for few files.\n    2.  **File Notification:** Subscribes to Cloud Queue (SQS/Event Grid) to ingest files immediately upon arrival without listing directories.\n*   **Schema Evolution:** Can automatically infer and evolve schema (add new columns) as data changes.\n\n### 12. Delta Live Tables (DLT)\n**Q:** When would you use DLT?\n**A:**\nA declarative framework for building reliable pipelines.\n*   **expectations (Data Quality):** `CONSTRAINT valid_id EXPECT (id IS NOT NULL) ON VIOLATION DROP ROW`.\n*   **Automated dependency management:** No need to chain tasks manually; DLT figures out the DAG.\n*   **Auto-scaling:** Enhanced Autoscaling works specifically well for DLT streaming workloads.\n\n### 13. COPY INTO vs Auto Loader\n**Q:** Difference between `COPY INTO` and Auto Loader?\n**A:**\n*   **COPY INTO:** Best for bulk ingestion (thousands of files). One-time idempotent loads. simpler syntax for SQL users.\n*   **Auto Loader:** Best for continuous streaming ingestion (millions of files). Scalable incremental processing.\n"}, {"name": "dbt_interview_questions.md", "type": "file", "content": "# dbt (Data Build Tool) Interview Questions\n*Mid to Senior Level*\n\nA comprehensive guide to Analytics Engineering interviews, focusing on dbt Core, modeling strategies, and advanced configurations.\n\n---\n\n## Table of Contents\n1. [Core Concepts & Architecture](#core-concepts--architecture)\n2. [Materializations](#materializations)\n3. [Incremental Models](#incremental-models)\n4. [Testing & Documentation](#testing--documentation)\n5. [Snapshots (SCD Type 2)](#snapshots-scd-type-2)\n6. [Advanced Patterns (Hooks/Macros)](#advanced-patterns)\n\n---\n\n## Core Concepts & Architecture\n\n### 1. What is dbt and how does it differ from traditional ETL?\n**Q:** Explain dbt's role in the \"Modern Data Stack\" (ELT).\n**A:**\n*   **T in ELT:** dbt only does the **Transformation** step. It assumes data is already loaded (Extract/Load) into the warehouse (Snowflake/BigQuery/Redshift).\n*   **Code-First:** It allows writing data transformations in SQL (Select statements), checking them into version control (Git), and testing/documenting them like software code.\n*   **Compilation:** dbt compiles Jinja-SQL code into raw SQL and executes it inside the data warehouse.\n\n### 2. File Structure (`dbt_project.yml`)\n**Q:** What is the purpose of `dbt_project.yml`?\n**A:**\nIt is the configuration file for the project. It defines:\n*   Project Name and Version.\n*   Profile to use (connection details).\n*   Directory paths (model-paths, seed-paths).\n*   **Global Configs:** Setting default materializations (e.g., `+materialized: view`) or tags for specific folders.\n\n---\n\n## Materializations\n\n### 3. Types of Materializations\n**Q:** Name and explain the 4 core materializations in dbt.\n**A:**\n1.  **View (Default):** Creates a database view. Fast to run, but computed at query time (slow for end users).\n2.  **Table:** Creates a physical table. Slower to run (rebuilds entire table), but fast query performance.\n3.  **Incremental:** Only inserts/updates new rows since the last run. Essential for big data.\n4.  **Ephemeral:** No database object created. It acts like a CTE snippet injected into downstream models (good for reusable logic that shouldn't clutter the DB).\n\n### 4. Source vs Ref\n**Q:** What is the difference between `{{ source() }}` and `{{ ref() }}`?\n**A:**\n*   `{{ source('schema', 'table') }}`: References raw data loaded by external tools (Fivetran/Airbyte). Defined in `src_*.yml`.\n*   `{{ ref('model_name') }}`: References another dbt model within your project. Critical for building the **DAG** (Dependency Graph) so dbt knows the order of execution.\n\n---\n\n## Incremental Models\n\n### 5. Designing an Incremental Model\n**Q:** How do you configure a model to be incremental? What is the `is_incremental()` macro?\n**A:**\n```sql\n{{ config(\n    materialized='incremental',\n    unique_key='transaction_id'\n) }}\n\nSELECT *\nFROM {{ source('raw', 'transactions') }}\n\n{% if is_incremental() %}\n  -- This filter only runs on subsequent runs, not the first run\n  WHERE event_time > (SELECT MAX(event_time) FROM {{ this }})\n{% endif %}\n```\n*   **Logic:** The `if` block allows fetching only new data.\n*   **Unique Key:** dbt uses this to `MERGE` (Update existing, Insert new) instead of just appending.\n\n### 6. Full Refresh\n**Q:** What happens if you run `dbt run --full-refresh` on an incremental model?\n**A:**\nIt ignores the `is_incremental()` logic, drops the existing table, and rebuilds it entirely from scratch. This is needed if schema changes or logic changes drastically.\n\n---\n\n## Testing & Documentation\n\n### 7. Generic vs Singular Tests\n**Q:** What is the difference?\n**A:**\n*   **Generic Tests:** Reusable tests defined in `schema.yml`. dbt ships with 4 built-in:\n    1.  `unique`\n    2.  `not_null`\n    3.  `accepted_values`\n    4.  `relationships` (Foreign Key check)\n*   **Singular Tests:** Specific SQL files in the `tests/` folder. If the query returns *any rows*, the test fails. (e.g., `SELECT * FROM orders WHERE total < 0`).\n\n### 8. Freshness\n**Q:** How do you ensure your source data isn't stale?\n**A:**\nDefine `freshness` blocks in `src_*.yml`:\n```yaml\nsources:\n  - name: stripe\n    tables:\n      - name: payments\n        freshness:\n          warn_after: {count: 12, period: hour}\n          error_after: {count: 24, period: hour}\n        loaded_at_field: _etl_loaded_at\n```\nRun `dbt source freshness` to check.\n\n---\n\n## Snapshots (SCD Type 2)\n\n### 9. Implementing Slowly Changing Dimensions\n**Q:** How does dbt handle SCD Type 2 (Tracking history of changes)?\n**A:**\nVia **Snapshots**.\n*   **Strategy:** `timestamp` (updates based on `updated_at` column) or `check` (hashing a list of columns to detect change).\n*   dbt manages the columns `dbt_valid_from` and `dbt_valid_to` automatically.\n*   Stored in `snapshots/` folder.\n\n```sql\n{% snapshot orders_snapshot %}\n{{\n    config(\n      target_schema='snapshots',\n      unique_key='id',\n      strategy='timestamp',\n      updated_at='updated_at',\n    )\n}}\nselect * from {{ source('jaffle_shop', 'orders') }}\n{% endsnapshot %}\n```\nRun with `dbt snapshot`.\n\n---\n\n## Advanced Patterns\n\n### 10. Seeds\n**Q:** What are Seeds and when to use them?\n**A:**\n*   CSV files in `seeds/` directory.\n*   `dbt seed` loads them into the data warehouse.\n*   **Use Case:** Static lookup data like `country_codes` or `mapping_tables`. Not for loading large raw data.\n\n### 11. Custom Macros (DRY)\n**Q:** How do you avoid repeating SQL logic (e.g., converting cents to dollars)?\n**A:**\nWrite a Jinja macro in `macros/`.\n```sql\n{% macro cents_to_dollars(column_name, decimal_places=2) %}\n    ROUND( ({{ column_name }} / 100), {{ decimal_places }} )\n{% endmacro %}\n```\nUsage in model:\n`SELECT {{ cents_to_dollars('amount_cents') }} FROM payments`\n\n### 12. Deployment Environments\n**Q:** How do you handle Dev vs Prod environments?\n**A:**\n*   **profiles.yml:** Define distinct targets (`dev`, `prod`).\n*   **dbt Cloud / CI:**\n    *   `dev`: Writes to `dbt_jdoe` schema.\n    *   `prod`: Writes to `analytics` schema.\n*   **Limit Data in Dev:**\n    ```sql\n    {% if target.name == 'dev' %}\n    LIMIT 100\n    {% endif %}\n    ```\n"}, {"name": "faang_fintech_interview_questions.md", "type": "file", "content": "# Top 30 FAANG & FinTech Data Engineering Questions\n*Companies: Amazon, Google, Netflix, JPMC, Citi, Flipkart, Microsoft, Meta*\n\nA curated list of 30 highly distinct and challenging questions often asked in top-tier product and finance companies, focusing on SQL, Python, and PySpark.\n\n---\n\n## Table of Contents\n1. [SQL Scenarios (1-10)](#sql-scenarios)\n2. [Python Coding (11-20)](#python-coding)\n3. [PySpark & Big Data (21-30)](#pyspark--big-data)\n\n---\n\n## SQL Scenarios\n\n### 1. Market Basket Analysis (Amazon/Flipkart)\n**Q:** Find pairs of products that are most frequently bought together.\n**A:**\n```sql\nSELECT \n    p1.product_id as Product_A, \n    p2.product_id as Product_B, \n    COUNT(*) as frequency\nFROM Orders p1\nJOIN Orders p2 \n    ON p1.order_id = p2.order_id     -- Same Order\n    AND p1.product_id < p2.product_id -- Avoid duplicates (A-B vs B-A) and self-pairs (A-A)\nGROUP BY 1, 2\nORDER BY frequency DESC\nLIMIT 5;\n```\n\n### 2. Calculate Median Salary (Google/JPMC)\n**Q:** Calculate the median salary of employees without using a built-in `MEDIAN()` function.\n**A:**\n```sql\nSELECT AVG(salary) as MedianSalary\nFROM (\n    SELECT salary,\n           ROW_NUMBER() OVER (ORDER BY salary) as row_num,\n           COUNT(*) OVER () as total_count\n    FROM Employees\n) sub\nWHERE row_num IN ((total_count + 1) / 2, (total_count + 2) / 2); -- Handles both Odd and Even counts\n```\n\n### 3. Consecutive Active Days (Facebook/Zynga)\n**Q:** Find all users who logged in on 3 consecutive days.\n**A:**\n```sql\nSELECT DISTINCT user_id\nFROM (\n    SELECT user_id, login_date,\n           LAG(login_date, 1) OVER (PARTITION BY user_id ORDER BY login_date) as prev_1,\n           LAG(login_date, 2) OVER (PARTITION BY user_id ORDER BY login_date) as prev_2\n    FROM UserLogins\n) sub\nWHERE DATEDIFF(day, prev_1, login_date) = 1\n  AND DATEDIFF(day, prev_2, prev_1) = 1;\n```\n\n### 4. Running Balance with Reset (Citi/Morgan Stanley)\n**Q:** Calculate a running total of transactions, but reset the running total to 0 if the account type changes.\n**A:**\n```sql\nSELECT \n    transaction_id, account_type, amount,\n    SUM(amount) OVER (PARTITION BY account_type ORDER BY transaction_date) as running_balance\nFROM Transactions;\n```\n\n### 5. Resurrected Users (Netflix/Spotify)\n**Q:** Identify users who were active last month, inactive this month, but active again next month (Churned then Returned).\n**A:**\n```sql\nWITH MonthlyActivity AS (\n    SELECT user_id, \n           DATE_TRUNC('month', activity_date) as mth\n    FROM activity\n    GROUP BY 1, 2\n)\nSELECT t1.user_id\nFROM MonthlyActivity t1\nLEFT JOIN MonthlyActivity t2 ON t1.user_id = t2.user_id AND t2.mth = t1.mth + INTERVAL '1 month'\nJOIN MonthlyActivity t3 ON t1.user_id = t3.user_id AND t3.mth = t1.mth + INTERVAL '2 month'\nWHERE t2.user_id IS NULL; -- Active Month 1, Inactive Month 2, Active Month 3\n```\n\n### 6. First Touch Attribution (Marketing Tech)\n**Q:** Find the *first* channel a user came from before converting.\n**A:**\n```sql\nSELECT DISTINCT user_id, \n       FIRST_VALUE(channel) OVER (PARTITION BY user_id ORDER BY timestamp ASC) as first_channel\nFROM user_journey\nWHERE conversion_event = 1;\n```\n\n### 7. Identifying Mutual Friends (Meta)\n**Q:** Given table `Friends(user1, user2)`, count mutual friends for all pairs.\n**A:**\n```sql\nWITH AllFriends AS (\n    SELECT user1, user2 FROM Friends\n    UNION ALL\n    SELECT user2, user1 FROM Friends -- Ensure bidirectionality\n)\nSELECT a.user1, b.user1 as user2, COUNT(*) as mutuals\nFROM AllFriends a\nJOIN AllFriends b ON a.user2 = b.user2 -- Join on common friend\nAND a.user1 < b.user1\nGROUP BY 1, 2;\n```\n\n### 8. Session Timeout Identification\n**Q:** Calculate session duration, defining a session break as 30 mins of inactivity.\n**A:**\n```sql\nSELECT user_id, session_id, MIN(ts) as start, MAX(ts) as end\nFROM (\n    SELECT user_id, ts,\n           SUM(is_new_session) OVER (PARTITION BY user_id ORDER BY ts) as session_id\n    FROM (\n        SELECT user_id, ts, \n               CASE WHEN ts - LAG(ts) OVER (PARTITION BY user_id ORDER BY ts) > 1800 \n               THEN 1 ELSE 0 END as is_new_session\n        FROM clicks\n    ) a\n) b\nGROUP BY 1, 2;\n```\n\n### 9. Delete Duplicates using SQL (No Distinct)\n**Q:** Delete duplicate emails, keeping the one with the smallest ID.\n**A:**\n```sql\nDELETE FROM Person \nWHERE Id NOT IN (\n    SELECT MIN(Id) \n    FROM Person \n    GROUP BY Email\n);\n```\n\n### 10. Histogram of Frequency (Uber)\n**Q:** Calculate the distribution of the number of rides users took (e.g., How many users took 1 ride? How many took 2?).\n**A:**\n```sql\nSELECT rides_count, COUNT(user_id) as num_users\nFROM (\n    SELECT user_id, COUNT(*) as rides_count\n    FROM rides\n    GROUP BY user_id\n) sub\nGROUP BY rides_count\nORDER BY rides_count;\n```\n\n---\n\n## Python Coding\n\n### 11. Flatten Nested JSON (Amazon)\n**Q:** Write a function to flatten an arbitrarily nested JSON object.\n**A:**\n```python\ndef flatten_json(y):\n    out = {}\n    def flatten(x, name=''):\n        if type(x) is dict:\n            for a in x: flatten(x[a], name + a + '.')\n        elif type(x) is list:\n            for i, a in enumerate(x): flatten(a, name + str(i) + '.')\n        else:\n            out[name[:-1]] = x\n    flatten(y)\n    return out\n```\n\n### 12. Parse Large Log File (Google)\n**Q:** Top 10 IP addresses from 50GB log file (Constant Memory).\n**A:**\n```python\nfrom collections import Counter\ndef top_ips(path):\n    c = Counter()\n    with open(path) as f:\n        for line in f:\n            c[line.split()[0]] += 1\n    return c.most_common(10)\n```\n\n### 13. Valid Parentheses (Microsoft)\n**Q:** Check if a string of brackets `()[]{}` is valid.\n**A:**\n```python\ndef isValid(s):\n    stack = []\n    mapping = {\")\": \"(\", \"}\": \"{\", \"]\": \"[\"}\n    for char in s:\n        if char in mapping:\n            top_element = stack.pop() if stack else '#'\n            if mapping[char] != top_element:\n                return False\n        else:\n            stack.append(char)\n    return not stack\n```\n\n### 14. Group Anagrams (Apple)\n**Q:** Group `[\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"]`.\n**A:**\n```python\nfrom collections import defaultdict\ndef groupAnagrams(strs):\n    ans = defaultdict(list)\n    for s in strs:\n        ans[tuple(sorted(s))].append(s)\n    return list(ans.values())\n```\n\n### 15. Merge K Sorted Lists (Streaming Data)\n**Q:** Merge K sorted streams (iterators) into one sorted stream.\n**A:**\n```python\nimport heapq\ndef merge_k_sorted(lists):\n    heap = []\n    # Add first element of each list to heap\n    for i, lst in enumerate(lists):\n        if lst: heapq.heappush(heap, (lst[0], i, 0)) # val, list_idx, element_idx\n    \n    result = []\n    while heap:\n        val, list_idx, element_idx = heapq.heappop(heap)\n        result.append(val)\n        \n        # Push next element from the same list\n        if element_idx + 1 < len(lists[list_idx]):\n            heapq.heappush(heap, (lists[list_idx][element_idx+1], list_idx, element_idx+1))\n            \n    return result\n```\n\n### 16. Missing Number in Sequence\n**Q:** Find the missing number in array `[0...n]`.\n**A:**\n```python\ndef missingNumber(nums):\n    n = len(nums)\n    expected_sum = n * (n + 1) // 2\n    return expected_sum - sum(nums)\n```\n\n### 17. LRU Cache Implementation (System Design)\n**Q:** Design an LRU Cache with O(1) operations.\n**A:** Use `OrderedDict`.\n```python\nfrom collections import OrderedDict\nclass LRUCache(OrderedDict):\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n\n    def get(self, key: int) -> int:\n        if key not in self: return -1\n        self.move_to_end(key)\n        return self[key]\n\n    def put(self, key: int, value: int) -> None:\n        if key in self: self.move_to_end(key)\n        self[key] = value\n        if len(self) > self.capacity: self.popitem(last=False)\n```\n\n### 18. API Rate Limiter (Logic)\n**Q:** Write a function that returns False if called more than N times in 1 second.\n**A:**\n```python\nimport time\nfrom collections import deque\n\nclass RateLimiter:\n    def __init__(self, limit):\n        self.limit = limit\n        self.timestamps = deque()\n    \n    def allow_request(self):\n        now = time.time()\n        # Remove timestamps older than 1s\n        while self.timestamps and self.timestamps[0] <= now - 1:\n            self.timestamps.popleft()\n            \n        if len(self.timestamps) < self.limit:\n            self.timestamps.append(now)\n            return True\n        return False\n```\n\n### 19. ETL: Clean and Transform Date Strings\n**Q:** Convert list of mixed date formats `['2021-01-01', '01/02/2021']` to standard `YYYY-MM-DD`.\n**A:** \n```python\nfrom dateutil import parser\ndef normalize_dates(dates):\n    return [parser.parse(d).strftime('%Y-%m-%d') for d in dates]\n```\n\n### 20. Find Intersection of Two Arrays\n**Q:** Efficiently find intersection or two huge arrays.\n**A:**\n```python\ndef intersection(nums1, nums2):\n    return list(set(nums1) & set(nums2))\n```\n\n---\n\n## PySpark & Big Data\n\n### 21. Handling Skewed Join (Amazon)\n**Q:** Fix OOM when joining Sales (Billion) and Products (Million - skewed).\n**A:** Use **Salting**.\n```python\n# 1. Salt Big Table\ndf_big = df_big.withColumn(\"salt\", (rand() * 10).cast(\"int\"))\n# 2. Explode Small Table\ndf_small = df_small.withColumn(\"salt_arr\", array([lit(i) for i in range(10)]))\\\n                   .withColumn(\"salt\", explode(\"salt_arr\"))\n# 3. Join\ndf_joined = df_big.join(df_small, [\"key\", \"salt\"])\n```\n\n### 22. Sessionization (Netflix)\n**Q:** New session after 30 mins inactivity.\n**A:** \n```python\nw = Window.partitionBy(\"user\").orderBy(\"ts\")\ndf = df.withColumn(\"new_sess\", (col(\"ts\") - lag(\"ts\").over(w) > 1800).cast(\"int\"))\ndf = df.withColumn(\"sess_id\", sum(\"new_sess\").over(w))\n```\n\n### 23. Top N Items per Group at Scale (Uber)\n**Q:** Top 3 items sold per category (Optimized).\n**A:**\n```python\nw = Window.partitionBy(\"category\").orderBy(desc(\"sales\"))\ndf_top = df.withColumn(\"rn\", rank().over(w)).filter(\"rn <= 3\")\n```\n\n### 24. Explode and Aggregate (Array Handling)\n**Q:** Database has column `tags` (Array). Count frequency of each tag.\n**A:**\n```python\ndf.select(explode(col(\"tags\")).alias(\"tag\")) \\\n  .groupBy(\"tag\").count().orderBy(desc(\"count\")).show()\n```\n\n### 25. Broadcast Variable Usage\n**Q:** When to use Broadcast Variable vs Broadcast Join?\n**A:**\n*   **Broadcast Join:** Distributes a DataFrame (Table).\n*   **Broadcast Variable:** Distributes a read-only object (e.g., a huge Dictionary/List) to executors for efficient lookups in UDFs or map operations, avoiding shipping it with every task closure.\n\n### 26. Custom Accumulator\n**Q:** How to count \"Bad Records\" without stopping the job or bringing data to driver?\n**A:**\n```python\nbad_records = spark.sparkContext.accumulator(0)\n\ndef process_row(row):\n    if row['val'] is None:\n        bad_records.add(1)\n        \ndf.foreach(process_row)\nprint(bad_records.value)\n```\n\n### 27. Filter Pushdown optimization\n**Q:** You filter a parquet file by `date`. Why is it fast?\n**A:** \nSpark pushes the filter to the Parquet reader. Parquet files have footer metadata (Min/Max values for columns). Spark skips entire Row Groups that don't match the filter (Predicate Pushdown), minimizing I/O.\n\n### 28. Coalesce vs Repartition (Scenario)\n**Q:** You have 1000 small files. You want to write 10 files.\n**A:** `df.coalesce(10).write...`.\n**Q:** You want to write by date partition?\n**A:** `df.repartition(\"date\").write.partitionBy(\"date\")...`\n\n### 29. Handling Bad Data (JSON)\n**Q:** How to handle corrupt JSON lines in a read?\n**A:**\n```python\ndf = spark.read.option(\"mode\", \"PERMISSIVE\") \\\n               .option(\"columnNameOfCorruptRecord\", \"_corrupt\") \\\n               .json(\"path\")\ndf_bad = df.filter(col(\"_corrupt\").isNotNull())\n```\n\n### 30. Unit Testing PySpark\n**Q:** How do you test a PySpark transformation function locally?\n**A:**\nUse `chispa` or `pandas` testing for assertion.\nCreate a local SparkSession in the test fixture.\n```python\ndef test_transform(spark):\n    source_df = spark.createDataFrame([(1,)], [\"id\"])\n    actual_df = my_transform(source_df)\n    expected_df = spark.createDataFrame([(1, 10)], [\"id\", \"val\"])\n    assert_df_equality(actual_df, expected_df)\n```\n"}, {"name": "misc_questions_1.md", "type": "file", "content": "# Miscellaneous PySpark Deep Dive\n*Core Internals & Optimization*\n\nDetailed explanations of Spark architecture components and data organization strategies.\n\n---\n\n## Table of Contents\n1. [Spark Execution Flow](#spark-execution-flow)\n2. [Catalyst Optimizer](#catalyst-optimizer)\n3. [Transformations: Wide vs Narrow](#transformations-wide-vs-narrow)\n4. [Optimization Features (Tungsten, PPD)](#optimization-features)\n5. [Data Layout: Partitioning vs Bucketing](#data-layout-partitioning-vs-bucketing)\n6. [SQL Concepts (Ranking, Window)](#sql-concepts-ranking-window)\n7. [Python Concepts (Decorators, Structures)](#python-concepts-decorators-structures)\n8. [Join Logic Scenarios](#join-logic-scenarios)\n9. [Common Coding Scenarios (Empty DF, Sessionization)](#common-coding-scenarios-empty-df-sessionization)\n10. [Azure & Databricks Integrations](#azure--databricks-integrations)\n11. [Azure Data Factory Components](#azure-data-factory-components)\n\n---\n\n## Spark Execution Flow\n\n### 1. Stages of Execution\n**Q:** Explain the lifecycle of a Spark Application from code to execution.\n**A:**\n1.  **Driver Program:** The main entry point (`SparkSession`). It converts valid user code into a **Logical Plan**.\n2.  **DAG Scheduler:**\n    *   Converts the Logical Plan into a Physical Plan (using Catalyst).\n    *   Breaks the graph of RDDs into **Stages**.\n    *   Stages are determined by **Shuffle Boundaries** (Wide transformations like `groupBy` break the DAG into new stages).\n3.  **Task Scheduler:**\n    *   Breaks Stages into **Tasks**.\n    *   A Task is the smallest unit of work (one task per partition).\n    *   Submits TaskSets to the Cluster Manager (YARN/K8s).\n4.  **Executor:**\n    *   Receives tasks and executes them on the worker nodes.\n    *   Reports status/results back to the Driver.\n\n---\n\n## Catalyst Optimizer\n\n### 2. What is the Catalyst Optimizer?\n**Q:** Describe the phases of the Catalyst Optimizer.\n**A:**\nThe engine that optimizes DataFrames/Datasets/SQL.\n1.  **Analysis:** Checks syntax and resolves references (e.g., checking if column `id` exists in table `users`) using the Catalog.\n2.  **Logical Optimization:** Applies standard rule-based optimizations:\n    *   *Predicate Pushdown* (Filter early).\n    *   *Constant Folding* (e.g., convert `1 + 2` to `3`).\n    *   *Projection Pruning* (Remove unused columns).\n3.  **Physical Planning:** Generates multiple physical plans (strategies) to execute the logical plan (e.g., choosing `BroadcastHashJoin` vs `SortMergeJoin`). Is uses a **Cost Model** to check which strategy is cheapest.\n4.  **Code Generation:** Generates optimized Java Bytecode to run on the executors.\n\n---\n\n## Transformations: Wide vs Narrow\n\n### 3. Narrow vs Wide Transformations\n**Q:** Difference between Narrow and Wide transformations?\n**A:**\n*   **Narrow Transformation:**\n    *   *Definition:* Each partition of the parent RDD is used by at most one partition of the child RDD.\n    *   *Shuffle:* No shuffle required. Data stays on the same node.\n    *   *Examples:* `map`, `filter`, `union`, `select`.\n    *   *Performance:* Fast, pipelined in memory.\n*   **Wide Transformation:**\n    *   *Definition:* Each partition of the parent RDD may be used by multiple partitions of the child RDD.\n    *   *Shuffle:* **Requires Shuffle**. Data must be redistributed across the network.\n    *   *Examples:* `groupBy`, `distinct`, `join` (except broadcast), `repartition`.\n    *   *Performance:* Slower, breaks the stage boundary.\n\n---\n\n## Optimization Features\n\n### 4. Predicate Pushdown (PPD)\n**Q:** What is Predicate Pushdown?\n**A:**\nAn optimization where filtering logic is \"pushed down\" to the database or file source, minimizing the amount of data transferred to Spark.\n*   *Without PPD:* Read 1TB file -> Filter in Spark -> Result.\n*   *With PPD:* Spark tells Parquet reader \"Only give me rows where `date='2023-01-01'`\". Source scans metadata/indices and returns only 10GB.\n*   *Supported formats:* Parquet, ORC, JDBC sources.\n\n### 5. Project Tungsten\n**Q:** What is the goal of Project Tungsten?\n**A:**\nTo optimize CPU and Memory efficiency (improving hardware utilization).\n1.  **Memory Management (Off-Heap):** Manages memory explicitly (sun.misc.Unsafe) to avoid JVM Garbage Collection overhead.\n2.  **Cache-aware Computation:** Designs algorithms to exploit L1/L2/L3 CPU caches.\n3.  **Code Generation (Whole-Stage Codegen):** Collapses multiple operators (Filter -> Map -> Select) into a single optimized function (function fusing) to eliminate virtual function calls.\n\n---\n\n## Data Layout: Partitioning vs Bucketing\n\n### 6. Partitioning vs Bucketing\n**Q:** When to use Partitioning vs Bucketing?\n\n| Feature | Partitioning (`partitionBy`) | Bucketing (`bucketBy`) |\n|---------|------------------------------|------------------------|\n| **Structure** | Creates sub-directories (`/year=2023/`) | Creates fixed number of files per directory |\n| **Cardinality** | Best for **Low** cardinality (Year, Month, Country) | Best for **High** cardinality (User_ID, Product_ID) |\n| **Use Case** | Optimizes Filters (`WHERE year=2023`) | Optimizes Joins (`ON user_id`) to avoid Shuffle |\n| **Maintenance** | Easy to append new partitions | Harder (Must rewrite table to change # buckets) |\n\n### 7. Types of Partitioning\n**Q:** What are the partitioning strategies in Spark?\n**A:**\n1.  **Hash Partitioning (Default):**\n    *   `partitionBy(col)` splits data based on `hash(col) % numPartitions`. ensures even distribution if data isn't skewed.\n2.  **Range Partitioning:**\n    *   Partitions based on sorted range of keys (e.g., A-F, G-M). Used in `sort` operations. Good for range queries.\n3.  **Round Robin Partitioning:**\n    *   Distributes data simply to equalize partition size. No guarantee that keys end up together. Used in `repartition()` without columns.\n\n---\n\n## SQL Concepts (Ranking, Window)\n\n### 8. Rank, Dense Rank, Row Number\n**Q:** Explain the difference with data `[10, 20, 20, 30]`.\n**A:**\n*   **ROW_NUMBER():** Unique sequential number.\n    *   `1, 2, 3, 4`\n*   **RANK():** Same rank for ties, skips next number.\n    *   `1, 2, 2, 4` (Notice 3 is skipped)\n*   **DENSE_RANK():** Same rank for ties, does NOT skip next number.\n    *   `1, 2, 2, 3`\n\n### 9. Window Functions\n**Q:** What is a Window Function?\n**A:**\nPerforms a calculation across a set of table rows that are somehow related to the current row. Unlike `GROUP BY`, it retains the original rows.\n*   **Structure:** `Function() OVER (PARTITION BY col ORDER BY col)`\n*   **Types:**\n    *   *Ranking:* `rank`, `dense_rank`, `row_number`\n    *   *Aggregate:* `sum`, `avg`, `min`, `max` (Running totals)\n    *   *Value:* `lag`, `lead`, `first_value`, `last_value`\n\n---\n\n## Python Concepts (Decorators, Structures)\n\n### 10. Decorators\n**Q:** What are decorators?\n**A:**\nA design pattern to modify the behavior of a function without changing its code. It takes a function as argument and returns a wrapper function.\n```python\ndef my_decorator(func):\n    def wrapper():\n        print(\"Before function\")\n        func()\n        print(\"After function\")\n    return wrapper\n\n@my_decorator\ndef say_hello():\n    print(\"Hello!\")\n```\n\n### 11. List vs Tuple vs Dictionary\n**Q:** Comparison of Python data structures.\n**A:**\n*   **List (`[]`):** Mutable, Ordered. Good for collections that change. `[1, 2, 3]`\n*   **Tuple (`()`):** Immutable, Ordered. Faster than lists. Good for fixed data. `(1, 2, 3)`\n*   **Dictionary (`{}`):** Mutable, Unordered (until Python 3.7), Key-Value pairs. Hash-map implementation (O(1) lookup). `{'a': 1}`\n*   **Set (`{}`):** Mutable, Unordered, Unique elements. `{'a', 'b'}`\n\n---\n\n## Join Logic Scenarios\n\n### 12. Join Output Scenario\n**Q:** Given two tables with duplicate keys `1`, calculate the number of rows for each join type.\n\n**Table A:**\n```\nid\n1\n1\n1\n```\n(3 rows of '1')\n\n**Table B:**\n```\nid\n1\n1\n1\n```\n(3 rows of '1')\n\n**A:**\n*   **Inner Join:** Cartesian product of matching keys.\n    *   `3 (Table A) * 3 (Table B)` = **9 rows**\n*   **Left Join:** All from Left (3) * Matching from Right (3).\n    *   **9 rows**\n*   **Right Join:** All from Right (3) * Matching from Left (3).\n    *   **9 rows**\n*   **Cross Join:** Cartesian product of all rows.\n    *   `3 * 3` = **9 rows**\n*   **Full Outer Join:** Matches (9) + Unmatched Left (0) + Unmatched Right (0).\n    *   **9 rows**\n\n---\n\n## Common Coding Scenarios (Empty DF, Sessionization)\n\n### 13. Create Empty DataFrame\n**Q:** How do you create an empty DataFrame with a schema in PySpark?\n**A:**\n```python\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\n# Define Schema\nschema = StructType([\n    StructField(\"Name\", StringType(), True),\n    StructField(\"Age\", IntegerType(), True)\n])\n\n# Create Empty DataFrame\ndf = spark.createDataFrame([], schema)\n\n# Verify\ndf.printSchema()\n```\n\n### 14. Sessionization Logic\n**Q:** How do you create sessions based on inactivity (e.g., 30 mins)?\n**A:**\nUse `lag` to find the time difference and a cumulative sum to generate IDs.\n```python\nfrom pyspark.sql.functions import lag, col, sum, when\nfrom pyspark.sql.window import Window\n\n# 1. Calculate time difference\nw = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\ndf = df.withColumn(\"prev_ts\", lag(\"timestamp\").over(w))\ndf = df.withColumn(\"diff\", col(\"timestamp\").cast(\"long\") - col(\"prev_ts\").cast(\"long\"))\n\n# 2. Flag new session if diff > 1800 sec (30 mins) or if first row (null)\ndf = df.withColumn(\"is_new_session\", \n    when((col(\"diff\") > 1800) | (col(\"diff\").isNull()), 1).otherwise(0)\n)\n\n# 3. Generate Session ID using cumulative sum\ndf_sessionized = df.withColumn(\"session_id\", sum(\"is_new_session\").over(w))\n```\n\n---\n\n## Azure & Databricks Integrations\n\n### 15. Key Vault & Secrets\n**Q:** How do you access secrets (passwords/keys) in Databricks securely?\n**A:**\nUse **Secret Scopes**.\n1.  **Create Scope:** Create an Azure Key Vault backed secret scope in Databricks (e.g., `my-scope`).\n2.  **Access in Notebook:**\n    ```python\n    password = dbutils.secrets.get(scope=\"my-scope\", key=\"db-password\")\n    ```\n3.  **Security:** The secret value is redacted `[REDACTED]` in notebook logs if printed.\n\n### 16. Mounting ADLS Gen2\n**Q:** How do you connect Databricks to ADLS Gen2?\n**A:**\nUse a **Service Principal** and mount the storage.\n```python\nconfigs = {\n  \"fs.azure.account.auth.type\": \"OAuth\",\n  \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n  \"fs.azure.account.oauth2.client.id\": client_id,\n  \"fs.azure.account.oauth2.client.secret\": client_secret,\n  \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/...\"\n}\n\ndbutils.fs.mount(\n  source = \"abfss://container@storageaccount.dfs.core.windows.net/\",\n  mount_point = \"/mnt/data\",\n  extra_configs = configs\n)\n```\n\n---\n\n## Azure Data Factory Components\n\n### 17. Copy Activity & Performance\n**Q:** Explain ADF Copy Activity and how to tune it.\n**A:**\n*   **What it is:** The primary activity to move data between sources and sinks (e.g., SQL -> Blob).\n*   **Performance Tuning:**\n    *   **DIU (Data Integration Units):** Increase compute power (Cloud IR).\n    *   **Parallel Copies:** Increase parallel file copy count.\n    *   **Staging:** Enable Staging (via Blob Storage) when loading into Synapse/Snowflake to use PolyBase/COPY command (much faster than row-by-row insertion).\n\n### 18. Triggers\n**Q:** What are the types of triggers in ADF?\n**A:**\n1.  **Schedule Trigger:** Wall-clock schedule (Every day at 9 AM).\n2.  **Tumbling Window Trigger:** Processes time slices (9:00-10:00, 10:00-11:00). Supports backfilling and dependencies.\n3.  **Event-Based Trigger:** Reacts to storage events (`BlobCreated`, `BlobDeleted`). Starts pipeline immediately when file lands.\n4.  **Custom Event Trigger:** Reacts to events from Event Grid (custom topics).\n\n### 19. Event Hubs Integration\n**Q:** How does ADF/Databricks interact with Event Hubs?\n**A:**\n*   **ADF:** Can *not* natively trigger pipelines from individual Event Hub messages (streaming). It can primarily write to Event Hubs or read batches *if* captured.\n*   **Databricks (Spark):** The standard consumer.\n    *   **Read:** `spark.readStream.format(\"eventhubs\")...`\n    *   **Write:** `df.writeStream.format(\"eventhubs\")...`\n*   **Capture:** Event Hubs can \"Capture\" data automatically to ADLS (Avro files). ADF can then process these files in batch triggers (`BlobCreated`).\n"}, {"name": "other", "type": "directory", "children": [{"name": "achievements.md", "type": "file", "content": "# Explaining Resume Achievements in Interviews (Data Engineering)\n\nUse the **Context \u2192 Problem \u2192 Action \u2192 Measurement \u2192 Impact** framework to explain each quantified achievement clearly and confidently.\n\n---\n\n## 1. Improved data pipeline efficiency by 30%\n\n**Context**  \nThe platform had multiple batch pipelines built on Azure Databricks and Azure Data Factory processing high-volume transactional and operational data.\n\n**Problem**  \nPipelines were slow, frequently retried, and often missed SLAs due to inefficient Spark jobs and suboptimal storage patterns.\n\n**Actions**  \n- Analyzed Spark job metrics to identify bottlenecks (data skew, excessive shuffles, small files).  \n- Implemented Delta Lake optimizations such as `OPTIMIZE` and `Z-ORDER`.  \n- Tuned partitioning and removed redundant transformations.  \n- Parallelized ingestion workflows where possible and cached frequently used datasets.\n\n**Measurement**  \nCompared average end-to-end pipeline runtime using Databricks job metrics before and after optimization.\n\n**Impact**  \nPipeline runtime reduced by ~30%, enabling faster downstream analytics and earlier availability of business insights.\n\n---\n\n## 2. Delivered operational dashboards reducing reporting delays by 25%\n\n**Context**  \nOperations teams relied on delayed and partially manual reports for daily decision-making.\n\n**Problem**  \nReporting lag was typically T+1 or T+2 due to batch dependencies and manual reconciliation.\n\n**Actions**  \n- Designed incremental data models and aggregated fact tables.  \n- Automated data refresh pipelines feeding Tableau dashboards.  \n- Standardized KPIs with business stakeholders.  \n- Implemented data validation checks to reduce manual corrections.\n\n**Measurement**  \nMeasured the time gap between data generation and dashboard availability.\n\n**Impact**  \nReporting delays were reduced by ~25%, allowing teams to act on near-real-time operational metrics.\n\n---\n\n## 3. Enabled real-time transaction monitoring, reducing errors by 15%\n\n**Context**  \nHigh-volume transactional systems detected errors only after batch processing.\n\n**Problem**  \nLate detection increased operational risk and reprocessing costs.\n\n**Actions**  \n- Implemented real-time ingestion using Kafka and Spark Structured Streaming.  \n- Added schema validation, deduplication logic, and anomaly detection rules.  \n- Built monitoring dashboards and alerting mechanisms for early issue detection.\n\n**Measurement**  \nTracked error rates and incident counts before and after real-time monitoring implementation.\n\n**Impact**  \nEarly detection reduced transaction errors by ~15% and improved system reliability.\n\n---\n\n## 4. Reduced deployment errors by 20% through GitLab-driven CI/CD integration\n\n**Context**  \nDeployments were largely manual across multiple environments.\n\n**Problem**  \nManual steps led to configuration mismatches and frequent deployment failures.\n\n**Actions**  \n- Designed GitLab CI/CD pipelines to automate build, test, and deployment steps.  \n- Added code validation, unit tests, and environment-specific configurations.  \n- Implemented rollback strategies and standardized release processes.\n\n**Measurement**  \nCompared deployment failure rates and rollback incidents before and after CI/CD adoption.\n\n**Impact**  \nDeployment errors reduced by ~20%, improving release stability and predictability.\n\n---\n\n## 5. Delivered end-to-end Tableau dashboards for campaign and regulatory compliance reporting\n\n**Context**  \nBusiness and compliance teams needed consistent, audit-ready reporting.\n\n**Problem**  \nData was fragmented across systems, leading to inconsistencies and manual reconciliation.\n\n**Actions**  \n- Owned the full pipeline from ingestion to transformation and visualization.  \n- Aligned transformation logic with regulatory definitions.  \n- Implemented data quality checks and audit-friendly documentation.  \n- Designed Tableau dashboards with drill-downs and filters.\n\n**Impact**  \nDashboards became a single source of truth for campaign tracking and regulatory submissions.\n\n---\n\n## 6. Successfully migrated 100+ ingestion flows, increasing pipeline throughput\n\n**Context**  \nLegacy ingestion pipelines were brittle and difficult to scale with growing data volumes.\n\n**Problem**  \nExisting pipelines had low throughput and high maintenance overhead.\n\n**Actions**  \n- Migrated pipelines to a standardized, metadata-driven ingestion framework.  \n- Parameterized configurations and enabled parallel ingestion.  \n- Improved error handling, logging, and retry mechanisms.\n\n**Measurement**  \nMeasured records processed per hour and pipeline stability metrics before and after migration.\n\n**Impact**  \nPipeline throughput increased significantly while reducing operational and maintenance effort.\n\n---\n\n## Interview Delivery Tips\n\n- Do not repeat resume bullets verbatim; tell the story behind them.  \n- Always mention **tools, design decisions, and trade-offs**.  \n- Be prepared to explain how metrics were measured and validated.  \n- Align impacts to **business value**, not just technical improvements.\n"}]}, {"name": "other_questions", "type": "directory", "children": [{"name": "airbnb_questions.md", "type": "file", "content": "Below is a **realistic, Airbnb-style interview question set** mapped **directly to the job description**, not generic DE questions. This mirrors how Airbnb typically evaluates **senior data engineers**: depth in distributed systems + strong product & data thinking (merchandising focus).\n\nI\u2019ve grouped questions by **interview rounds / competencies** so you can prepare structurally.\n\n---\n\n## 1\ufe0f\u20e3 Data Pipeline Design & Architecture (Core Round)\n\n**High-probability questions**\n\n1. Design an end-to-end data pipeline to capture **user interactions (clicks, searches, bookings)** and make it available for **merchandising optimization**.\n2. How would you design a pipeline that ingests **batch + real-time data** at Airbnb scale?\n3. How do you handle **late-arriving events** in user interaction data?\n4. What strategies do you use to make pipelines **idempotent**?\n5. How do you design pipelines to handle **schema evolution** without breaking downstream consumers?\n6. Explain how you would design a **petabyte-scale Spark pipeline** that runs daily and hourly workloads.\n7. How do you decide between **batch vs streaming** for a given use case?\n8. What trade-offs exist between **Lambda vs Kappa architecture**?\n\n**Follow-ups Airbnb likes**\n\n* What metrics would you track to know the pipeline is healthy?\n* How would you backfill 6 months of corrupted data?\n\n---\n\n## 2\ufe0f\u20e3 Spark, SparkSQL & Scala (Very Important)\n\n**Deep-dive questions**\n\n1. Explain Spark\u2019s **execution model** (DAG, stages, tasks).\n2. Difference between **narrow vs wide transformations** with real examples.\n3. How does **SparkSQL Catalyst Optimizer** work?\n4. How do you optimize a slow SparkSQL query scanning billions of rows?\n5. When would you use **Dataset vs DataFrame vs RDD**?\n6. Explain **shuffle**, why it\u2019s expensive, and how you minimize it.\n7. How do you handle **data skew** in Spark?\n8. What are **broadcast joins** and when do they fail?\n9. How do you tune **Spark memory** (executor vs driver)?\n\n**Scala-specific**\n\n* How does immutability help in distributed systems?\n* Example of writing a type-safe Spark Dataset in Scala\n\n---\n\n## 3\ufe0f\u20e3 Airflow & Workflow Orchestration\n\n**Likely questions**\n\n1. How does Airflow work internally? (Scheduler, Executor, Metastore)\n2. Design an Airflow DAG for:\n\n   * Raw \u2192 Clean \u2192 Aggregated \u2192 ML-ready tables\n3. How do you manage **dependencies between hundreds of DAGs**?\n4. How do you implement **backfills safely**?\n5. Difference between **catchup**, **depends_on_past**, and **SLAs**.\n6. How do you handle **partial DAG failures**?\n7. How would you design Airflow to support **multi-tenant teams**?\n\n**Advanced**\n\n* Airflow scaling challenges at large companies\n* When Airflow is the wrong tool\n\n---\n\n## 4\ufe0f\u20e3 Data Modeling & Warehousing (Merchandising Focus)\n\n**High-signal questions**\n\n1. How would you model data for **listing ranking and merchandising optimization**?\n2. Explain **fact vs dimension tables** using Airbnb listings.\n3. How would you design a table to track **price changes over time**?\n4. How do you handle **slowly changing dimensions (SCD Type 2)**?\n5. How do you balance **query performance vs storage cost**?\n6. How would you design data models for **A/B experiments**?\n\n**Warehousing**\n\n* Difference between row-based vs columnar storage\n* When would you choose ClickHouse vs BigQuery vs Redshift?\n\n---\n\n## 5\ufe0f\u20e3 Data Quality, Reliability & Observability (Airbnb Cares a LOT)\n\n**Common questions**\n\n1. How do you define **data quality**?\n2. How do you detect **missing, duplicated, or anomalous data** automatically?\n3. How do you design **data validation checks** at scale?\n4. What happens when a bad pipeline publishes wrong data to business dashboards?\n5. How would you implement **data contracts**?\n6. What SLAs/SLOs make sense for data pipelines?\n\n**Scenario**\n\n> A dashboard used by leadership shows a 20% drop in bookings. What do you do?\n\n---\n\n## 6\ufe0f\u20e3 Distributed Systems & Scale\n\n**Expected depth**\n\n1. How does Kafka guarantee ordering?\n2. Exactly-once vs at-least-once semantics\u2014trade-offs?\n3. How would you design a pipeline handling **millions of events/sec**?\n4. What happens when a Spark executor dies?\n5. How do you handle **hot partitions**?\n6. How do you design systems that degrade gracefully?\n\n---\n\n## 7\ufe0f\u20e3 Collaboration & Product Thinking (Behavioral but Technical)\n\nAirbnb strongly values **product-aware engineers**.\n\n**Questions**\n\n1. How do you work with Data Scientists to productionize features?\n2. How do you push back when PMs request unrealistic data timelines?\n3. Describe a time you improved data usability for non-technical users.\n4. How do you prioritize tech debt vs feature delivery?\n5. How do you influence standards without authority?\n\n---\n\n## 8\ufe0f\u20e3 Code Quality & Internal Tooling\n\n**Likely**\n\n1. How do you enforce code quality in data pipelines?\n2. What CI/CD checks would you add for Spark jobs?\n3. How do you do safe deployments for data pipelines?\n4. Describe a tool you built that improved data engineering productivity.\n5. How do you detect breaking schema changes automatically?\n\n---\n\n## 9\ufe0f\u20e3 SQL & Analytics Thinking\n\n**Real-world SQL**\n\n1. Write a query to find listings whose **conversion dropped WoW**.\n2. How would you detect **outliers in pricing data** using SQL?\n3. Window functions you frequently use and why.\n4. How do you optimize a slow analytical query?\n\n---\n\n## \ud83d\udd1f Airbnb-Style System Design Question (Very High Probability)\n\n> **Design a data platform that supports real-time and batch merchandising optimization for Airbnb listings.**\n\nThey expect:\n\n* Data sources\n* Ingestion (Kafka / batch)\n* Storage layers\n* Processing (Spark)\n* Modeling\n* Data quality\n* Consumers (DS, dashboards, ML)\n* Monitoring & failure handling\n\n---\n\n## \ud83c\udfaf How You Should Prepare (Given Your Background)\n\nGiven your **Databricks + Spark + Azure + CI/CD** experience:\n\n* Map **Unity Catalog \u2192 Airbnb data governance**\n* Map **Delta Lake \u2192 internal Airbnb lake**\n* Emphasize **data quality + product impact**, not just tech\n* Prepare **1\u20132 strong stories** where your work directly improved business metrics\n\n---\n\nIf you want, next I can:\n\n* \ud83d\udd25 Do **mock Airbnb system design**\n* \ud83e\udde0 Give **model answers** for top 15 questions\n* \ud83c\udfaf Tailor answers **specifically to your resume**\n\nJust tell me.\n"}, {"name": "amazon_answers.md", "type": "file", "content": "# Amazon Data Engineering Interview - Practice Model Answers\n\nThis document provides solving strategies and model answers for the key areas mentioned in your preparation guide: **HackerRank SQL**, **Data Modeling**, and **Leadership Principles**.\n\n---\n\n## Phase 1: HackerRank SQL Solutions\n\n### Challenge 1: Finding Top 3 Selling Products Per Category\n**Concept:** Window Functions (`DENSE_RANK()`)\n**Scenario:** You have a `Sales` table with `product_id`, `category_id`, and `total_sales`. Find the top 3 highest-selling products in each category. If there is a tie, they share the rank.\n\n**Mock Schema:**\n`Products`: product_id, category, sales_amount\n\n**Logic:**\n1.  Partition the data by `category`.\n2.  Order by `sales_amount` in descending order.\n3.  Use `DENSE_RANK()` so that ties (e.g., two products with $100 sales) both get rank 1, and the next one gets rank 2.\n\n**Solution (PostgreSQL/MySQL):**\n```sql\nWITH RankedSales AS (\n    SELECT\n        category,\n        product_id,\n        sales_amount,\n        DENSE_RANK() OVER (\n            PARTITION BY category\n            ORDER BY sales_amount DESC\n        ) as rank_val\n    FROM Products\n)\nSELECT\n    category,\n    product_id,\n    sales_amount,\n    rank_val\nFROM RankedSales\nWHERE rank_val <= 3;\n```\n\n---\n\n### Challenge 2: Month-over-Month Sales Growth\n**Concept:** `LAG()` Window Function\n**Scenario:** Calculate the percentage growth in **monthly sales** compared to the previous month.\n\n**Mock Schema:**\n`MonthlySales`: sales_month (Date), revenue (Decimal)\n\n**Solution:**\n```sql\nWITH MonthlyStats AS (\n    SELECT\n        sales_month,\n        revenue,\n        -- Get the revenue from the previous row (Partition not strictly needed if only one timeline)\n        LAG(revenue) OVER (ORDER BY sales_month) as prev_month_revenue\n    FROM MonthlySales\n)\nSELECT\n    sales_month,\n    revenue,\n    prev_month_revenue,\n    CASE\n        WHEN prev_month_revenue IS NULL THEN 0 -- First month has no growth\n        ELSE ROUND(((revenue - prev_month_revenue) / prev_month_revenue) * 100, 2)\n    END as growth_percentage\nFROM MonthlyStats;\n```\n\n---\n\n### Challenge 3: Customers with 2nd Purchase within 7 Days\n**Concept:** Self-Join or `LEAD()` + Date Math\n**Scenario:** Identify `customer_id`s where the user made a second purchase within 7 days of their **very first** purchase.\n\n**Mock Schema:**\n`Orders`: order_id, customer_id, order_date\n\n**Solution (Using Self-Join - Easier to debug):**\n```sql\nWITH FirstPurchase AS (\n    -- Step 1: Find the first order date for every customer\n    SELECT\n        customer_id,\n        MIN(order_date) as first_order_date\n    FROM Orders\n    GROUP BY customer_id\n),\nSubsequentPurchases AS (\n    -- Step 2: Join back to Orders to find other orders\n    SELECT\n        o.customer_id,\n        o.order_date\n    FROM Orders o\n    JOIN FirstPurchase fp ON o.customer_id = fp.customer_id\n    -- We want orders that are NOT the first one, but are effectively \"Next\"\n    WHERE o.order_date > fp.first_order_date\n)\n-- Step 3: Filter for the 7-day window\nSELECT DISTINCT\n    sp.customer_id\nFROM SubsequentPurchases sp\nJOIN FirstPurchase fp ON sp.customer_id = fp.customer_id\nWHERE DATEDIFF(day, fp.first_order_date, sp.order_date) <= 7;\n-- Note: Syntax for Date Diff varies. Postgre: (sp.order_date - fp.first_order_date) <= 7\n```\n\n---\n\n## Phase 2: Technical Interview - Data Modeling\n\n### Scenario: Design a Schema for \"Prime Video\" Recommendation System\n**Goal:** We need to track **User Watch History** to recommend new movies.\n**Key Requirements:**\n1.  Scalable (Billions of events).\n2.  Support queries like: \"What genre does User X watch most?\" or \"How many users stopped watching Movie Y after 5 minutes?\".\n\n**Proposed Solution: Star Schema (Dimensional Modeling)**\n\n**1. Fact Table: `fact_watch_events`**\n*   This table records the transaction/action. It is very narrow and long.\n*   **Columns:**\n    *   `event_id` (PK, BigInt/UUID)\n    *   `user_key` (FK to dim_user)\n    *   `media_key` (FK to dim_media)\n    *   `device_key` (FK to dim_device)\n    *   `start_timestamp` (DateTime)\n    *   `duration_watched_sec` (Int)\n    *   `event_type` (Enum: 'PLAY', 'PAUSE', 'COMPLETE', 'ABANDON')\n    *   `session_id` (UUID)\n\n**2. Dimension Tables:**\n*   **`dim_user`**:\n    *   `user_key` (PK), `user_original_id`, `subscription_tier` (Prime/Rent), `geo_region`, `age_group`.\n    *   *SCD Type 2:* If a user moves regions, we might track history, but usually Type 1 (overwrite) is fine for recommendations.\n*   **`dim_media`**:\n    *   `media_key` (PK), `title`, `genre` (Important!), `release_year`, `director`, `content_rating`, `duration_total`.\n*   **`dim_date`**:\n    *   Standard date dimension for fast rollups by Week/Month/Holiday.\n\n**Why this design?**\n*   **Performance:** Numerical Keys (Surrogate Keys) in the Fact table make joins faster than UUIDs.\n*   **Analytics:** To find \"Top Genre\", we join `fact_watch_events` -> `dim_media` and `GROUP BY genre`.\n*   **SCD:** We handle changes in User Subscription status in `dim_user` without touching the billions of rows in the Fact table.\n\n---\n\n## Phase 3: Python Coding (Data Manipulation)\n\n**Scenario:** You have a list of raw log strings: `\"2024-01-01 ERROR: Database connection failed\"`.\n**Task:** Parse logs and return a Dictionary counting the frequency of each error type.\n\n**Solution:**\n```python\ndef count_error_types(logs):\n    error_counts = {}\n\n    for log in logs:\n        # Simple splitting strategy\n        # Log format: \"DATE TYPE: MESSAGE\"\n        parts = log.split(\" \")\n\n        # Safety check for malformed lines\n        if len(parts) < 3:\n            continue\n\n        log_level = parts[1].replace(\":\", \"\") # Get \"ERROR\", \"INFO\"\n\n        if log_level == \"ERROR\":\n            # Extract the message (everything after the level)\n            # \"2024-01-01 ERROR: Database connection failed\"\n            # split produced: ['2024-01-01', 'ERROR:', 'Database', 'connection', 'failed']\n            # We want 'Database connection failed' as the unique key, logical? Or just 'Database'?\n            # Usually strict parsing is required. Let's assume the message starts at index 2.\n            message = \" \".join(parts[2:])\n\n            if message in error_counts:\n                error_counts[message] += 1\n            else:\n                error_counts[message] = 1\n\n    return error_counts\n\n# Test\nraw_logs = [\n    \"2024-01-01 ERROR: Connection Timeout\",\n    \"2024-01-01 INFO: Job Started\",\n    \"2024-01-02 ERROR: Connection Timeout\",\n    \"2024-01-02 ERROR: Null Pointer\"\n]\nprint(count_error_types(raw_logs))\n# Output: {'Connection Timeout': 2, 'Null Pointer': 1}\n```\n\n---\n\n## Phase 4: Leadership Principles (The Bar Raiser)\n\n### Principle: **Ownership** / **Dive Deep**\n**Question:** \"Tell me about a time you had to handle a data quality issue under a tight deadline.\"\n\n**STAR Answer:**\n\n**Situation:**\n\"In my previous role, during the Black Friday peak, our Marketing Dashboard\u2014used by the VP of Sales to adjust ad spend hourly\u2014suddenly showed $0 revenue for the EMEA region at 10 AM.\"\n\n**Task:**\n\"I needed to identify the root cause, fix the data pipeline, and backfill the missing data before the noon executive meeting, as incorrect ad spend decisions could cost us ~$50k/hour.\"\n\n**Action:**\n1.  **Ownership (Took Charge):** I immediately flagged the issue to stakeholders (Transparency) so they wouldn't use the bad data.\n2.  **Dive Deep (Debugging):** I traced the lineage back to the source. I bypassed the standard dashboard logs and queried the raw S3 landing zone. I found that the EMEA source system had added a new column to the CSV file, shifting the 'Revenue' column index by one, which broke our rigid ETL parser.\n3.  **Fix:** I wrote a quick 'hotfix' Python script to correctly map the columns for the new format and deployed it manually to unblock the pipeline (bias for action).\n4.  **Long-term:** I scheduled a backfill job for the historical files and, effectively, updated the parser to be header-aware (so column ordering wouldn't matter in the future) to prevent recurrence.\n\n**Result:**\n\"The dashboard was back up by 11:15 AM with 100% accurate data. The VP made the correct ad spend adjustments. Following this, I implemented a 'Schema Drift' alert that would pause the pipeline and notify us *before* ingesting bad files, rather than failing silently.\"\n"}, {"name": "amazon_question.md", "type": "file", "content": "Congratulations on getting the initial assessment! Amazon's Data Engineering (DE) process in India is known for being rigorous but very structured. The online assessment (OA) on HackerRank is the first major hurdle to prove your technical foundation.\n\nHere is a breakdown of what to expect in the OA and how to navigate the rest of the process.\n\n---\n\n## Phase 1: The HackerRank Online Assessment\n\nThe SQL OA typically consists of **2\u20133 questions** and lasts about **60\u201390 minutes**. Amazon's SQL tests don't just check if you know the syntax; they test if you can handle **e-commerce logic** (e.g., calculating monthly active users, identifying top-selling products, or handling overlapping dates).\n\n### Core SQL Topics to Master\n\n* **Joins & Aggregations:** This is the bread and butter. Be extremely comfortable with `LEFT JOIN` vs `INNER JOIN` and using `GROUP BY` with `HAVING`.\n* **Window Functions:** These are **highly likely** to appear. Focus on:\n* `RANK()`, `DENSE_RANK()`, and `ROW_NUMBER()` (e.g., \"Find the top 3 selling products per category\").\n* `LEAD()` and `LAG()` (e.g., \"Calculate the growth in sales compared to the previous month\").\n* `SUM() OVER()` for running totals.\n\n\n* **Date Manipulation:** Amazon loves time-series data. Practice `EXTRACT`, `DATE_TRUNC`, and calculating differences between dates (e.g., \"Find customers who made a second purchase within 7 days of their first\").\n* **CTE (Common Table Expressions):** Use `WITH` clauses to keep your code readable. It helps you think through multi-step logic.\n\n### OA Pro-Tips\n\n1. **Read the Schema Carefully:** Pay attention to **NULL** values and primary keys.\n2. **Edge Cases:** Always consider what happens if there are no sales for a month or if two products have the same rank.\n3. **HackerRank Environment:** You can choose your SQL dialect (usually MySQL or PostgreSQL). Stick to the one you know best.\n\n---\n\n## Phase 2: The Full Interview Loop\n\nIf you pass the OA, the process usually moves to a **Technical Phone Screen** followed by a **Virtual Onsite (5 rounds)**.\n\n### 1. Technical Pillars\n\n* **Data Modeling:** You will likely be asked to design a schema for an Amazon-specific scenario (e.g., \"Design a schema for a Prime Video recommendation system\"). Focus on **Star Schema**, **Normalization vs. Denormalization**, and **SCD (Slowly Changing Dimensions)**.\n* **ETL/Pipeline Design:** Explain how you move data from source to warehouse. Be ready to discuss **Batch vs. Streaming**, data quality checks, and handling failures (idempotency).\n* **Coding (Python):** Usually one round involves a Python coding challenge, often focusing on data manipulation (like using Dictionaries or Lists) rather than complex LeetCode-style algorithms.\n\n### 2. The \"Leadership Principles\" (The Bar Raiser)\n\nAmazon is unique because **50% of your evaluation** is based on their **16 Leadership Principles (LPs)**.\n\n* **Ownership** and **Dive Deep** are critical for Data Engineers.\n* Prepare **2\u20133 stories per principle** using the **STAR Method** (Situation, Task, Action, Result).\n* *Example Question:* \"Tell me about a time you had to handle a data quality issue under a tight deadline.\"\n\n---\n\n## Preparation Roadmap\n\n| Week | Focus Area | Recommended Resources |\n| --- | --- | --- |\n| **Days 1-3** | **SQL Mastery** | LeetCode (Medium/Hard SQL), DataLemur (Amazon-specific) |\n| **Days 4-5** | **Data Modeling** | \"The Data Warehouse Toolkit\" (Kimball basics), System Design videos |\n| **Days 6-7** | **Leadership Principles** | Review Amazon's LP page; write down your STAR stories |\n\n**Would you like me to generate a few \"Amazon-style\" SQL practice questions for you to solve right now?**"}, {"name": "gartner_answers.md", "type": "file", "content": "# Gartner Associate Director - Platform Engineering (Azure + Databricks)\n## Model Interview Answers\n\n> **Note to Candidate:** These answers are framed for an **Associate Director** level. They focus less on \"how to write code\" and more on **governance, scalability, cost management, standard patterns, and risk mitigation**.\n\n---\n\n## 1. Platform Architecture & Azure Core Services\n\n### 1. How would you design a **scalable Azure data platform** using Databricks, ADF, and Azure Storage for both batch and near-real-time workloads?\n**STAR / Architectural Answer:**\n\"I adhere to the **Medallion Architecture (Bronze/Silver/Gold)** principles, typically implementing a **Lakehouse** pattern.\n*   **Ingestion:** I use **ADF** specifically for lightweight orchestration and connector-based ingestion (e.g., from on-prem SQL or SaaS APIs) into the **Bronze** layer (Azure Data Lake Gen2) in its raw format. For near-real-time, I prefer **Event Hubs** coupled with **Spark Structured Streaming** in Databricks directly into Delta tables.\n*   **Processing:** All transformation logic resides in **Databricks** (using Delta Live Tables or distinct jobs) to promote better CI/CD and unit testing compared to rigid ADF Data Flows.\n*   **Storage:** ADLS Gen2 is the single source of truth. I enforce lifecycle management policies to move cold data to Cool/Archive tiers to optimize costs.\n*   **Serving:** Gold data is exposed via **Unity Catalog** for governance, and usually synced to a **Serverless SQL Endpoint** for BI tools like Power BI, ensuring we don't lock concurrency on the engineering clusters.\"\n\n### 2. When would you choose **ADF vs Azure Functions vs Databricks Jobs** in a data pipeline?\n**Strategic Answer:**\n\"I follow a 'Right Tool for the Job' policy to avoid technical debt:\n*   **ADF:** Best for **orchestration** and **data movement** (Copy Activity) where no complex transformation is needed. It\u2019s low-code and has excellent connectors. I avoid using ADF Data Flows for complex logic as they are harder to version control and debug than code.\n*   **Azure Functions:** Ideal for **event-driven, lightweight integration tasks** (e.g., triggering a pipeline when a file lands, calling a REST API to get a token, or simple parsing). If processing takes >5-10 mins or requires heavy compute, it\u2019s the wrong tool.\n*   **Databricks Jobs:** The standard for **heavy data transformation (ETL/ELT)**. Any logic involving complex joins, aggregations, or ML inference belongs here. It allows for proper software engineering practices (testing, modularization) that ADF and Functions struggle with at scale.\"\n\n### 3. Explain how you\u2019ve used **AKS** in data platforms. What workloads ran on it and why?\n**Answer:**\n\"While Databricks allows model serving, **AKS (Azure Kubernetes Service)** is often more cost-effective and flexible for **high-concurrency, low-latency API serving** of ML models or custom microservices.\nIn my previous platform, we treated Databricks as the *training* engine. Once a model was registered in MLflow, a CI/CD pipeline containerized it and deployed it to AKS. This separated the **batch training compute** (Databricks) from the **24/7 inference compute** (AKS), allowing us to scale them independently using KEDA scalers based on request queue depth.\"\n\n### 4. How do you architect **multi-environment setups (dev / test / prod)** in Azure for data platforms?\n**Answer:**\n\"I strictly separate environments at the **Resource Group** or **Subscription** level to prevent cross-contamination.\n*   **Infrastructure:** Provisioned via **Terraform**. This ensures Dev, Test, and Prod are identical in configuration (drift detection).\n*   **Data:** Production data is **never** copied to Dev. We use synthetic data or strictly anonymized subsets for lower environments.\n*   **Databricks:** Separate Workspaces per environment. Code is promoted via **Databricks Asset Bundles (DABs)** or Git integration.\n*   **ADF:** We use the 'ADF utilities' npm package in our release pipeline to deploy ARM templates from the collaboration branch (Dev) to higher environments, overriding parameters (like KeyVault URLs) for each stage.\"\n\n### 5. How do you manage **cost optimization** across Databricks clusters, ADF pipelines, and storage?\n**Answer:**\n\"Cost governance is a proactive discipline, not reactive:\n1.  **Databricks:** I enforce **Cluster Policies** to restrict max DBUs and require tagging (CostCenter, Project). I mandate **Job Clusters** for all automated workloads (much cheaper than All-Purpose) and use **Photon** only where the speedup justifies the premium. Spot instances are used for stateless, robust retryable jobs.\n2.  **ADF:** I monitor Self-Hosted Integration Runtime (SHIR) node utilization to avoid over-provisioning.\n3.  **Storage:** Lifecycle policies are non-negotiable. I also regularly review 'Unmanaged' Delta files using `VACUUM` commands to remove stale snapshots that bloat storage costs.\n4.  **Monitoring:** I set up Azure Budget alerts at 50%, 75%, and 100% thresholds, sending notifications to the engineering leads.\"\n\n### 6. What are common **failure points in Azure data platforms**, and how do you proactively monitor them?\n**Answer:**\n\"Common failures include **Throttling** (Storage Account limits), **Spot Instance Evictions** in Databricks, and **SHIR connectivity** issues.\n*   **Mitigation:** I design for retry-ability. ADF pipelines use exponential backoff policies.\n*   **Monitoring:** I don't rely solely on 'Pipeline Failed' emails. I implement **Azure Monitor / Log Analytics** dashboards that track 'Data Freshness' (SLA breaches). If a critical table hasn't updated by 7 AM, PagerDuty is triggered, regardless of whether the specific job failed or simply hung.\"\n\n---\n\n## 2. Databricks Platform Engineering\n\n### 1. How do you manage **Databricks workspace governance** across multiple teams?\n**Answer:**\n\"I treat the Platform as a Product. I use **Unity Catalog (UC)** as the central governance layer.\n*   **Metastore:** One Metastore per region, attached to all workspaces (Dev/Test/Prod).\n*   **Catalogs:** Segregated by Business Unit or Domain (e.g., `Finance_Catalog`, `Marketing_Catalog`).\n*   **Access Control:** We do NOT assign permissions to users directly. All access is via **Azure AD (Entra ID) Groups**.\n*   **Cluster Policies:** I define 'T-Shirt size' policies (Small, Medium, Large) effectively preventing users from spinning up massive GPU clusters without approval.\"\n\n### 2. Explain your approach to **cluster policies, job clusters vs interactive clusters**.\n**Answer:**\n\"This is critical for cost control.\n*   **Policies:** I implement a 'Personal Compute' policy for interactive analysis that restricts users to Single Node clusters or very small autoscaling ranges to prevent runaway costs.\n*   **Job vs. Interactive:** Interactive (All-Purpose) clusters are *only* for development and debugging. **100% of production workloads must run on Job Clusters**. Job clusters are ephemeral, isolated, and significantly cheaper (approx. 40-50% less). I enforce this via CI/CD; the deployment pipeline will reject any job pointing to an existing interactive cluster ID.\"\n\n### 3. How do you handle **Databricks runtime upgrades** without breaking pipelines?\n**Answer:**\n\"We decouple the Runtime version from the job definition where possible, but realistically, upgrades require testing.\n1.  **LTS Policy:** We stick to **LTS (Long Term Support)** versions (e.g., 15.4 LTS) and only upgrade when the next LTS is stable.\n2.  **Canary Testing:** When a new runtime is selected, we run a subset of non-critical 'Canary' pipelines on the new version in Non-Prod for a week.\n3.  **Regression Suite:** We run our standard unit and integration tests.\n4.  **Rollout:** We update the standard 'Job Cluster Policy' to the new version, forcing new runs to pick it up, while keeping a 'Legacy' policy available for immediate rollback if edge cases appear.\"\n\n### 4. How do you implement **Unity Catalog or equivalent governance** in Databricks?\n**Answer:**\n\"Migration to Unity Catalog is a priority for modern platforms.\n*   **Identity:** SCIM integration with Azure Active Directory.\n*   **Data Lineage:** UC provides this out-of-the-box. I ensure all jobs leverage UC-enabled clusters so we capture column-level lineage.\n*   **External Locations:** We stop using access keys or SAS tokens in code. We set up **Storage Credentials** and **External Locations** in UC, granting access to specific Service Principals. This removes all secrets from notebooks.\"\n\n### 5. How do you manage **secrets, service principals, and credential passthrough**?\n**Answer:**\n\"Hardcoded credentials are a firing offense.\n*   **Azure Key Vault (AKV):** All secrets live here.\n*   **Databricks Secret Scopes:** We create Key Vault-backed secret scopes.\n*   **Service Principals:** Pipelines run as Service Principals, not users.\n*   **Credential Passthrough:** We are moving *away* from this in favor of **Unity Catalog** standard authentication, which is more secure and works better with SQL Endpoints and non-interactive jobs.\"\n\n### 6. What performance tuning techniques have you applied in Databricks for large datasets?\n**Answer:**\n\"Performance tuning is an iterative process. My checklist:\n1.  **File Sizing:** The 'Small File Problem' is a killer. I use `OPTIMIZE` and `Z-ORDER` (on high-cardinality filter columns) regularly.\n2.  **Shuffle Partitions:** Default is 200, which is often wrong. I use **Adaptive Query Execution (AQE)** which handles this dynamically in most modern runtimes.\n3.  **Broadcasting:** For joins, I ensure small lookup tables are Broadcasted to avoid shuffling large fact tables.\n4.  **Caching:** Disk Caching (formerly Delta Cache) is enabled on worker nodes for repeated reads.\n5.  **Spill to Disk:** If I see heavy spill in the Spark UI, I upgrade leverage memory-optimized instances.\"\n\n### 7. How do you ensure **platform reliability and SLAs** for Databricks users?\n**Answer:**\n\"Reliability is improved through isolation and automation.\n*   **Quotas:** We monitor Azure vCPU quotas to ensure we don't hit region limits during peak scale-up.\n*   **Pools:** For latency-sensitive jobs, I use **Instance Pools** to reduce cluster startup time (though Serverless is making this less relevant).\n*   **SLA Tracking:** We tag jobs with 'Tier-1', 'Tier-2'. Tier-1 failures trigger immediate PagerDuty alerts. We measure 'Time to Availability' and report it weekly.\"\n\n---\n\n## 3. Azure Data Factory (ADF) \u2013 Advanced Scenarios\n\n### 1. How do you design **metadata-driven ADF pipelines**?\n**Answer:**\n\"hardcoding pipelines for every table is unscalable.\nI design a **Framework** approach:\n*   **Control Table:** A SQL table or JSON config listing TableName, SourceQuery, DestinationPath, WatermarkColumn, and IsActive flag.\n*   **Master Pipeline:** A generic pipeline with a `Lookup` (get list of tables) -> `ForEach` loop -> `Execute Pipeline` (Child).\n*   **Child Pipeline:** Takes parameters (Source, Sink) and executes the Copy Activity.\n*   This way, onboarding a new dataset is just an `INSERT` into the Control Table, not a code deployment.\"\n\n### 2. How do you manage **ADF connector upgrades** and breaking changes?\n**Answer:**\n\"AWS or SaaS API versions change.\n*   We abstract source specifics into **Linked Services**.\n*   If a connector is deprecated (e.g., a specific Salesforce API version), we update the Linked Service or the Dataset definition.\n*   Because we use a metadata-driven framework, we can often just update the 'Source Query' in our control table to adapt to schema changes without redeploying the ADF pipeline artifacts themselves.\"\n\n### 3. How do you handle **ADF performance bottlenecks** at scale?\n**Answer:**\n\"ADF bottlenecks usually happen in two places:\n1.  **Control Flow Limits:** ADF has a limit of 40 parallel activities in a loop. I use **Batching** (processing groups of 20 tables) to work around this.\n2.  **Copy Throughput:** If the Copy Activity is slow, I check the **Integration Runtime (IR)**. If it's a Self-Hosted IR, is the CPU maxed? If so, we scale out (add nodes) or scale up. If it's Azure IR, I increase the **DIUs (Data Integration Units)** explicitly for that activity.\"\n\n### 4. Explain your strategy for **ADF CI/CD across environments**.\n**Answer:**\n\"I adhere to the **npm-based deployment** standard recommended by Microsoft.\n*   **Dev:** Developers work in their own Git branches. Debug runs happen here.\n*   **Build:** When merging to the collaboration branch (e.g., `main`), an Azure DevOps pipeline runs the NPM build command to generate the **ARM Templates**.\n*   **Release:** The release pipeline deploys these ARM templates to Test and Prod.\n*   **Parameters:** We heavily use **Global Parameters** and KeyVault references to ensure environment-specific values (like connection strings) are injected at deployment time.\"\n\n### 5. What\u2019s your approach to **self-hosted integration runtime** vs managed IR?\n**Answer:**\n\"Security dictates this choice.\n*   **Azure IR (Managed):** Default choice for Cloud-to-Cloud copy (e.g., Blob to SQL DB). It's serverless and scales automatically.\n*   **Self-Hosted IR:** Mandatory for **On-Premise** data sources or VNet-protected resources that Azure IR cannot reach. I deploy these on dedicated VMs close to the data source. I always deploy at least **two nodes** for High Availability (HA) so that patching one node doesn't stop ingestion.\"\n\n---\n\n## 4. CI/CD, DevOps & Infrastructure as Code\n\n### 1. How would you design a **CI/CD pipeline for Databricks + ADF** from scratch?\n**Answer:**\n\"I design pipelines with **Separation of Concerns**:\n*   **Build Pipeline (CI):** Triggered on PR merge. Runs automated linters (sqlfluff, pylint), unit tests (pytest for Databricks transforms), and builds artifacts (ARM templates for ADF, Wheels/DABs for Databricks). It publishes these artifacts to the Artifact feed.\n*   **Release Pipeline (CD):** Triggered on artifact publication. It pulls the artifacts and deploys them to Dev, then (after approval) Test, then Prod.\n*   **Infrastructure:** Terraform \\\u0007pply\\ usually runs in a separate pipeline or stage before the code deployment to ensure the target infrastructure (storage containers, key vaults) exists.\"\n\n### 2. What artifacts do you version control for data platforms?\n**Answer:**\n\"Everything is code.\n*   **Infrastructure:** Terraform (\\.tf\\) files.\n*   **ADF:** The usage of \\ARMTemplateForFactory.json\\ is standard for deployment, but we check in the individual JSON files for collaboration.\n*   **Databricks:** Notebooks (py, sql), Wheel setup files (\\setup.py\\), and Cluster configurations (JSON).\n*   **Configuration:** We do *not* version control secrets, but we do version control parameter files (e.g., \\dev.params.json\\) that reference those secrets.\"\n\n### 3. How have you used **Terraform** to provision Azure + Databricks infrastructure?\n**Answer:**\n\"I use the \\\u0007zurerm\\ and \\databricks\\ providers.\nI module-ize common patterns:\n*   **Base Module:** Creates VNet, Subnets, Network Security Groups, and Storage Accounts (with Private Endpoints).\n*   **Databricks Module:** Deploys the Workspace (VNet Injected), sets up Unity Catalog Metastore assignment, and provisions standard Cluster Policies and User Groups.\n*   **State Management:** Remote state is stored in an encrypted Azure Storage container with a locking policy.\"\n\n### 4. Compare **Jenkins vs GitHub Actions vs Azure DevOps** for data engineering CI/CD.\n**Answer:**\n\"I have used all three, but for an Azure shop, **Azure DevOps (ADO)** is the natural fit due to its seamless integration with AD permissions and Boards.\nHowever, **GitHub Actions** is rapidly becoming the preference for modern teams due to its proximity to the code and marketplace of actions.\n**Jenkins** is powerful but requires maintenance of the Jenkins server itself (patching, upgrades), which is toil I try to avoid. I prefer SaaS CI/CD (ADO/GHA) to reduce operational overhead.\"\n\n### 5. How do you manage **secrets and credentials** in CI/CD pipelines?\n**Answer:**\n\"Zero-trust approach.\n*   Pipelines run as Service Principals with federated credentials (OIDC) where possible to avoid managing even Secret Client Secrets.\n*   Deployment tasks retrieve secrets from **Azure Key Vault** using specific steps (e.g., \\AzureKeyVault@2\\ in ADO) and map them to environment variables.\n*   We use **Variable Groups** linked to Key Vaults, so the pipeline logs never show the actual value, only \\***\\.\"\n\n### 6. How do you enforce **quality gates (tests, linting, approvals)** for data pipelines?\n**Answer:**\n\"Quality gates are automated steps that block deployment.\n1.  **Linting:** \\Black\\ and \\Ruff\\ for Python, \\sqlfluff\\ for SQL. Failed linting fails the build.\n2.  **Unit Tests:** \\pytest\\ for internal logic libraries.\n3.  **Integration Tests:** Post-deploy smoke tests in the 'Test' environment that run a sample pipeline and check if data lands.\n4.  **Approvals:** Deployment to Production strictly requires manual approval from a Tech Lead in the Release pipeline.\"\n\n### 7. How do you roll back a failed production deployment?\n**Answer:**\n\"We consistently use **State-Based Deployment**.\nTo rollback, we essentially 'roll forward' to the previous state.\n*   **ADF:** We redeploy the ARM template from the *previous* successful release.\n*   **Databricks:** Since we use versioned libraries (Wheels), we update the job definition to point back to the previous version (e.g., \\\u000b1.2.0\\ instead of \\\u000b1.2.1\\).\n*   **Database:** This is harder. If a bad migration ran, we rely on Delta Lake's \\RESTORE\\ command to Time Travel back to the pre-deployment version ID.\"\n\n---\n\n## 5. Disaster Recovery (DR) & High Availability\n\n### 1. How do you design **DR for Databricks, ADF, and Storage**?\n**Answer:**\n\"I implement an **Active-Passive** DR strategy for cost efficiency, with a 'Pilot Light' in the secondary region.\n*   **Storage:** GRS (Geo-Redundant Storage) is the foundation. Data replicates asynchronously to the paired region.\n*   **Code:** All code is in Git, so it's region-agnostic.\n*   **ADF & Databricks:** We use Terraform to define these resources. In a DR event, we run a 'DR Deployment' pipeline that spins up the workspaces and factories in the secondary region (if not already there as a pilot light) and points them to the secondary storage endpoint.\"\n\n### 2. What is your **RPO/RTO strategy** for data platforms?\n**Answer:**\n\"These metrics must be defined by Business continuity needs, not just IT.\n*   **RPO (Recovery Point Objective):** Determined by storage replication lag (GRS is usually < 15 mins).\n*   **RTO (Recovery Time Objective):** Determined by automation. If we have a 'Warm Standby' (workspaces pre-provisioned), RTO is ~30 mins. If 'Cold Standby' (provision on demand), RTO can be 4-6 hours. I typically aim for RPO < 1 hour and RTO < 4 hours for critical reporting.\"\n\n### 3. How do you handle **cross-region replication** for data lakes?\n**Answer:**\n\"For Bronze/Silver/Gold data, I rely on Azure's native **GRS/RA-GRS**.\nHowever, for Delta Tables, GRS replication ends up being 'Crash Consistent' but not always 'Application Consistent' due to the transaction log vs parquet file sync.\nFor *critical* tables, I prefer **Deep Clone** jobs that run nightly to copy the Delta table to the DR region explicitly. This ensures a transactionally consistent copy.\"\n\n### 4. What components are hardest to recover in a data platform and why?\n**Answer:**\n\"**Stateful** components are the hardest.\n*   **The Metastore:** If using Hive Metastore (legacy), syncing the backing DB is painful. Unity Catalog simplifies this as it's a global account-level construct.\n*   **In-flight Processing:** Resuming a streaming job exactly where it left off in a new region requires careful management of checkpoint locations (which must also be replicated).\"\n\n### 5. Have you executed a **real DR drill**? What did you learn?\n**Answer:**\n\"Yes, we conduct bi-annual 'Game Days'.\nA key learning was that **Secrets** were a single point of failure. We had Key Vaults replicating, but the RBAC permissions didn't carry over automatically. We had to update our Terraform to ensure the Service Principals had permission on the Secondary Key Vaults ahead of time.\"\n\n---\n\n## 6. Security, IAM & Compliance\n\n### 1. How do you integrate data platforms with **Okta / Active Directory**?\n**Answer:**\n\"We use **SCIM (System for Cross-domain Identity Management)** provisioning.\n*   Users and Groups are managed in Azure AD (Entra ID).\n*   Databricks and ADF are configured to sync these groups automatically.\n*   We never create local users. If a user leaves the company and is disabled in AD, they lose access to the platform immediately.\"\n\n### 2. Explain **RBAC vs ABAC** in Azure data platforms.\n**Answer:**\n\"**RBAC (Role-Based)** is our bread and butter: 'Finance Team' group gets 'Read' on 'Finance Folder'.\n**ABAC (Attribute-Based)** is the next level. We use it for fine-grained control. For example, tagging a dataset with \\Confidentiality=High\\ and having a policy that says 'Only users with \\Clearance=High\\ attribute can access resources with \\Confidentiality=High\\'. This scales better than managing thousands of individual role assignments.\"\n\n### 3. How do you manage **least-privilege access** for data engineers vs analysts?\n**Answer:**\n\"Engineers differ from Analysts.\n*   **Engineers:** access to 'Dev' and 'Test' with Write permissions. **Read-Only** in Prod. They can only deploy to Prod via CI/CD.\n*   **Analysts:** **Read-Only** access on 'Gold' tables in Prod via SQL Endpoints. No access to Bronze/Silver storage accounts directly. No ability to create clusters.\"\n\n### 4. How do you secure **PII / sensitive data** in Databricks?\n**Answer:**\n\"Layered defense:\n*   **Discovery:** Use the \\SENSITIVE_DATA\\ tag in Unity Catalog.\n*   **Masking:** I use **Dynamic View functions** (e.g., \\CASE WHEN is_member('HR_Group') THEN email ELSE '***' END\\).\n*   **Encryption:** Customer-Managed Keys (CMK) for the storage account if required by regulation.\n*   **Audit:** Strictly monitoring the audit logs for who queried these columns.\"\n\n### 5. What compliance frameworks have you worked with (SOC2, ISO, GDPR)?\n**Answer:**\n\"I've designed platforms for **GDPR** compliance.\nThe biggest challenge is the **Right to be Forgotten**.\nWe implemented a 'Tombstone' pattern in our Data Lake. When a deletion request comes in, we upsert a record with \\DeleteFlag=True\\. A weekly maintenance job then physically rewrites the Delta files to purge the record definitively to satisfy the 30-day requirement.\"\n\n### 6. How do you audit and monitor **data access and activity logs**?\n**Answer:**\n\"I enable **Diagnostic Settings** on all Azure resources (ADF, Databricks, Storage) to ship logs to a centralized **Log Analytics Workspace**.\nI build Kusto (KQL) queries to alert on anomalies, such as:\n*   A user downloading > 1GB of data.\n*   Access from an unknown IP address (though Private Link mostly prevents this).\n*   Failed login attempts.\"\n\n---\n\n## 7. Monitoring, Observability & Reliability\n\n### 1. How do you use **Azure Log Analytics** for platform monitoring?\n**Answer:**\n\"Log Analytics is my single pane of glass.\n*   I configure **Diagnostic Settings** on all ADF, Databricks, and Logic App resources to sink logs there.\n*   I build custom **Workbooks** on top of it.\n*   Example Query: I track 'Duration of Pipeline X over time' to spot performance degradation before it becomes an SLA breach.\"\n\n### 2. What KPIs do you track for **data platform health**?\n**Answer:**\n\"I track Operational and Business KPIs:\n1.  **Pipeline Reliability:** % of successful runs vs failures (Target > 99.9%).\n2.  **Data Freshness:** Delay between Source Time and Availability in Gold Layer.\n3.  **Cost:** Daily burn rate vs Budget.\n4.  **TTR (Time to Remediation):** How fast we fix broken pipelines.\"\n\n### 3. How do you detect and resolve **data quality issues early**?\n**Answer:**\n\"I believe in **Shift Left** for data quality.\n*   **Schema Validation:** Enforced at the Bronze layer (Delta Schema Enforcement).\n*   **Contract Tests:** We use **Great Expectations** or **dbt tests** in the pipeline. If a column has > 5% nulls where it shouldn't, the pipeline acts: it either fails (Stop the Line) or quarantines the bad records to an 'Error Table' and proceeds with the good data, sending an alert to the Data Stewards.\"\n\n### 4. How do you implement **alerting without alert fatigue**?\n**Answer:**\n\"Alert fatigue kills responsiveness.\n*   **Grouping:** We group alerts. Instead of 100 emails for 100 failed files, we send 1 digest.\n*   **Routing:** Warning alerts go to a Slack channel (\\#data-alerts-warning\\). Only Critical alerts (SLA Breach, Prod Down) trigger **PagerDuty** to call the On-Call engineer.\"\n\n### 5. Describe a major production incident you handled end-to-end.\n**Answer:**\n*(Example)* \"We had a 'Storage Throttling' incident.\n*   **Symptom:** All ADF pipelines started failing with 503 errors.\n*   **Diagnosis:** Metrics showed we hit the IOPS limit on the storage account because a new 'Backfill' job was running in parallel with BAU loads.\n*   **Fix:** Immediately paused the Backfill.\n*   **Root Cause:** We were using a single Storage Account for Bronze, Silver, and Gold.\n*   **Long Term Fix:** We sharded the data across multiple storage accounts and implemented 'Throughput constraints' on our backfill jobs.\"\n\n---\n\n## 8. Leadership, Coaching & Team Management\n\n### 1. How do you balance **hands-on technical work vs leadership**?\n**Answer:**\n\"In an Associate Director role, I expect a **20/80 split** (20% Hands-on, 80% Strategy/Management).\nI stay hands-on by:\n*   Conducting **Code Reviews**.\n*   Writing **RFCs (Request for Comments)** and Architecture Design Docs.\n*   Prototyping 'Spikes' for new tech (e.g., trying out a new Databricks feature) to evaluate if the team should adopt it.\nI do *not* write critical path production code that would block the team if I\ufffdm in meetings.\"\n\n### 2. How do you mentor junior and mid-level engineers?\n**Answer:**\n\"I adhere to the 'See one, Do one, Teach one' model.\n*   **Pair Programming:** I actively pair on complex problems, not to drive, but to navigate.\n*   **Design Reviews:** I force them to write a design doc before coding. I critique the *design*, not just the syntax.\n*   **Career Path:** I have monthly 1:1s focused solely on career growth (not status updates), mapping their work to the promotion rubric.\"\n\n### 3. How do you handle **underperforming team members**?\n**Answer:**\n\"Empathy first, then accountability.\n1.  **Diagnose:** Is it a skill gap? Personal issue? Or lack of clarity?\n2.  **Clear Expectations:** I set specific, measurable, short-term goals (Micro-goals).\n3.  **Support:** I provide the resources/coaching needed to hit those goals.\n4.  **Action:** If they consistently miss them despite support, I initiate a formal Performance Improvement Plan (PIP). Protecting the team's velocity and morale is paramount.\"\n\n### 4. How do you prioritize platform work vs feature delivery?\n**Answer:**\n\"This is the eternal struggle. I use a **'Tax' model**.\nI negotiate with Product Management to reserve **20% of every sprint capacity** for 'Platform Engineering & Tech Debt' (The Tax).\n*   Features get 80%.\n*   We use that 20% to upgrade runtimes, refactor modules, or improve monitoring. This prevents the 'Big Bang Rewrite' scenario down the road.\"\n\n### 5. Describe a time you had to **push back on stakeholders**.\n**Answer:**\n\"A stakeholder wanted 'Real-Time' 1-second latency for a financial report that was only reviewed weekly.\nI explained the **Cost vs. Value**.\n'Real-time will cost /month in compute. Moving to a 1-hour refresh will cost /month. Is the 1-second latency worth .5k/month to the business?'\nThey immediately agreed to the 1-hour refresh. It's about framing technical constraints in business terms.\"\n\n### 6. How do you build a culture of **engineering excellence and ownership**?\n**Answer:**\n\"I treat operations as a software problem.\n*   **You Build It, You Run It:** The team that writes the pipeline is on-call for it. This incentivizes them to write robust, error-free code because nobody wants to be woken up at 3 AM.\n*   **Post-Mortems:** We have blameless post-mortems for every incident. The goal is 'How do we prevent this class of error?' not 'Who caused it?'.\"\n\n### 7. How do you assess technical debt in data platforms?\n**Answer:**\n\"I look for **Cognitive Load**.\nIf onboarding a new engineer takes 3 months because the code is spaghetti, debt is high.\nIf adding a new column requires changing code in 5 different places, debt is high.\nI maintain a 'Tech Debt Radar' on our board and prioritize items based on 'Interest Rate' (how much is this slowing us down daily?).\"\n\n---\n\n## 9. Cross-Functional & Stakeholder Collaboration\n\n### 1. How do you translate **business requirements** into platform capabilities?\n**Answer:**\n\"I start with the **consumption use case**.\n'Who needs this data? usage frequency? latency requirement?'\nIf Marketing needs 'Customer Segmentation', I translate that to: 'We need an identity resolution pipeline (Databricks), a curating Gold table (Delta), and a secure serving layer (SQL Endpoint) connected to Tableau'.\"\n\n### 2. How do you work with **Data Science teams** using Databricks?\n**Answer:**\n\"Data Engineering lays the pavement; Data Science drives the cars.\n*   **Collaboration:** We provide them with stable, quality-assured 'Feature Stores' in the Silver/Gold layer.\n*   **MLOps:** We help them containerize their models. We don't write the model, but we build the CI/CD pipeline that deploys it. We ensure their notebooks utilize standard clusters to avoid cost overruns.\"\n\n### 3. How do you handle conflicting priorities between Product, IT, and Analytics?\n**Answer:**\n\"I host a bi-weekly **'Data Council' or Steering Committee**.\nWe review the backlog together. If Product wants Feature A and IT wants Security Patch B, we visualize the impact.\n'If we skip Security Patch B, we risk a compliance fine. If we skip Feature A, we miss Q3 goals.'\nI guide the decision, but I make the trade-offs explicit and documented.\"\n\n### 4. How do you communicate platform outages or risks to leadership?\n**Answer:**\n\"**Radical Transparency.**\nI send a 'Status Note' immediately upon confirmation of a major outage.\n*   **What happened:** (High level)\n*   **Impact:** (Business terms: 'Billing report is delayed')\n*   **ETA:** (Best guess)\n*   **Next Update:** (e.g., in 1 hour).\nLeadership fears the unknown. Frequent, clear updates calm the nerves, even if the news is bad.\"\n\n### 5. How do you justify platform investments to non-technical stakeholders?\n**Answer:**\n\"I focus on **Risk and Speed**.\n'Invest in this CI/CD automation now ( effort), and we will reduce our deployment time from 2 days to 2 hours forever. This allows us to ship features to you faster.'\nOR\n'Invest in this DR setup, or risk losing 1 week of revenue () if East US goes down.'\nROI calculations win arguments.\"\n\n---\n\n## 10. Behavioral & Scenario-Based Questions\n\n### 1. Describe a time you **modernized a legacy data platform**.\n**Answer:**\n*(STAR Method)*\n*   **Situation:** We had a legacy on-prem Hadoop cluster that was failing SLAs and costing /yr in maintenance.\n*   **Task:** Migrate to Azure Databricks with zero downtime for consumers.\n*   **Action:** I designed a 'Strangler Fig' migration. We dual-ingested data into Azure. We moved consumption views one by one (Finance first, then Marketing) to point to the new Silver/Gold tables. I trained the team on Spark.\n*   **Result:** Reduced costs by 40%, improved SLA from 24h to 1h, and retired the Hadoop nodes 3 months early.\n\n### 2. Tell me about a **failed platform initiative** and what you learned.\n**Answer:**\n*   **Situation:** I tried to enforce a strict 'One Size Fits All' ingestion framework using a complex custom Python library I wrote.\n*   **Action:** I rolled it out to all teams.\n*   **Result:** Teams rebelled because it lacked flexibility for edge cases. Adoption stalled.\n*   **Learning:** I learned that **User Adoption** is more important than Architectural Purity. Now, I treat the Platform as a Product\ufffdI interview the 'customers' (engineers) first, build an MVP, and iterate based on their feedback.\n\n### 3. How do you decide whether to **build vs buy** platform capabilities?\n**Answer:**\n\"I use the **'Core vs. Context'** framework.\n*   **Context (Commodity):** If it's a solved problem (e.g., orchestration, lineage, secrets), I **Buy** (use Azure Data Factory, Unity Catalog, KeyVault). Building a custom orchestrator is a waste of resources.\n*   **Core (Differentiator):** If it gives us a competitive advantage (e.g., a proprietary pricing algorithm), we **Build** that logic in Databricks.\n*   **Rule:** meaningful engineering hours should focus on business logic, not plumbing.\"\n\n### 4. How do you stay updated with **new Azure & Databricks features**?\n**Answer:**\n\"I read the **Azure Updates** RSS feed and the **Databricks Engineering Blog**.\nI attend the Data + AI Summit (virtually or in-person).\nI encourage my team to do 'Tech Radar' sessions where we pick one new feature (e.g., Databricks Shield) and do a 1-day spike to see if it solves a current pain point.\"\n\n### 5. Why Gartner, and why this **Associate Director** role specifically?\n**Answer:**\n\"I\ufffdve spent years building data platforms, and I've always admired Gartner's ability to define the 'Magic Quadrant' and set the standard for the industry.\nI want to bring my **practical, in-the-trenches experience** of what actually works (and what doesn't) to Gartner's internal platforms.\nI love the mix of strategy and technical leadership this Associate Director role offers\ufffdit allows me to scale my impact by building a high-performing team and a robust, modern platform.\"\n\n---\n"}, {"name": "gartner_question.md", "type": "file", "content": "Below are **role-specific, realistic interview questions** Gartner is likely to ask for this **Associate Director \u2013 Platform Engineering (Azure + Databricks)** position. I\u2019ve grouped them the way Gartner typically structures senior technical + leadership interviews.\n\n---\n\n## 1. Platform Architecture & Azure Core Services\n\n**(Expect deep \u201chow & why\u201d, not definitions)**\n\n1. How would you design a **scalable Azure data platform** using Databricks, ADF, and Azure Storage for both batch and near-real-time workloads?\n2. When would you choose **ADF vs Azure Functions vs Databricks Jobs** in a data pipeline?\n3. Explain how you\u2019ve used **AKS** in data platforms. What workloads ran on it and why?\n4. How do you architect **multi-environment setups (dev / test / prod)** in Azure for data platforms?\n5. How do you manage **cost optimization** across Databricks clusters, ADF pipelines, and storage?\n6. What are common **failure points in Azure data platforms**, and how do you proactively monitor them?\n\n---\n\n## 2. Databricks Platform Engineering (Very Important)\n\n**They\u2019ll test governance, not just Spark coding**\n\n1. How do you manage **Databricks workspace governance** across multiple teams?\n2. Explain your approach to **cluster policies, job clusters vs interactive clusters**.\n3. How do you handle **Databricks runtime upgrades** without breaking pipelines?\n4. How do you implement **Unity Catalog or equivalent governance** in Databricks?\n5. How do you manage **secrets, service principals, and credential passthrough**?\n6. What performance tuning techniques have you applied in Databricks for large datasets?\n7. How do you ensure **platform reliability and SLAs** for Databricks users?\n\n---\n\n## 3. Azure Data Factory (ADF) \u2013 Advanced Scenarios\n\n**Expect migration & version-upgrade questions**\n\n1. How do you design **metadata-driven ADF pipelines**?\n2. How do you manage **ADF connector upgrades** and breaking changes?\n3. How do you handle **ADF performance bottlenecks** at scale?\n4. Explain your strategy for **ADF CI/CD across environments**.\n5. How do you debug intermittent pipeline failures in production?\n6. What\u2019s your approach to **self-hosted integration runtime** vs managed IR?\n\n---\n\n## 4. CI/CD, DevOps & Infrastructure as Code\n\n**This is a core responsibility**\n\n1. How would you design a **CI/CD pipeline for Databricks + ADF** from scratch?\n2. What artifacts do you version control for data platforms?\n3. How have you used **Terraform** to provision Azure + Databricks infrastructure?\n4. Compare **Jenkins vs GitHub Actions vs Azure DevOps** for data engineering CI/CD.\n5. How do you manage **secrets and credentials** in CI/CD pipelines?\n6. How do you enforce **quality gates (tests, linting, approvals)** for data pipelines?\n7. How do you roll back a failed production deployment?\n\n---\n\n## 5. Disaster Recovery (DR) & High Availability\n\n**Associate Director-level topic**\n\n1. How do you design **DR for Databricks, ADF, and Storage**?\n2. What is your **RPO/RTO strategy** for data platforms?\n3. How do you handle **cross-region replication** for data lakes?\n4. What components are hardest to recover in a data platform and why?\n5. Have you executed a **real DR drill**? What did you learn?\n\n---\n\n## 6. Security, IAM & Compliance\n\n**Gartner is big on governance**\n\n1. How do you integrate data platforms with **Okta / Active Directory**?\n2. Explain **RBAC vs ABAC** in Azure data platforms.\n3. How do you manage **least-privilege access** for data engineers vs analysts?\n4. How do you secure **PII / sensitive data** in Databricks?\n5. What compliance frameworks have you worked with (SOC2, ISO, GDPR)?\n6. How do you audit and monitor **data access and activity logs**?\n\n---\n\n## 7. Monitoring, Observability & Reliability\n\n**Platform-first mindset**\n\n1. How do you use **Azure Log Analytics** for platform monitoring?\n2. What KPIs do you track for **data platform health**?\n3. How do you detect and resolve **data quality issues early**?\n4. How do you implement **alerting without alert fatigue**?\n5. Describe a major production incident you handled end-to-end.\n\n---\n\n## 8. Leadership, Coaching & Team Management\n\n**This will differentiate you**\n\n1. How do you balance **hands-on technical work vs leadership**?\n2. How do you mentor junior and mid-level engineers?\n3. How do you handle **underperforming team members**?\n4. How do you prioritize platform work vs feature delivery?\n5. Describe a time you had to **push back on stakeholders**.\n6. How do you build a culture of **engineering excellence and ownership**?\n7. How do you assess technical debt in data platforms?\n\n---\n\n## 9. Cross-Functional & Stakeholder Collaboration\n\n**Very Gartner-specific**\n\n1. How do you translate **business requirements** into platform capabilities?\n2. How do you work with **Data Science teams** using Databricks?\n3. How do you handle conflicting priorities between Product, IT, and Analytics?\n4. How do you communicate platform outages or risks to leadership?\n5. How do you justify platform investments to non-technical stakeholders?\n\n---\n\n## 10. Behavioral & Scenario-Based Questions\n\n**Expect situational depth**\n\n1. Describe a time you **modernized a legacy data platform**.\n2. Tell me about a **failed platform initiative** and what you learned.\n3. How do you decide whether to **build vs buy** platform capabilities?\n4. How do you stay updated with **new Azure & Databricks features**?\n5. Why Gartner, and why this **Associate Director** role specifically?\n\n---\n\n## 11. Likely \u201cDeep-Dive\u201d Follow-Ups\n\nThey\u2019ll pick **one project** and drill into:\n\n* Architecture diagrams\n* Design trade-offs\n* Security decisions\n* Cost vs performance\n* Incident handling\n* Leadership decisions\n\n---\n\n### If you want next:\n\n* \u2705 **Model answers tailored to your background**\n* \u2705 **Architecture diagrams (Azure + Databricks)**\n* \u2705 **Gartner-style STAR answers**\n* \u2705 **Mock interview simulation**\n\nJust tell me what you want to drill into first.\n"}, {"name": "time_series_questions.md", "type": "file", "content": "# Time Series Interview Questions (SQL & PySpark)\n\nThis guide covers 10 essential time-series patterns. Each section includes a scenario, a sample input dataset, and solutions in both SQL and PySpark.\n\n---\n\n## 1. Active vs Inactive Users\n**Scenario:** Identify users who have logged in within the last 7 days as \"Active\"; otherwise, \"Inactive\".\n**Input Data:** `user_logins`\n| user_id | login_date |\n|---|---|\n| 101 | 2024-01-20 |\n| 102 | 2024-01-10 |\n| 103 | 2024-01-22 |\n*(Current Date: 2024-01-22)*\n\n**SQL Solution:**\n```sql\nSELECT\n    user_id,\n    MAX(login_date) as last_login,\n    CASE\n        WHEN DATEDIFF(day, MAX(login_date), '2024-01-22') <= 7 THEN 'Active'\n        ELSE 'Inactive'\n    END as status\nFROM user_logins\nGROUP BY user_id;\n```\n\n**PySpark Solution:**\n```python\nfrom pyspark.sql.functions import col, max, datediff, lit, when\n\ndf.groupBy(\"user_id\").agg(max(\"login_date\").alias(\"last_login\")) \\\n  .withColumn(\"status\", when(datediff(lit(\"2024-01-22\"), col(\"last_login\")) <= 7, \"Active\").otherwise(\"Inactive\"))\n```\n\n**Output:**\n| user_id | last_login | status |\n|---|---|---|\n| 101 | 2024-01-20 | Active |\n| 102 | 2024-01-10 | Inactive |\n| 103 | 2024-01-22 | Active |\n\n---\n\n## 2. Day-over-Day Growth\n**Scenario:** Calculate the percentage growth in daily revenue compared to the previous day.\n**Input Data:** `daily_revenue`\n| date | revenue |\n|---|---|\n| 2024-01-01 | 100 |\n| 2024-01-02 | 120 |\n| 2024-01-03 | 108 |\n\n**SQL Solution:**\n```sql\nSELECT\n    date,\n    revenue,\n    CASE\n        WHEN LAG(revenue) OVER (ORDER BY date) IS NULL THEN 0\n        ELSE ROUND(((revenue - LAG(revenue) OVER (ORDER BY date)) / LAG(revenue) OVER (ORDER BY date)) * 100, 2)\n    END as growth_pct\nFROM daily_revenue;\n```\n\n**PySpark Solution:**\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag, col, round, when\n\nw = Window.orderBy(\"date\")\ndf.withColumn(\"prev_rev\", lag(\"revenue\").over(w)) \\\n  .withColumn(\"growth_pct\", when(col(\"prev_rev\").isNull(), 0)\n    .otherwise(round(((col(\"revenue\") - col(\"prev_rev\")) / col(\"prev_rev\")) * 100, 2)))\n```\n\n**Output:**\n| date | revenue | growth_pct |\n|---|---|---|\n| 2024-01-01 | 100 | 0.00 |\n| 2024-01-02 | 120 | 20.00 |\n| 2024-01-03 | 108 | -10.00 |\n\n---\n\n## 3. Rolling Averages\n**Scenario:** Calculate the 3-day rolling average of temperatures.\n**Input Data:** `weather`\n| date | temp |\n|---|---|\n| 2024-01-01 | 20 |\n| 2024-01-02 | 22 |\n| 2024-01-03 | 24 |\n| 2024-01-04 | 26 |\n\n**SQL Solution:**\n```sql\nSELECT\n    date,\n    temp,\n    AVG(temp) OVER (\n        ORDER BY date\n        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n    ) as rolling_avg_3d\nFROM weather;\n```\n\n**PySpark Solution:**\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import avg, col\n\nw = Window.orderBy(\"date\").rowsBetween(-2, 0)\ndf.withColumn(\"rolling_avg_3d\", avg(\"temp\").over(w))\n```\n\n**Output:**\n| date | temp | rolling_avg_3d |\n|---|---|---|\n| 2024-01-01 | 20 | 20.0 |\n| 2024-01-02 | 22 | 21.0 |\n| 2024-01-03 | 24 | 22.0 |\n| 2024-01-04 | 26 | 24.0 |\n\n---\n\n## 4. Peak Hour/Day\n**Scenario:** Identify the hour of the day with the highest total visits.\n**Input Data:** `traffic`\n| timestamp | visits |\n|---|---|\n| 2024-01-01 08:30:00 | 50 |\n| 2024-01-01 08:45:00 | 30 |\n| 2024-01-01 09:10:00 | 100 |\n\n**SQL Solution:**\n```sql\nWITH HourlyStats AS (\n    SELECT\n        DATEPART(hour, timestamp) as hour_of_day,\n        SUM(visits) as total_visits\n    FROM traffic\n    GROUP BY DATEPART(hour, timestamp)\n)\nSELECT TOP 1 * FROM HourlyStats ORDER BY total_visits DESC;\n-- Postgres: EXTRACT(HOUR FROM timestamp) ... LIMIT 1\n```\n\n**PySpark Solution:**\n```python\nfrom pyspark.sql.functions import hour, sum, desc\n\ndf.groupBy(hour(\"timestamp\").alias(\"hour_of_day\")) \\\n  .agg(sum(\"visits\").alias(\"total_visits\")) \\\n  .orderBy(desc(\"total_visits\")) \\\n  .limit(1)\n```\n\n**Output:**\n| hour_of_day | total_visits |\n|---|---|\n| 8 | 80 |\n\n---\n\n## 5. Consecutive Days Activity (Gaps & Islands)\n**Scenario:** Find users who have logged in for 3 consecutive days.\n**Input Data:** `logins`\n| user_id | date |\n|---|---|\n| A | 2024-01-01 |\n| A | 2024-01-02 |\n| A | 2024-01-03 |\n| B | 2024-01-01 |\n| B | 2024-01-03 |\n\n**SQL Solution:**\n```sql\nWITH Grouped AS (\n    SELECT\n        user_id,\n        date,\n        -- Magic: Date - RowNumber creates a constant value for consecutive dates\n        DATEADD(day, -ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY date), date) as island_id\n    FROM logins\n)\nSELECT user_id, COUNT(*) as streak_days\nFROM Grouped\nGROUP BY user_id, island_id\nHAVING COUNT(*) >= 3;\n```\n\n**PySpark Solution:**\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, col, date_sub, count\n\nw = Window.partitionBy(\"user_id\").orderBy(\"date\")\ndf.withColumn(\"rn\", row_number().over(w)) \\\n  .withColumn(\"island_id\", date_sub(col(\"date\"), col(\"rn\"))) \\\n  .groupBy(\"user_id\", \"island_id\").agg(count(\"*\").alias(\"streak\")) \\\n  .filter(col(\"streak\") >= 3)\n```\n\n**Output:**\n| user_id | streak_days |\n|---|---|\n| A | 3 |\n\n---\n\n## 6. First and Last Event\n**Scenario:** Find the first and last `page_url` visited by a user in a session.\n**Input Data:** `clicks`\n| user_id | time | page_url |\n|---|---|---|\n| 1 | 10:00 | /home |\n| 1 | 10:05 | /cart |\n| 1 | 10:10 | /checkout |\n\n**SQL Solution:**\n```sql\nSELECT\n    user_id,\n    MIN(time) as start_time,\n    MAX(time) as end_time,\n    FIRST_VALUE(page_url) OVER (PARTITION BY user_id ORDER BY time) as first_page,\n    LAST_VALUE(page_url) OVER (\n        PARTITION BY user_id ORDER BY time\n        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n    ) as last_page\nFROM clicks\nGROUP BY user_id, page_url, time; -- Note: Distinct is cleaner here\n-- Cleaner approach:\n-- SELECT DISTINCT user_id, first_value(...) ..., last_value(...) ...\n```\n\n**PySpark Solution:**\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import first, last, col\n\nw = Window.partitionBy(\"user_id\").orderBy(\"time\")\ndf.groupBy(\"user_id\").agg(\n    first(\"page_url\").alias(\"first_page\"),\n    last(\"page_url\").alias(\"last_page\")\n)\n```\n\n**Output:**\n| user_id | first_page | last_page |\n|---|---|---|\n| 1 | /home | /checkout |\n\n---\n\n## 7. Missing Dates\n**Scenario:** You have sales data for specific days, but want to show a report with \"0\" for days with no sales.\n**Input Data:** `sales`\n| date | amount |\n|---|---|\n| 2024-01-01 | 50 |\n| 2024-01-03 | 70 |\n\n**SQL Solution:**\n```sql\nWITH DateSeries AS (\n    -- Recursive CTE to generate dates\n    SELECT CAST('2024-01-01' AS DATE) as d\n    UNION ALL\n    SELECT DATEADD(day, 1, d) FROM DateSeries WHERE d < '2024-01-03'\n)\nSELECT\n    ds.d as date,\n    COALESCE(s.amount, 0) as amount\nFROM DateSeries ds\nLEFT JOIN sales s ON ds.d = s.date;\n```\n\n**PySpark Solution:**\n```python\nfrom pyspark.sql.functions import sequence, to_date, explode, col, lit\n\n# Create a range of dates\ndate_df = spark.sql(\"SELECT sequence(to_date('2024-01-01'), to_date('2024-01-03'), interval 1 day) as date_range\") \\\n               .withColumn(\"date\", explode(\"date_range\")).select(\"date\")\n\nresult = date_df.join(sales_df, \"date\", \"left\").fillna(0)\n```\n\n**Output:**\n| date | amount |\n|---|---|\n| 2024-01-01 | 50 |\n| 2024-01-02 | 0 |\n| 2024-01-03 | 70 |\n\n---\n\n## 8. Sessionization\n**Scenario:** Group events into a \"Session\" if they occur within 30 minutes of the previous event.\n**Input Data:** `events`\n| user_id | time |\n|---|---|\n| U1 | 10:00 |\n| U1 | 10:20 |\n| U1 | 11:00 |\n\n**SQL Solution:**\n```sql\nWITH Lagged AS (\n    SELECT\n        user_id,\n        time,\n        LAG(time) OVER (PARTITION BY user_id ORDER BY time) as prev_time\n    FROM events\n),\nNewSessionFlag AS (\n    SELECT\n        user_id,\n        time,\n        -- If time diff > 30 mins, mark as 1 (new session), else 0\n        CASE WHEN DATEDIFF(minute, prev_time, time) > 30 THEN 1 ELSE 0 END as is_new_session,\n        -- Special case: First event is always new\n        CASE WHEN prev_time IS NULL THEN 1 ELSE 0 END as is_first\n    FROM Lagged\n)\nSELECT\n    user_id,\n    time,\n    SUM(is_new_session + is_first) OVER (PARTITION BY user_id ORDER BY time) as session_id\nFROM NewSessionFlag;\n```\n\n**PySpark Solution:**\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag, col, sum, when, unix_timestamp\n\nw = Window.partitionBy(\"user_id\").orderBy(\"time\")\ndf = df.withColumn(\"prev_time\", lag(\"time\").over(w))\n\n# Calculate is_new_session (using timestamp seconds difference > 1800)\ndf = df.withColumn(\"is_new_session\",\n    when((col(\"time\").cast(\"long\") - col(\"prev_time\").cast(\"long\")) > 1800, 1).otherwise(0)\n)\n\n# Rolling sum to create session ID\ndf.withColumn(\"session_id\", sum(\"is_new_session\").over(w))\n```\n\n**Output:**\n| user_id | time | session_id |\n|---|---|---|\n| U1 | 10:00 | 0 |\n| U1 | 10:20 | 0 |\n| U1 | 11:00 | 1 |\n\n---\n\n## 9. Retention (Day 1 / Day 7)\n**Scenario:** Calculate Day 1 Retention (Users who joined on Day 0 and came back on Day 1).\n**Input Data:** `activity` (includes join event and plain login)\n| user_id | date | event_type |\n|---|---|---|\n| U1 | 2024-01-01 | signup |\n| U1 | 2024-01-02 | login |\n| U2 | 2024-01-01 | signup |\n\n**SQL Solution:**\n```sql\nWITH Cohort AS (\n    SELECT user_id, date as join_date\n    FROM activity WHERE event_type = 'signup'\n)\nSELECT\n    c.join_date,\n    COUNT(DISTINCT c.user_id) as total_users,\n    COUNT(DISTINCT a.user_id) as retained_users,\n    CAST(COUNT(DISTINCT a.user_id) AS FLOAT) / COUNT(DISTINCT c.user_id) as retention_rate\nFROM Cohort c\nLEFT JOIN activity a\n    ON c.user_id = a.user_id\n    AND a.date = DATEADD(day, 1, c.join_date) -- Day 1 check\nGROUP BY c.join_date;\n```\n\n**PySpark Solution:**\n```python\nfrom pyspark.sql.functions import date_add, countDistinct\n\ncohort = df.filter(col(\"event_type\") == \"signup\").select(col(\"user_id\"), col(\"date\").alias(\"join_date\"))\n\nretained = df.alias(\"a\").join(cohort.alias(\"c\"),\n    (col(\"a.user_id\") == col(\"c.user_id\")) &\n    (col(\"a.date\") == date_add(col(\"c.join_date\"), 1)), \"left\")\n\nretained.groupBy(\"c.join_date\").agg(\n    countDistinct(\"c.user_id\").alias(\"cohort_size\"),\n    countDistinct(\"a.user_id\").alias(\"retained_count\")\n)\n```\n\n**Output:**\n| join_date | total_users | retained_users | retention_rate |\n|---|---|---|---|\n| 2024-01-01 | 2 | 1 | 0.50 |\n\n---\n\n## 10. Time-based Deduplication\n**Scenario:** Keep only the *latest* record for each user based on timestamp.\n**Input Data:** `updates`\n| user_id | updated_at | status |\n|---|---|---|\n| A | 10:00 | Pending |\n| A | 10:05 | Approved |\n\n**SQL Solution:**\n```sql\nWITH Ranked AS (\n    SELECT\n        user_id,\n        updated_at,\n        status,\n        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY updated_at DESC) as rn\n    FROM updates\n)\nSELECT user_id, updated_at, status\nFROM Ranked\nWHERE rn = 1;\n```\n\n**PySpark Solution:**\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, desc\n\nw = Window.partitionBy(\"user_id\").orderBy(desc(\"updated_at\"))\ndf.withColumn(\"rn\", row_number().over(w)) \\\n  .filter(col(\"rn\") == 1) \\\n  .drop(\"rn\")\n```\n\n**Output:**\n| user_id | updated_at | status |\n|---|---|---|\n| A | 10:05 | Approved |\n"}]}, {"name": "pyspark_scenario_questions.md", "type": "file", "content": "# PySpark Scenario-Based Interview Questions\n*Mid to Senior Level*\n\nThis guide focuses on real-world production scenarios, performance tuning, and architectural challenges commonly asked in Data Engineering interviews.\n\n---\n\n## Table of Contents\n1. [Performance Tuning & OOM Issues](#performance-tuning--oom-issues)\n2. [Data Skew Scenarios](#data-skew-scenarios)\n3. [File Management & Storage](#file-management--storage)\n4. [Complex Transformation Logic](#complex-transformation-logic)\n5. [Structured Streaming Scenarios](#structured-streaming-scenarios)\n\n---\n\n## Performance Tuning & OOM Issues\n\n### 1. The \"Last 2 Tasks\" Hanging Problem\n**Scenario:** You are running a Spark job with 200 tasks. 198 tasks finish in 2 minutes, but the last 2 tasks hang for 45 minutes and finally cause an OOM (Out Of Memory) error. What is happening and how do you fix it?\n\n**Analysis:**\nThis is a classic symptom of **Data Skew**. The data is not evenly distributed across partitions. One or two keys have millions of records (e.g., `null` keys or a default 'Unknown' value), causing a few executors to process 50x more data than others.\n\n**Solutions:**\n1.  **Salted Key Join:** Add a random number (salt) to the skew key to distribute it across multiple partitions (explained in Section 2).\n2.  **Broadcast Join:** If one side of the join is small, force a Broadcast join to avoid shuffling the large skew table.\n3.  **Filter Nulls:** If the skew is caused by `null` keys that you don't actually need, filter them out *before* the join.\n\n### 2. Driver OOM vs Executor OOM\n**Scenario:** Your job fails with `java.lang.OutOfMemoryError`. How do you distinguish if it's the Driver or the Executor, and what are the fixes for each?\n\n**Answer:**\n*   **Driver OOM (`java.lang.OutOfMemoryError: Java heap space` on Driver):**\n    *   **Cause:** Doing `df.collect()` on a large dataset, broadcasting a table that is too big (>8GB), or maintaining too much metadata in a complex DAG outside of DataFrames.\n    *   **Fix:** Avoid `collect()`, use `toLocalIterator()`, increase `spark.driver.memory`, or increase `spark.sql.autoBroadcastJoinThreshold` if a broadcast is crashing it.\n*   **Executor OOM:**\n    *   **Cause:** Large partitions (Skew), creating very large objects in UDFs, or simple lack of memory for the task.\n    *   **Fix:** Handle Skew, decrease `spark.sql.files.maxPartitionBytes` to break up inputs, or increase `spark.executor.memory`.\n\n### 3. Catalyst Optimizer & Physical Plans\n**Scenario:** A junior engineer wrote a query joining 3 tables. How can you tell if Spark is using the most efficient Join Strategy?\n\n**Answer:**\nUse `df.explain(True)` to view the Physical Plan. Look for:\n*   **BroadcastHashJoin:** Fastest. Used for Big Table + Small Table.\n*   **SortMergeJoin:** Standard for Big + Big tables. Requires a Shuffle and Sort phase.\n*   **ShuffleHashJoin:** Used when tables are large but one fits in executor memory (rarely defaults over SortMerge).\n*   **CartesianProduct:** DANGER. Nested loop join. Occurs if you join without conditions or using a non-equi join on 2 distinct tables.\n\n---\n\n## Data Skew Scenarios\n\n### 4. Implementing the Salted Key Join\n**Scenario:** You need to join a `Transactions` table (100 Billion rows) with a `Customers` table (10 Million rows) on `customer_id`. The `Customers` table is too big to broadcast. A few VIP customers have millions of transactions, causing skew.\n\n**Solution (Salting Technique):**\nWe split the skewed keys in the Big Table into smaller chunks and replicate the matching keys in the Medium Table.\n\n1.  **Explode (Replicate) the Smaller Table:** \n    Create `N` copies of each row in `Customers`, adding a `salt` ID (0 to N-1).\n2.  **Salt the Larger Table:** \n    Add a random number (0 to N-1) to every row in `Transactions`.\n3.  **Join:** \n    Join on `customer_id` AND `salt`.\n\n**Code Snippet:**\n```python\nfrom pyspark.sql.functions import rand, explode, array, lit, col\n\nSALT_FACTOR = 10\n\n# 1. Salt the Big Table (Transactions)\ndf_tx_salted = df_tx.withColumn(\"salt\", (rand() * SALT_FACTOR).cast(\"int\"))\n\n# 2. Replicate the Medium Table (Customers)\n# Create array [0, 1, ... 9] and explode it to generate rows\ndf_cust_salted = df_cust.withColumn(\"salt_array\", array([lit(i) for i in range(SALT_FACTOR)])) \\\n                        .withColumn(\"salt\", explode(\"salt_array\"))\n\n# 3. Join on Key + Salt (Evenly distributed!)\ndf_joined = df_tx_salted.join(df_cust_salted, [\"customer_id\", \"salt\"])\n```\n\n---\n\n## File Management & Storage\n\n### 5. Small File Problem\n**Scenario:** Your Hive/Delta table has 100 partitions, but each partition contains 5,000 tiny files (KB size). Query performance is terrible. Why?\n\n**Answer:**\n*   **Why:** Opening a file in HDFS/S3 has high overhead (latency/metadata ops). Spark spends more time listing and opening files than reading data.\n*   **Fix (Write Side):**\n    *   `df.coalesce(5).write...` (Reduces files per partition without shuffling).\n    *   `df.repartition(5, \"col\").write...` (Guarantees exactly 5 files, good for partition writes).\n*   **Fix (Maintenance):** Perform \"Compaction\". Read the partition and overwrite it with `repartition()`.\n*   **Fix (Auto):** In Delta Lake, run `OPTIMIZE table_name`.\n\n### 6. Parquet vs Avro vs ORC\n**Scenario:** Which file format would you choose for a Write-Heavy transactional system vs a Read-Heavy analytical system?\n\n*   **Parquet:**\n    *   **Type:** Columnar.\n    *   **Best For:** Heavy Analytics (OLAP). Compresses very well, allows **Column Pruning** (reading only needed columns) and **Predicate Pushdown**.\n    *   **Use Case:** Data Lake reporting layer.\n*   **Avro:**\n    *   **Type:** Row-based.\n    *   **Best For:** Write-heavy ops, Schema evolution support.\n    *   **Use Case:** Kafka landing zones, CDC capture (where you need to write whole rows fast).\n\n---\n\n## Complex Transformation Logic\n\n### 7. Sessionization (Gaps and Islands)\n**Scenario:** You have clickstream data: `user_id`, `timestamp`. Identify \"Sessions\" where a session expires if the user is inactive for more than 30 minutes.\n\n**Solution:**\nUse Window functions to compare the current timestamp with the previous one.\n\n```python\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import lag, col, sum as spark_sum, unix_timestamp\n\n# Check difference between current and prev timestamp\nw = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n\ndf = df.withColumn(\"prev_ts\", lag(\"timestamp\").over(w))\n       \n# Flag new session if gap > 30 mins (1800 seconds)\ndf = df.withColumn(\"is_new_session\", \n    (unix_timestamp(\"timestamp\") - unix_timestamp(\"prev_ts\") > 1800).cast(\"int\"))\n\n# Cumulative Sum to generate Session ID\ndf = df.withColumn(\"session_id\", \n    spark_sum(\"is_new_session\").over(w))\n```\n\n### 8. Handling \"Late\" Data in Aggregations\n**Scenario:** You are calculating hourly aggregates. Data from 9:00 AM might arrive at 10:15 AM due to network lag. How do you handle this?\n\n**Answer:**\n*   **Batch:** Reprocess partitions (e.g., overwrite today AND yesterday's data every run).\n*   **Structured Streaming:** Use **Watermarking**.\n    *   `withWatermark(\"timestamp\", \"2 hours\")` tells Spark to keep the aggregation state for 2 hours. If data arrives within that window, the aggregate updates. If it arrives 3 hours late, it is dropped.\n\n---\n\n## Optimization Checklist (Quick Fire)\n\n1.  **Cache/Persist:** Only cache if you reuse the DataFrame multiple times. Unpersist when done!\n2.  **Serialization:** Use Kryo serialization (`conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")`) for better performance than Java serialization.\n3.  **Pandas UDFs:** If you MUST use python code (UDF), use Vectorized Pandas UDFs (Arrow) instead of standard Python UDFs. Standard UDFs serialize row-by-row (slow), Pandas UDFs use batches (fast).\n4.  **Filter Early:** Filter data as close to the source as possible to reduce shuffle data volume.\n"}, {"name": "python_interview_questions.md", "type": "file", "content": "# Advanced Python Interview Questions for Data Engineers\n*Mid to Senior Level*\n\nA detailed guide to Python concepts critical for building robust, scalable data pipelines. Focuses on memory management, functional programming, high-performance pandas, and concurrency.\n\n---\n\n## Table of Contents\n1. [Core Python & Memory Management](#core-python--memory-management)\n2. [Data Processing (Pandas & NumPy)](#data-processing-pandas--numpy)\n3. [Concurrency & Parallelism](#concurrency--parallelism)\n4. [Functional Programming & Patterns](#functional-programming--patterns)\n5. [Algorithmic Scenarios for DE](#algorithmic-scenarios-for-de)\n\n---\n\n## Core Python & Memory Management\n\n### 1. Generators vs Iterators (`yield` keyword)\n**Q:** What is the difference between a list and a generator? Why prefer generators for ETL?\n**A:**\n- **List:** Stores all elements in memory at once. `[x*2 for x in range(10000000)]` can cause OOM (Out of Memory).\n- **Generator:** Lazily produces items one by one. `(x*2 for x in range(10000000))` uses constant memory.\n- **DE Use Case:** Reading a 50GB log file line-by-line using `yield` instead of `readlines()`.\n\n```python\ndef read_large_file(file_path):\n    with open(file_path, 'r') as f:\n        for line in f:\n            yield line.strip()\n\n# Usage - Memory efficient iteration\nfor line in read_large_file('huge_log.txt'):\n    process(line)\n```\n\n### 2. Context Managers (`with` statement)\n**Q:** How do Context Managers work? Write a custom one for a Database connection.\n**A:** They ensure resources are acquired and released (setup/teardown) automatically using `__enter__` and `__exit__`.\n```python\nclass DBConnection:\n    def __init__(self, db_url):\n        self.db_url = db_url\n\n    def __enter__(self):\n        print(\"Connecting to DB...\")\n        self.conn = connect_to_db(self.db_url)\n        return self.conn\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        print(\"Closing Connection...\")\n        self.conn.close()\n        # Returning False propagates exceptions, True suppresses them\n        return False\n\n# Usage\nwith DBConnection('postgres://...') as conn:\n    conn.execute(\"SELECT * FROM users\")\n```\n\n### 3. Shallow vs Deep Copy\n**Q:** Explain the issue here: `list_a = [[1], [2]]; list_b = list_a`.\n**A:** `list_b` is just a reference. Modifying `list_b[0][0] = 99` changes `list_a` too.\n- **Shallow Copy (`copy.copy`):** Copies the container but references the inner objects.\n- **Deep Copy (`copy.deepcopy`):** Recursively copies everything. Critical when manipulating nested JSON/Dicts in pipelines to avoid side effects.\n\n---\n\n## Data Processing (Pandas & NumPy)\n\n### 4. Vectorization vs `apply()`\n**Q:** Why is iteration over a DataFrame slow? How do you speed it up?\n**A:**\n1.  **Iteration (`iterrows`)**: Slowest. Python level loop.\n2.  **`apply()`**: Better, but still processes row-by-row (mostly).\n3.  **Vectorization (NumPy/Pandas Native)**: Fastest. Uses optimized C/Cython execution.\n\n**Example:** Calculate `col_C = col_A * col_B`\n```python\n# SLOW\ndf['C'] = df.apply(lambda row: row['A'] * row['B'], axis=1)\n\n# FAST (Vectorized)\ndf['C'] = df['A'] * df['B']\n```\n\n### 5. Memory Optimization in Pandas\n**Q:** You have a 10GB CSV but only 8GB RAM. How do you load it?\n**A:**\n1.  **Chunking:** `pd.read_csv('file.csv', chunksize=10000)` and process iteratively.\n2.  **Dtypes:**\n    - Downcast ints: `int64` -> `int32` or `int16`.\n    - Strings to Categoricals: If a column has low cardinality (e.g., 'Gender', 'Country'), `astype('category')` saves ~90% memory.\n    - Use `usecols` to load only necessary columns.\n\n---\n\n## Concurrency & Parallelism\n\n### 6. GIL (Global Interpreter Lock)\n**Q:** What is the GIL? Does it affect Data Engineering tasks?\n**A:**\n- **GIL:** A mutex that prevents multiple native threads from executing Python bytecodes at once.\n- **Impact:** Python threads are not true parallel for **CPU-bound** tasks (e.g., heavy math, complex scrubbing).\n- **Solution:** Use **Multiprocessing** (separate processes, unrelated memory) for CPU tasks.\n- **Exception:** For **I/O-bound** tasks (API requests, DB queries, File downloads), the GIL is released. Threading works well here.\n\n### 7. Multiprocessing vs Threading\n**Scenario:** You need to scrape 1000 websites vs You need to resize 1000 images.\n- **Scraping (I/O Bound):** Use `threading` or `asyncio`.\n- **Image Resizing (CPU Bound):** Use `multiprocessing`.\n\n---\n\n## Functional Programming & Patterns\n\n### 8. Decorators\n**Q:** Write a decorator that retries a function 3 times if it fails.\n**A:**\n```python\nimport time\nfrom functools import wraps\n\ndef retry(retries=3, delay=1):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for i in range(retries):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if i == retries - 1: raise e\n                    time.sleep(delay)\n                    print(f\"Retrying {i+1}...\")\n        return wrapper\n    return decorator\n\n@retry(retries=3)\ndef extract_data_from_api():\n    # Flaky API call\n    pass\n```\n\n### 9. Map, Filter, Reduce\n**Q:** Transform a list of numbers: keep evens, multiply by 2, and sum them up using functional tools.\n```python\nfrom functools import reduce\n\nnums = [1, 2, 3, 4, 5, 6]\n\n# Functional approach\nresult = reduce(lambda x, y: x + y, \n               map(lambda x: x * 2, \n                   filter(lambda x: x % 2 == 0, nums)))\n# Result: (2*2) + (4*2) + (6*2) = 4 + 8 + 12 = 24\n```\n\n---\n\n## Algorithmic Scenarios for DE\n\n### 10. Streaming Data Moving Average\n**Q:** Design a class `MovingAverage` that receives a stream of integers and computes the moving average of the last N items.\n**A:** Using a `deque` (Doubly Ended Queue) is O(1) for appending/popping, unlike a list which is O(N) for popping from the start.\n```python\nfrom collections import deque\n\nclass MovingAverage:\n    def __init__(self, size):\n        self.size = size\n        self.queue = deque()\n        self.total_sum = 0 # Maintain sum to avoid O(N) recalculation\n\n    def next(self, val):\n        self.queue.append(val)\n        self.total_sum += val\n        \n        if len(self.queue) > self.size:\n            removed = self.queue.popleft()\n            self.total_sum -= removed\n            \n        return self.total_sum / len(self.queue)\n```\n\n### 11. Custom Sort\n**Q:** Sort a list of file names `['file_1.txt', 'file_10.txt', 'file_2.txt']` numerically, not alphabetically.\n**A:** Default sort gives `1, 10, 2`. We need a lambda key.\n```python\nfiles = ['file_1.txt', 'file_10.txt', 'file_2.txt']\n\nfiles.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n# Output: ['file_1.txt', 'file_2.txt', 'file_10.txt']\n```\n"}, {"name": "real_world_interview_scenarios.md", "type": "file", "content": "# Real-World Data Engineering Interview Scenarios\n*Compiled from Interview Notes*\n\nA collection of mixed SQL, PySpark, Python, and Conceptual questions derived from real interview experiences (Jio, Netflix, etc.).\n\n---\n\n## Table of Contents\n1. [Advanced SQL Scenarios](#advanced-sql-scenarios)\n2. [PySpark Coding Challenges](#pyspark-coding-challenges)\n3. [Python Algorithms](#python-algorithms)\n4. [Conceptual & Linux](#conceptual--linux)\n\n---\n\n## Advanced SQL Scenarios\n\n### 1. Nth Highest Record\n**Q:** Find the movie with the 9th highest collection.\n**A:**\n```sql\nSELECT name, collection \nFROM (\n    SELECT name, collection, \n           DENSE_RANK() OVER (ORDER BY collection DESC) as rnk\n    FROM movie\n) WHERE rnk = 9;\n```\n\n### 2. Year-Over-Year Growth Flag\n**Q:** Determine if a customer's billing increased or decreased compared to the previous year.\n**A:**\n```sql\nSELECT cust, year, total_billing,\n    CASE \n        WHEN total_billing > LAG(total_billing) OVER (PARTITION BY cust ORDER BY year) \n        THEN 'Increase'\n        ELSE 'Decrease'\n    END as billing_flag\nFROM billing_table;\n```\n\n### 3. Manager-Employee Hierarchy\n**Q:** List employees and their manager's name. If no manager, show 'No Manager'.\n**A:**\n```sql\nSELECT \n    e1.emp_name as Employee, \n    COALESCE(e2.emp_name, 'No Manager') as Manager\nFROM employee_tbl e1\nLEFT JOIN employee_tbl e2 ON e1.manager_id = e2.emp_id;\n```\n\n### 4. Pivot Scenarios\n**Q:** Pivot Vendor/Qty table to columns (Amazon, Flipkart, etc.).\n**A:**\n```sql\nSELECT \n    SUM(CASE WHEN Vendor = 'Amazon' THEN Qty ELSE 0 END) as Amazon,\n    SUM(CASE WHEN Vendor = 'Flipkart' THEN Qty ELSE 0 END) as Flipkart,\n    SUM(CASE WHEN Vendor = 'Myntra' THEN Qty ELSE 0 END) as Myntra\nFROM orders;\n```\n\n### 5. Join Cardinality Puzzle\n**Q:** Given Table A (1, 1, 2, NULL) and Table B (1, 3, 4, NULL), calculate counts.\n*   **Inner Join:** Matches `1, 1` (Count: 2) -> (Only 1s match)\n*   **Left Join:** `1, 1, 2, NULL` (Count: 4) -> (All rows from A)\n*   **Right Join:** `1, 1, 3, 4, NULL` (Count: 5) -> (Matching 1s + B's unique)\n*   **Full Outer:** All unique rows + matches.\n*   **Cross Join:** 4 rows * 4 rows = 16 rows.\n\n---\n\n## PySpark Coding Challenges\n\n### 6. Career Path Analysis\n**Q:** Find employees who started at 'Microsoft' immediately followed by 'Google'.\n**A:**\n```python\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag, col, lead\n\nw = Window.partitionBy(\"emp_id\").orderBy(\"startyear\")\n\ndf_lead = df.withColumn(\"next_employer\", lead(\"employer\").over(w))\n\nresult = df_lead.filter(\n    (col(\"employer\") == \"Microsoft\") & \n    (col(\"next_employer\") == \"Google\")\n).select(\"emp_id\")\n```\n\n### 7. Conditional Status Flag\n**Q:** Create a new column 'Status'. If Sum(A+B+C) <= 10 -> 'Unbreached', else 'Breached'.\n**A:**\n```python\nfrom pyspark.sql.functions import col, when, lit\n\ndf = df.withColumn(\"Status\", \n    when((col(\"A\") + col(\"B\") + col(\"C\")) <= 10, lit(\"Unbreached\"))\n    .otherwise(lit(\"Breached\"))\n)\n```\n\n### 8. Character Count (Explode)\n**Q:** Count the frequency of every character in a text file.\n**A:**\n```python\nfrom pyspark.sql.functions import split, explode\n\n# Read text file\ndf = spark.read.text(\"path/to/file\")\n\n# Split lines into characters and explode\ndf_chars = df.select(explode(split(col(\"value\"), \"\")).alias(\"char\"))\n\n# Filter out empty strings if needed and count\ndf_chars.filter(col(\"char\") != \"\").groupby(\"char\").count().show()\n```\n\n---\n\n## Python Algorithms\n\n### 9. Max Product Pair\n**Q:** Find the pair of numbers in `arr = {1, 7, -23, -58, 7, 0}` that has the maximum product.\n**Note:** Two huge negative numbers can make a large positive product!\n**A:**\n```python\narr = [1, 7, -23, -58, 7, 0]\n\ndef max_product(arr):\n    n = len(arr)\n    if n < 2: return 0\n    \n    max_val = float('-inf')\n    \n    # O(N^2) Approach\n    for i in range(n):\n        for j in range(i + 1, n):\n            prod = arr[i] * arr[j]\n            if prod > max_val:\n                max_val = prod\n                \n    return max_val\n    \n    # O(N log N) Approach: Sort and compare first 2 vs last 2\n    # arr.sort()\n    # return max(arr[0] * arr[1], arr[-1] * arr[-2])\n\nprint(max_product(arr)) # Output: 1334 (-23 * -58)\n```\n\n### 10. Manual Word Count\n**Q:** Count words in string without `len()` or `split()` (conceptual logic).\n**A:**\n```python\ntext = \"Hi Himanshu Welcome\"\ncount = 0\nis_word = False\n\nfor char in text:\n    if char != ' ':\n        if not is_word:\n            count += 1\n            is_word = True\n    else:\n        is_word = False\n        \nprint(count)\n```\n\n---\n\n## Conceptual & Linux\n\n### 11. Linux Commands for DE\n*   **Find large old files:**\n    `find /path -size -1M -mtime +180 -print`\n    *(Find files smaller than 1MB and older than 180 days)*\n*   **Replace string in file:**\n    `sed -i 's/himanshu/othername/g' filename.txt`\n*   **Check File Difference:**\n    `diff file1.txt file2.txt`\n\n### 12. Spark Internals\n*   **Broadcast Join:** Copies small table (<10MB) to all nodes to avoid shuffle.\n*   **Shuffle vs Sort Merge:** \n    *   *Shuffle Hash:* Good for large tables where one fits in memory.\n    *   *Sort Merge:* Standard for 2 huge tables (Sorts both, then merges).\n*   **Catalyst Optimizer:**\n    1.  **Analysis:** Checks schema/types.\n    2.  **Logical Plan:** Optimizes logical steps (filter pushdown).\n    3.  **Physical Plan:** Selects join strategies (Broadcast vs SortMerge).\n    4.  **Code Generation:** Generates Java bytecode (WholeStageCodegen).\n\n### 13. Cluster Sizing Calculation\n**Scenario:** Total Cores: 150, RAM: 64GB per Node.\n**Q:** How many executors?\n*   **Rule of Thumb:** 5 Cores per executor (Good balance for I/O & CPU).\n*   ** Executors per Node:** `Total Cores / 5` = `150 / 5` = 30 Executors (Likely too high for one node, usually 5 executors per node with 5 cores each).\n*   *Typical calculation:*\n    *   Leave 1 Core/1GB for OS/Hadoop Deamon.\n    *   Executors = (Available Cores / 5).\n    *   Memory = (Available RAM / Executors).\n"}, {"name": "sql_interview_questions.md", "type": "file", "content": "# Advanced SQL Interview Questions for Data Engineers\n*Mid to Senior Level*\n\nA curated collection of advanced SQL scenarios focusing on query optimization, complex window functions, recursive queries, and data modeling challenges often encountered in Data Engineering interviews.\n\n---\n\n## Table of Contents\n1. [Window Functions & Analytics](#window-functions--analytics)\n2. [Recursive Queries & CTEs](#recursive-queries--ctes)\n3. [Advanced Aggregations (Gaps & Islands)](#advanced-aggregations-gaps--islands)\n4. [Performance Tuning & Indexing](#performance-tuning--indexing)\n5. [Data Modeling & Schema Design](#data-modeling--schema-design)\n6. [Practical Scenarios](#practical-scenarios)\n\n---\n\n## Window Functions & Analytics\n\n### 1. ROWS vs RANGE in Window Functions\n**Q:** What is the difference between `ROWS BETWEEN` and `RANGE BETWEEN`?\n**A:**\n- `ROWS`: Operates on physical rows. `ROWS BETWEEN 1 PRECEDING AND CURRENT ROW` looks at exactly the row before the current one, regardless of value.\n- `RANGE`: Operates on logical values. `RANGE BETWEEN 5 PRECEDING AND CURRENT ROW` looks at rows where the ordering value is within 5 units of the current row's value.\n\n**Example Scenario:** Calculating a rolling 7-day average sales.\n```sql\nSELECT \n    sale_date, \n    daily_sales,\n    AVG(daily_sales) OVER (\n        ORDER BY sale_date \n        RANGE BETWEEN INTERVAL '6' DAY PRECEDING AND CURRENT ROW\n    ) as rolling_7_day_avg\nFROM SALES;\n```\n*Note: Using `RANGE` handles missing dates correctly (it looks partially back in time), whereas `ROWS 6 PRECEDING` would just take the previous 6 records even if there are gaps in dates.*\n\n### 2. Retention Analysis (Cohort Analysis)\n**Q:** Calculate Month-over-Month retention rate.\n**Context:** Given a `user_logins` table (user_id, login_date).\n**Solution:**\n```sql\nWITH first_login AS (\n    SELECT user_id, DATE_TRUNC('month', MIN(login_date)) as cohort_month\n    FROM user_logins\n    GROUP BY user_id\n),\nretention AS (\n    SELECT \n        f.cohort_month,\n        DATE_TRUNC('month', l.login_date) as activity_month,\n        COUNT(DISTINCT l.user_id) as active_users\n    FROM user_logins l\n    JOIN first_login f ON l.user_id = f.user_id\n    GROUP BY 1, 2\n)\nSELECT \n    cohort_month,\n    activity_month,\n    active_users,\n    FIRST_VALUE(active_users) OVER (PARTITION BY cohort_month ORDER BY activity_month) as cohort_size,\n    active_users::float / FIRST_VALUE(active_users) OVER (PARTITION BY cohort_month ORDER BY activity_month) as retention_rate\nFROM retention;\n```\n\n---\n\n## Recursive Queries & CTEs\n\n### 3. Employee Hierarchy Traversal\n**Q:** Given an `Employees` table (id, name, manager_id), write a query to list all employees under a specific manager (e.g., Manager ID 1) at all levels (direct and indirect reports).\n**A:** requires a Recursive Common Table Expression (CTE).\n```sql\nWITH RECURSIVE Hierarchy AS (\n    -- Anchor member: Direct reports\n    SELECT id, name, manager_id, 1 as level\n    FROM Employees\n    WHERE manager_id = 1\n    \n    UNION ALL\n    \n    -- Recursive member: Reports of the reports\n    SELECT e.id, e.name, e.manager_id, h.level + 1\n    FROM Employees e\n    INNER JOIN Hierarchy h ON e.manager_id = h.id\n)\nSELECT * FROM Hierarchy;\n```\n\n---\n\n## Advanced Aggregations (Gaps & Islands)\n\n### 4. The \"Gaps and Islands\" Problem\n**Q:** Find the start and end ranges of consecutive numbers in a table.\n**Data:** `Ids: 1, 2, 3, 5, 6, 8, 9, 10`\n**Expected Output:**\n- 1-3\n- 5-6\n- 8-10\n\n**Solution:**\nThe trick is that `Id` minus `Row_Number` is constant for consecutive sequences.\n```sql\nWITH CTE AS (\n    SELECT \n        Id,\n        Id - ROW_NUMBER() OVER (ORDER BY Id) as Grp\n    FROM Numbers\n)\nSELECT \n    MIN(Id) as Start_Range,\n    MAX(Id) as End_Range\nFROM CTE\nGROUP BY Grp;\n```\n\n---\n\n## Performance Tuning & Indexing\n\n### 5. Clustered vs Non-Clustered Index\n**Q:** Explain the difference relative to storage and retrieval.\n**A:**\n- **Clustered Index:** Defines the physical order of data on disk. A table can have only **one** clustered index (usually PK). Leaf nodes contain the actual data pages.\n- **Non-Clustered Index:** Stored separately from the data rows. Contains key values and pointers (Row ID or Clustered Key) to the actual data. A table can have multiple.\n- **DE Context:** In columnar databases (Redshift, Snowflake), we talk about \"Sort Keys\" or \"Clustering Keys\" which mimic this concept to prune file scanning (Zone Maps/Data Skipping).\n\n### 6. Query Optimization Techniques\n**Q:** A query is performing poorly on a 10TB table. Steps to debug?\n**A:**\n1.  **Analyze Execution Plan:** Look for Full Table Scans vs Index Scans/Seeks.\n2.  **Partition Pruning:** Ensure `WHERE` clause filters on partition columns.\n3.  **Skew Analysis:** Check if one join key has disproportionately more rows (causing data skew in distributed systems).\n4.  **Join Order:** Filter largest tables *before* joining.\n5.  **Statistics:** Run `ANALYZE` or `UPDATE STATISTICS` to ensure the optimizer has fresh cardinality estimates.\n\n### 7. Broadcast Join vs Shuffle Hash Join\n**Q:** When does the optimizer choose a Broadcast join?\n**A:** When one side of the join is small enough to fit in memory (e.g., <10MB default in Spark). It replicates the small table to all nodes, avoiding a massive shuffle of the large table.\n\n---\n\n## Data Modeling & Schema Design\n\n### 8. Slowly Changing Dimensions (SCD Type 2)\n**Q:** How do you efficiently update an SCD Type 2 table using SQL (MERGE)?\n**A:**\nTypically involves a `MERGE` statement or a `FULL OUTER JOIN` approach.\n1.  Identify records that are new (Insert).\n2.  Identify records that changed (Update `end_date` of old, Insert new).\n3.  Identify unchanged records (Do nothing).\n\n**Pseudocode MERGE:**\n```sql\nMERGE INTO target_dim t\nUSING source_stage s ON t.id = s.id\nWHEN MATCHED AND t.current_flag = 'Y' AND t.hash != s.hash THEN\n    UPDATE SET current_flag = 'N', end_date = current_date\n    -- Note: This usually requires a second pass/insert for the new active record depending on DB support\nWHEN NOT MATCHED THEN\n    INSERT (id, col1, start_date, current_flag) VALUES (s.id, s.col1, current_date, 'Y');\n```\n\n### 9. Fact vs Dimension Tables\n**Q:** Explain a Star Schema vs Snowflake Schema. why prefer Star in Data Warehousing?\n**A:**\n- **Star:** Fact table in center, denormalized dimension tables around it. 1 level of join.\n- **Snowflake:** Dimension tables are normalized (split into sub-dimensions). Multiple hops/joins.\n- **Preference:** Star Schema is preferred for Analytics (OLAP) because:\n    - Simpler queries (fewer joins).\n    - Disk space is cheap; slight redundancy in dimensions is worth the read performance gain.\n\n---\n\n## Practical Scenarios\n\n### 10. Pivot and Unpivot\n**Q:** You have data: `Product | Jan_Sales | Feb_Sales`. Convert to `Product | Month | Sales`.\n**A:** Use `UNPIVOT` or `CROSS JOIN LATERAL` (Postgres) or `UNION ALL`.\n```sql\nSELECT Product, 'Jan' as Month, Jan_Sales as Sales FROM T\nUNION ALL\nSELECT Product, 'Feb' as Month, Feb_Sales as Sales FROM T;\n```\n\n**Q:** Reverse it? (Rows to Columns)\n**A:** Use `CASE WHEN` with Aggregation (Pivot).\n```sql\nSELECT \n    Product,\n    SUM(CASE WHEN Month = 'Jan' THEN Sales ELSE 0 END) as Jan_Sales,\n    SUM(CASE WHEN Month = 'Feb' THEN Sales ELSE 0 END) as Feb_Sales\nFROM T\nGROUP BY Product;\n```\n\n### 11. Finding Duplicates without `DISTINCT` or `GROUP BY`\n**Q:** How to find duplicates without standard aggregation? (Tricky question)\n**A:** Self-Join.\n```sql\nSELECT DISTINCT a.email \nFROM users a\nJOIN users b ON a.email = b.email AND a.id < b.id;\n```\n"}, {"name": "system_design_interview_questions.md", "type": "file", "content": "# Data Engineering System Design Interview Questions\n*Mid to Staff Level*\n\nA comprehensive guide to designing scalable data platforms, covering architecture, streaming, and operational excellence.\n\n---\n\n## Table of Contents\n1. [Architectural Patterns](#architectural-patterns)\n2. [Streaming & Real-Time](#streaming--real-time)\n3. [Data Lakehouse Design](#data-lakehouse-design)\n4. [Operations & Handling Failures](#operations--handling-failures)\n5. [Scenario: Rate Limiting & Aggregation](#scenario-rate-limiting--aggregation)\n\n---\n\n## Architectural Patterns\n\n### 1. Lambda vs Kappa Architecture\n**Q:** Compare Lambda and Kappa architectures. When would you use which?\n**A:**\n*   **Lambda:**\n    *   *Design:* Two separate layers: **Batch Layer** (Master dataset, high latency, high accuracy) and **Speed Layer** (Real-time, low latency, approximation). Results are merged in the Serving Layer.\n    *   *Pros:* Fault tolerance (Batch can fix Speed layer errors), separates concerns.\n    *   *Cons:* Maintaining two codebases (Batch logic + Streaming logic) is complex (`Complexity = N * 2`).\n*   **Kappa:**\n    *   *Design:* Everything is a stream. The Batch layer is removed. History is processed by re-playing the stream.\n    *   *Pros:* Single codebase. Simpler architecture.\n    *   *Cons:* Requires strict ordering and immutable logs (Kafka) with long retention.\n    *   *Trend:* Modern tools (Spark Structured Streaming, Flink, Databricks Delta) favor Kappa/Lakehouse models where batch and stream APIs are unified.\n\n### 2. Designing a Data Warehouse from Scratch\n**Q:** Design a DW for an E-commerce company. What layers do you create?\n**A:**\n1.  **Landing Zone (Raw):** Immutable files (JSON/CSV) as received from Source. Partitioned by `Source/Date`.\n2.  **Staging Zone (Clean):** Deduplicated, validated types, Parquet/Delta format.\n3.  **Integration Zone (Silver/Core):** Conformed dimensions, Enriched facts. 3NF or Data Vault modeling.\n4.  **Mart Zone (Gold/Serving):** Aggregated Star Schemas (Fact/Dim) optimized for BI (PowerBI/Tableau).\n5.  **Metadata Layer:** Data Catalog (Amundsen/Datahub) + Quality Checks (Great Expectations).\n\n---\n\n## Streaming & Real-Time\n\n### 3. Real-Time Dashboarding Pipeline\n**Q:** Design a system to count \"Active Users per Minute\" for a game with 10M concurrent users.\n**A:**\n*   **Ingestion:** Game Servers -> Load Balancer -> **Kafka** (Partition by UserID to ensure efficient scaling).\n*   **Processing:** **Apache Flink** or **Spark Streaming**.\n    *   Use *Tumbling Window* (1 minute).\n    *   Use *Watermarking* to handle late-arriving data (e.g., wait 30 seconds).\n    *   *State:* Maintain user state in memory (RocksDB) to handle \"Active\" definition.\n*   **Storage:** Write aggregated results (Time, Count) to a fast Time-Series DB (**TimescaleDB** / **Druid**) or **Redis** for caching.\n*   **Serving:** Grafana or Custom React App polls Redis/Druid.\n\n### 4. Handling Late Data\n**Q:** You are calculating daily revenue. A transaction from \"Yesterday\" arrives \"Today\" due to a system retry. How do you handle it?\n**A:**\n*   **Option 1 (Streaming):** Watermarking. If it's too late (outside watermark), drop it or send to \"Dead Letter Queue\".\n*   **Option 2 (Batch - Idempotency):**\n    *   Store data in partitions based on *Event Time*, not *Processing Time*.\n    *   The late record lands in `Date=Yesterday` partition.\n    *   Re-run the aggregation job for `Date=Yesterday` to include the new record (Backfill/Restatement).\n\n---\n\n## Data Lakehouse Design\n\n### 5. Schema Evolution Strategy\n**Q:** Upstream API adds a new column `discount_code`. How should your pipeline handle it without breaking?\n**A:**\n*   **Bronze (Raw):** Use `PERMISSIVE` mode or JSON columns to ingest *everything* without failing. Retain full fidelity.\n*   **Silver (Curated):**\n    *   *Explicit:* Pipeline fails, alerts engineer to update schema (Strict).\n    *   *Automatic:* Enable Schema Evolution (Delta Lake `mergeSchema`).\n*   **Downstream Protection:** Use Views on top of Silver tables to expose only \"Contracted\" columns, protecting BI dashboards from unexpected schema changes.\n\n### 6. Small File Problem Solution\n**Q:** Your streaming job creates 10,000 tiny KB files every hour in the Data Lake. Query perf is dying. Fix it.\n**A:**\n*   **Immediate Fix:** Run a scheduled **Compaction Job** (e.g., `OPTIMIZE` in Delta / `repartition` in Spark) every night to merge files into 1GB chunks.\n*   **Root Cause Fix:**\n    *   Increase `trigger` interval (Processing time 1 min -> 5 min).\n    *   Use **Auto Compaction** / **Optimized Writes** features if available (Databricks/Snowflake).\n\n---\n\n## Operations & Handling Failures\n\n### 7. Backfilling Historical Data\n**Q:** You found a bug in the logic for \"User Subscription Status\" calculating active users incorrectly for the last 6 months. How do you fix it?\n**A:**\n1.  **Fix Code:** Deploy corrected logic.\n2.  **Isolate:** Ensure the new code handles current incoming data correctly.\n3.  **Backfill Strategy:**\n    *   *Reverse Order:* Reprocess from today backwards? Or *Forward Order*? (Depends on state dependency).\n    *   *Parallelism:* Spin up a separate \"Backfill Cluster\". Process months in parallel if independent.\n    *   *Write Mode:* Use `INSERT OVERWRITE` on partitions to safely replace old bad data with new good data.\n\n### 8. Data Quality Monitoring (Circuit Breakers)\n**Q:** How do you prevent \"Bad Data\" from ruining your CEO's dashboard?\n**A:**\nImplement **Write-Audit-Publish (WAP)** pattern:\n1.  **Write:** ETL writes data to a *Staging/Branch* table (hidden).\n2.  **Audit:** Run automated Quality Checks (e.g., `revenue > 0`, `null_count < 1%`).\n3.  **Publish:**\n    *   *Pass:* Atomically swap/merge Staging to Production.\n    *   *Fail:* Alert Engineering. Do NOT update Production. Dashboard shows \"Stale\" but \"Correct\" data (better than fresh garbage).\n\n---\n\n## Scenario: Rate Limiting & Aggregation\n\n### 9. Deduplication at Scale\n**Q:** You receive 50k events/sec. 20% are duplicates. Deduplicate them efficiently.\n**A:**\n*   **Short Window (10 min):** Deduplicate in Streaming State (Spark `dropDuplicates(\"id\")` with watermark). Very fast.\n*   **Global (All Time):** Cannot keep all History in memory.\n    *   *Bloom Filter:* Probabilistically check if ID exists.\n    *   *Lookup Table:* Check against a high-speed KV store (DynamoDB/Cassandra) if ID was seen.\n    *   *Batch Reconciliation:* Let streams handle short-term dupes; run a nightly batch job to remove long-term dupes.\n\n### 10. GDPR \"Right to be Forgotten\"\n**Q:** A user requests deletion. Their data is in 5000 parquet files in S3. How do you delete it efficiently?\n**A:**\n*   **Naive:** Read all 5000 files -> Filter -> Write 5000 files. (Too expensive).\n*   **Lakehouse (Delta/Hudi):** Supports `DELETE FROM table WHERE user_id = X`. Uses metadata to identifying only the specific files containing that User ID, rewrites only those files (e.g., 5 files), and marks old ones as deleted.\n*   **Encryption approach:** Store user PII with a unique encryption key. Throw away the *Key* to \"Crypto-shred\" the data without rewriting files.\n"}]};

    // --- functionalities ---
    
    // Theme logic
    const themeBtn = document.getElementById('theme-toggle');
    const storedTheme = localStorage.getItem('theme') || 'light';
    if (storedTheme === 'dark') {
        document.documentElement.setAttribute('data-theme', 'dark');
        themeBtn.textContent = '‚òÄÔ∏è';
    }

    themeBtn.onclick = () => {
        const currentTheme = document.documentElement.getAttribute('data-theme');
        const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
        document.documentElement.setAttribute('data-theme', newTheme);
        localStorage.setItem('theme', newTheme);
        themeBtn.textContent = newTheme === 'dark' ? '‚òÄÔ∏è' : 'üåô';
    };

    // Zen Mode logic
    const zenBtns = [document.getElementById('zen-toggle-sidebar'), document.getElementById('zen-toggle-floating')];
    const isZen = localStorage.getItem('zenMode') === 'true';
    if (isZen) {
        document.body.classList.add('zen-mode');
    }

    zenBtns.forEach(btn => {
        btn.onclick = () => {
            document.body.classList.toggle('zen-mode');
            localStorage.setItem('zenMode', document.body.classList.contains('zen-mode'));
        };
    });

    // Rendering logic
    function renderTree(node, container) {
        const ul = document.createElement('ul');
        
        if (node.children) {
            node.children.forEach(child => {
                const li = document.createElement('li');
                
                if (child.type === 'directory') {
                    if (child.children.length === 0) return;

                    const folderDiv = document.createElement('div');
                    folderDiv.className = 'folder-node';
                    
                    const span = document.createElement('span');
                    span.className = 'folder-name';
                    span.textContent = child.name;
                    span.onclick = function(e) {
                        e.stopPropagation();
                        folderDiv.classList.toggle('folder-open');
                    };
                    
                    folderDiv.appendChild(span);
                    
                    const contentDiv = document.createElement('div');
                    contentDiv.className = 'folder-content';
                    renderTree(child, contentDiv);
                    folderDiv.appendChild(contentDiv);
                    
                    li.appendChild(folderDiv);
                } else {
                    const a = document.createElement('a');
                    a.className = 'file-link';
                    a.textContent = child.name;
                    a.onclick = function(e) {
                        e.preventDefault();
                        displayContent(child.content);
                        document.querySelectorAll('.file-link').forEach(el => el.style.fontWeight = 'normal');
                        a.style.fontWeight = 'bold';
                    };
                    li.appendChild(a);
                }
                
                ul.appendChild(li);
            });
        }
        
        container.appendChild(ul);
    }

    function displayContent(markdown) {
        const viewer = document.getElementById('markdown-viewer');
        viewer.innerHTML = marked.parse(markdown);
    }

    const rootContainer = document.getElementById('tree-root');
    renderTree(fileData, rootContainer);
    // Auto-open top level folders (Disabled by default)
    // document.querySelectorAll('.folder-node').forEach(el => el.classList.add('folder-open'));

</script>

</body>
</html>
