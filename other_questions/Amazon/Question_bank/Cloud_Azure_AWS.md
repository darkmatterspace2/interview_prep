# Cloud Platform (AWS & Azure) Interview Questions & Answers (Question Bank 1)

> **Amazon Data Engineer Style** - AWS Ecosystem Focus with Azure Comparisons

---

<a id="index"></a>
## ğŸ“‘ Table of Contents

| Section | Topics |
|---------|--------|
| [1ï¸âƒ£ AWS Storage & Compute](#1ï¸âƒ£-aws-storage--compute-part-2-questions) | S3, Athena, Glue, EMR, Kinesis |
| &nbsp;&nbsp;&nbsp;â”” [S3 Partitioning for Athena](#s3-partitioning-for-athena-optimization) | Hive-style partitioning |
| &nbsp;&nbsp;&nbsp;â”” [Glue vs EMR](#glue-vs-emr-when-to-choose-which) | Decision matrix |
| &nbsp;&nbsp;&nbsp;â”” [Kinesis Data Streams vs Firehose](#kinesis-data-streams-vs-firehose) | Real-time vs delivery |
| &nbsp;&nbsp;&nbsp;â”” [CloudFormation vs Terraform](#cloudformation-vs-terraform) | IaC comparison |
| [2ï¸âƒ£ Redshift Architecture](#2ï¸âƒ£-redshift-architecture) | Leader/Compute nodes |
| &nbsp;&nbsp;&nbsp;â”” [Leader Node vs Compute Node](#leader-node-vs-compute-node) | Architecture diagram |
| [3ï¸âƒ£ AWS to Azure Comparison](#3ï¸âƒ£-aws-to-azure-comparison) | Service mapping table |
| [4ï¸âƒ£ Hands-On Pipeline Components](#4ï¸âƒ£-hands-on-typical-pipeline-components) | AWS & Azure examples |
| &nbsp;&nbsp;&nbsp;â”” [AWS Data Pipeline Architecture](#aws-data-pipeline-architecture) | Glue job example |
| &nbsp;&nbsp;&nbsp;â”” [Azure Data Pipeline Architecture](#azure-data-pipeline-architecture) | Databricks example |
| [5ï¸âƒ£ Lake Formation & Data Governance](#5ï¸âƒ£-lake-formation--data-governance) | Permissions, Purview |
| &nbsp;&nbsp;&nbsp;â”” [Lake Formation Permissions](#lake-formation-permissions) | AWS permissions |
| &nbsp;&nbsp;&nbsp;â”” [Azure Purview Equivalent](#azure-purview-equivalent) | Azure governance |

---

<a id="1ï¸âƒ£-aws-storage--compute-part-2-questions"></a>
## 1ï¸âƒ£ AWS Storage & Compute (Part 2 Questions) [â†©ï¸](#index)

<a id="s3-partitioning-for-athena-optimization"></a>
### S3 Partitioning for Athena Optimization [â†©ï¸](#index)

**Question:** How would you partition S3 data for a log ingestion pipeline receiving terabytes of data daily to optimize for Athena queries?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    S3 PARTITIONING STRATEGY                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HIVE-STYLE PARTITIONING:
s3://bucket/logs/
  â”œâ”€â”€ year=2024/month=02/day=07/hour=10/   â† 4-level partitioning
  â”‚   â”œâ”€â”€ file1.parquet
  â”‚   â””â”€â”€ file2.parquet
  â””â”€â”€ year=2024/month=02/day=07/hour=11/

BENEFITS:
â€¢ Athena can skip irrelevant partitions (partition pruning)
â€¢ Query: WHERE year=2024 AND month=02 â†’ Scans only Feb 2024 data
â€¢ Cost: $5 per TB scanned â†’ Huge savings!
```

**Best Practices:**

| Practice | Recommendation | Why |
|----------|---------------|-----|
| **Partition Column** | year/month/day/hour | Matches common query patterns |
| **Partition Size** | 100 MB - 1 GB each | Balance between pruning benefit and small file overhead |
| **File Format** | Parquet (Snappy) | Columnar + compressed = 10x less scan |
| **File Size** | 128 MB - 512 MB | Optimal for Athena parallelism |

```python
# Write partitioned data from Spark
df.write \
    .partitionBy("year", "month", "day", "hour") \
    .option("compression", "snappy") \
    .parquet("s3://bucket/logs/")

# Athena table definition
CREATE EXTERNAL TABLE logs (
    timestamp TIMESTAMP,
    event_type STRING,
    payload STRING
)
PARTITIONED BY (year INT, month INT, day INT, hour INT)
STORED AS PARQUET
LOCATION 's3://bucket/logs/'
TBLPROPERTIES ('parquet.compression'='SNAPPY');

-- Add partitions automatically
MSCK REPAIR TABLE logs;
```

---

<a id="glue-vs-emr-when-to-choose-which"></a>
### Glue vs EMR: When to Choose Which? [â†©ï¸](#index)

| Factor | AWS Glue | EMR |
|--------|----------|-----|
| **Complexity** | Serverless, simple | More control, complex |
| **Cost Model** | Per DPU-hour ($0.44) | Per instance-hour |
| **Startup Time** | 2-5 minutes | 5-10 minutes |
| **Customization** | Limited (Glue libs) | Full control (any library) |
| **Long-running** | Not ideal (timeout 48h) | Persistent clusters OK |
| **State** | Stateless | Can maintain state (Spark context) |

**Decision Matrix:**

```yaml
Use Glue When:
  - Simple ETL jobs (< 4 hours)
  - Predictable workloads
  - Team lacks Spark expertise
  - Want minimal ops overhead
  - Using Glue Catalog already

Use EMR When:
  - Long-running jobs (> 4 hours)
  - Need custom libraries (ML, special formats)
  - Cost optimization (Spot instances)
  - Interactive development (notebooks)
  - High volume requiring persistent cluster
```

**Cost Comparison (1 TB daily processing):**

```
Glue:
  10 DPUs Ã— 2 hours Ã— $0.44 Ã— 30 days = $264/month

EMR (On-Demand):
  5 Ã— r5.xlarge Ã— 2 hours Ã— $0.252 Ã— 30 = $378/month

EMR (Spot, 70% savings):
  5 Ã— r5.xlarge Ã— 2 hours Ã— $0.076 Ã— 30 = $114/month â† Winner
```

---

<a id="kinesis-data-streams-vs-firehose"></a>
### Kinesis Data Streams vs Firehose [â†©ï¸](#index)

| Feature | Kinesis Data Streams | Kinesis Firehose |
|---------|---------------------|------------------|
| **Use Case** | Real-time processing | Data delivery to sinks |
| **Latency** | Milliseconds | 60 sec - 15 min (buffering) |
| **Consumers** | Lambda, KCL apps, Spark | S3, Redshift, Elasticsearch |
| **Scaling** | Manual (shards) | Automatic |
| **Retention** | 24h - 365 days | No retention (pass-through) |
| **Pricing** | Per shard hour + data | Per data ingested |

**Use Firehose for S3 Ingestion:**

```yaml
Scenario: Stream 1M events/day to S3 for batch analytics

Firehose Setup:
  Source: Direct PUT or Kinesis Data Streams
  Destination: S3 (partitioned by time)
  Buffer: 5 MB or 60 seconds
  Compression: Gzip
  Format: JSON â†’ Parquet (with Glue conversion)

Why Firehose (not Streams):
  - No need for real-time processing
  - Auto-scaling (no shard management)
  - Built-in delivery to S3 with retry
  - Lower cost for simple ingestion
```

---

<a id="cloudformation-vs-terraform"></a>
### CloudFormation vs Terraform [â†©ï¸](#index)

| Aspect | CloudFormation | Terraform |
|--------|----------------|-----------|
| **Cloud Support** | AWS only | Multi-cloud |
| **State** | AWS-managed | Self-managed (S3, TFC) |
| **Language** | YAML/JSON | HCL (more readable) |
| **Drift Detection** | Built-in | terraform plan |
| **Community** | AWS modules | Rich provider ecosystem |
| **Rollback** | Automatic on failure | Manual |

**Terraform State Management:**

```hcl
# backend.tf - Store state remotely
terraform {
  backend "s3" {
    bucket         = "my-terraform-state"
    key            = "data-platform/terraform.tfstate"
    region         = "us-west-2"
    dynamodb_table = "terraform-locks"  # State locking
    encrypt        = true
  }
}

# Best practices:
# 1. Remote state (never local in production)
# 2. State locking (DynamoDB or TFC)
# 3. Separate state files per environment
# 4. State versioning (S3 versioning enabled)
```

---

<a id="2ï¸âƒ£-redshift-architecture"></a>
## 2ï¸âƒ£ Redshift Architecture [â†©ï¸](#index)

<a id="leader-node-vs-compute-node"></a>
### Leader Node vs Compute Node [â†©ï¸](#index)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         REDSHIFT ARCHITECTURE                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   LEADER NODE     â”‚
                          â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
                          â”‚  â€¢ Parses SQL     â”‚
                          â”‚  â€¢ Creates plan   â”‚
                          â”‚  â€¢ Coordinates    â”‚
                          â”‚  â€¢ Aggregates     â”‚
                          â”‚    results        â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼               â–¼               â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ COMPUTE NODE â”‚ â”‚ COMPUTE NODE â”‚ â”‚ COMPUTE NODE â”‚
           â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
           â”‚ â€¢ Store data â”‚ â”‚ â€¢ Store data â”‚ â”‚ â€¢ Store data â”‚
           â”‚ â€¢ Execute    â”‚ â”‚ â€¢ Execute    â”‚ â”‚ â€¢ Execute    â”‚
           â”‚   queries    â”‚ â”‚   queries    â”‚ â”‚   queries    â”‚
           â”‚ â€¢ Local agg  â”‚ â”‚ â€¢ Local agg  â”‚ â”‚ â€¢ Local agg  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               Slice 1-2       Slice 3-4       Slice 5-6
```

**Key Points:**
- **Leader:** Single node, bottleneck for driver operations (ORDER BY LIMIT, LISTAGG)
- **Compute:** Multiple nodes, parallel processing
- **Slices:** Each node has multiple slices (1-16 per node), each processes portion of data

**Optimization:**
```sql
-- Bad: Leader-node functions
SELECT LISTAGG(product_name, ',') FROM products;  -- Leader bottleneck

-- Better: Aggregate on compute nodes
SELECT region, COUNT(*) FROM shipments GROUP BY region;  -- Parallel
```

---

<a id="3ï¸âƒ£-aws-to-azure-comparison"></a>
## 3ï¸âƒ£ AWS to Azure Comparison [â†©ï¸](#index)

| AWS Service | Azure Equivalent | Notes |
|-------------|------------------|-------|
| **S3** | Azure Blob Storage / ADLS Gen2 | ADLS Gen2 for analytics |
| **Kinesis** | Event Hubs | Event Hubs has Kafka compatibility |
| **Glue** | Data Factory + Synapse | ADF for orchestration, Synapse for Spark |
| **EMR** | HDInsight / Databricks | Databricks preferred |
| **Redshift** | Synapse Analytics | Dedicated SQL pools |
| **Athena** | Synapse Serverless SQL | Query S3/ADLS directly |
| **Lambda** | Azure Functions | Similar FaaS |
| **Step Functions** | Logic Apps / Durable Functions | Logic Apps is low-code |
| **MSK (Kafka)** | Event Hubs Kafka / HDInsight Kafka | Event Hubs simpler |
| **Lake Formation** | Purview + Unity Catalog | Data governance |

---

<a id="4ï¸âƒ£-hands-on-typical-pipeline-components"></a>
## 4ï¸âƒ£ Hands-On: Typical Pipeline Components [â†©ï¸](#index)

<a id="aws-data-pipeline-architecture"></a>
### AWS Data Pipeline Architecture [â†©ï¸](#index)

```python
# Glue Job Example: Bronze to Silver
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'SOURCE_PATH', 'TARGET_PATH'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read from S3 (Bronze)
bronze_df = glueContext.create_dynamic_frame.from_options(
    connection_type="s3",
    connection_options={"paths": [args['SOURCE_PATH']]},
    format="json"
)

# Transform
from pyspark.sql.functions import *
silver_df = bronze_df.toDF() \
    .dropDuplicates(['event_id']) \
    .filter(col('event_type').isNotNull()) \
    .withColumn('processed_at', current_timestamp())

# Write to S3 (Silver)
silver_df.write \
    .mode('overwrite') \
    .partitionBy('event_date') \
    .parquet(args['TARGET_PATH'])

job.commit()
```

<a id="azure-data-pipeline-architecture"></a>
### Azure Data Pipeline Architecture [â†©ï¸](#index)

```python
# Databricks Notebook: Bronze to Silver (Azure)
from pyspark.sql.functions import *

# Read from ADLS Gen2 (Bronze)
bronze_df = spark.read \
    .format("json") \
    .load("abfss://container@storage.dfs.core.windows.net/bronze/events/")

# Transform
silver_df = bronze_df \
    .dropDuplicates(['event_id']) \
    .filter(col('event_type').isNotNull()) \
    .withColumn('processed_at', current_timestamp())

# Write to ADLS Gen2 (Silver) as Delta
silver_df.write \
    .format("delta") \
    .mode('overwrite') \
    .option('replaceWhere', f"event_date = '{processing_date}'") \
    .save("abfss://container@storage.dfs.core.windows.net/silver/events/")
```

---

<a id="5ï¸âƒ£-lake-formation--data-governance"></a>
## 5ï¸âƒ£ Lake Formation & Data Governance [â†©ï¸](#index)

<a id="lake-formation-permissions"></a>
### Lake Formation Permissions [â†©ï¸](#index)

```python
# Grant permissions using Lake Formation
import boto3

client = boto3.client('lakeformation')

# Grant SELECT on table to role
response = client.grant_permissions(
    Principal={'DataLakePrincipalIdentifier': 'arn:aws:iam::123456789012:role/AnalystRole'},
    Resource={
        'Table': {
            'DatabaseName': 'analytics',
            'Name': 'shipments'
        }
    },
    Permissions=['SELECT'],
    PermissionsWithGrantOption=[]
)

# Column-level security
response = client.grant_permissions(
    Principal={'DataLakePrincipalIdentifier': 'arn:aws:iam::123456789012:role/RestrictedRole'},
    Resource={
        'TableWithColumns': {
            'DatabaseName': 'analytics',
            'Name': 'customers',
            'ColumnNames': ['name', 'email'],  # Only these columns
            'ColumnWildcard': None
        }
    },
    Permissions=['SELECT']
)
```

<a id="azure-purview-equivalent"></a>
### Azure Purview Equivalent [â†©ï¸](#index)

```python
# Register data source in Purview
from azure.purview.catalog import PurviewCatalogClient
from azure.identity import DefaultAzureCredential

credential = DefaultAzureCredential()
client = PurviewCatalogClient(
    endpoint="https://purview-account.purview.azure.com",
    credential=credential
)

# Create classification (PII tagging)
classification = {
    "name": "PII.Email",
    "description": "Email addresses"
}

# Assign classification to column
entity_update = {
    "guid": "column-guid",
    "classifications": [{"typeName": "PII.Email"}]
}
```
